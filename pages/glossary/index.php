<?php
?>
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AINexus</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="style.css">
</head>

<body>

    <?php include '../../partials/_navbar/navbar.php'; ?>
    <div class="container">

        <div class="main">
            
        
            <div class="middle">
                <div class="above-title">
                    <h1 class="text-dark">Artificial Intelligence Terms: A to Z<br> Glossary</h1>
                    <img src="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://images.ctfassets.net/wp1lcwdav1p1/Otc07JBoEZ2f0QA7E4k17/0cd37a4956703c4cac5188e5e7503b03/AI_Glossary_Terms.png?w=1500&h=680&q=60&fit=fill&f=faces&fm=jpg&fl=progressive&auto=format%2Ccompress&dpr=1&w=1000" alt="">
                    <p>
                        Embarking on the journey into artificial intelligence (AI) opens up a world of innovation and potential. As a student exploring AI, understanding core terms and concepts is a powerful first step. This glossary provides an easy-to-understand guide to key AI topics, covering everything from machine learning basics to advanced topics like neural networks and natural language processing (NLP). <br> <br>
    
                        This list is designed to help you build a strong foundation in AI vocabulary. Whether you're preparing for an academic project, drafting your resume, or simply satisfying your curiosity about AI, these terms will help you feel confident and well-prepared. <br> <br>
    
                        AI is shaping our future, and learning its language is a great way to start contributing to the field. Dive into these essential AI terms and get ready for an exciting journey into technology’s next frontier!
                    </p>
                    <h2>AI Terms</h2>
                </div>
                <div class="main-content">
    
                    <!-- 01. AI -->
                    <div class="card" id="artificialIntelligence">
                        <h3>01. Artificial Intelligence</h3>
                        <img src="https://fpf.org/wp-content/uploads/2020/12/AI-inforgraphic.jpg" alt="AI Infographic" class="toggleImg p-3 align-center" height="600px" width="100%" style="display: none;">
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">AI stands for artificial intelligence, which is the simulation of human intelligence processes by machines or computer systems. AI can mimic human capabilities such as communication, learning, and decision-making.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>Artificial intelligence is a field of science concerned with building computers and machines that can reason, learn, and act in such a way that would normally require human intelligence or that involves data whose scale exceeds what humans can analyze. <br>
    
                                AI is a broad field that encompasses many different disciplines, including computer science, data analytics and statistics, hardware and software engineering, linguistics, neuroscience, and even philosophy and psychology. <br>
    
                                On an operational level for business use, AI is a set of technologies that are based primarily on machine learning and deep learning, used for data analytics, predictions and forecasting, object categorization, natural language processing, recommendations, intelligent data retrieval, and more. <br>
    
                            <p class="fw-bold">How does AI work?</p>
                            While the specifics vary across different AI techniques, the core principle revolves around data. AI systems learn and improve through exposure to vast amounts of data, identifying patterns and relationships that humans may miss. <br>
    
                            This learning process often involves algorithms, which are sets of rules or instructions that guide the AI's analysis and decision-making. In machine learning, a popular subset of AI, algorithms are trained on labeled or unlabeled data to make predictions or categorize information. <br>
    
                            Deep learning, a further specialization, utilizes artificial neural networks with multiple layers to process information, mimicking the structure and function of the human brain. Through continuous learning and adaptation, AI systems become increasingly adept at performing specific tasks, from recognizing images to translating languages and beyond. <br>
    
                            <p class="fw-bold">Types of artificial intelligence
                                Artificial intelligence can be organized in several ways, depending on stages of development or actions being performed.</p>
    
                            For instance, four stages of AI development are commonly recognized. <br>
    
                            Reactive machines: Limited AI that only reacts to different kinds of stimuli based on preprogrammed rules. Does not use memory and thus cannot learn with new data. IBM’s Deep Blue that beat chess champion Garry Kasparov in 1997 was an example of a reactive machine. <br>
                            Limited memory: Most modern AI is considered to be limited memory. It can use memory to improve over time by being trained with new data, typically through an artificial neural network or other training model. Deep learning, a subset of machine learning, is considered limited memory artificial intelligence. <br>
                            Theory of mind: Theory of mind AI does not currently exist, but research is ongoing into its possibilities. It describes AI that can emulate the human mind and has decision-making capabilities equal to that of a human, including recognizing and remembering emotions and reacting in social situations as a human would. <br>
                            Self aware: A step above theory of mind AI, self-aware AI describes a mythical machine that is aware of its own existence and has the intellectual and emotional capabilities of a human. Like theory of mind AI, self-aware AI does not currently exist. <br>
                            A more useful way of broadly categorizing types of artificial intelligence is by what the machine can do. All of what we currently call artificial intelligence is considered artificial “narrow” intelligence, in that it can perform only narrow sets of actions based on its programming and training. For instance, an AI algorithm that is used for object classification won’t be able to perform natural language processing. Google Search is a form of narrow AI, as is predictive analytics, or virtual assistants. <br>
    
                            Artificial general intelligence (AGI) would be the ability for a machine to “sense, think, and act” just like a human. AGI does not currently exist. The next level would be artificial superintelligence (ASI), in which the machine would be able to function in all ways superior to a human. <br> </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 02. AIEthics -->
                    <div class="card" id="aiEthics">
                        <h3>02. AI Ethics</h3>
                        <img class="toggleImg p-3 align-center" src="https://www.orientsoftware.com/Themes/Content/Images/blog/2022-04-14/ethics-in-ai-2.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">AI ethics refers to the issues that AI stakeholders such as engineers and government officials must consider to ensure that the technology is developed and used responsibly. This means adopting and implementing systems that support a safe, secure, unbiased, and environmentally friendly approach to artificial intelligence.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>As artificial intelligence (AI) becomes increasingly important to society, experts in the field have identified a need for ethical boundaries when it comes to creating and implementing new AI tools. Although there's currently no wide-scale governing body to write and enforce these rules, many technology companies have adopted their own version of AI ethics or an AI code of conduct.
    
                                AI ethics are the moral principles that companies use to guide responsible and fair development and use of AI. In this article, we'll explore what ethics in AI are, why they matter, and some challenges and benefits of developing an AI code of conduct. <br>
    
                            <p class="fw-bold">What are AI ethics?</p>
                            AI ethics are the set of guiding principles that stakeholders (from engineers to government officials) use to ensure artificial intelligence technology is developed and used responsibly. This means taking a safe, secure, humane, and environmentally friendly approach to AI.
    
                            A strong AI code of ethics can include avoiding bias, ensuring privacy of users and their data, and mitigating environmental risks. Codes of ethics in companies and government-led regulatory frameworks are two main ways that AI ethics can be implemented. By covering global and national ethical AI issues, and laying the policy groundwork for ethical AI in companies, both approaches help regulate AI technology.
    
                            More broadly, discussion around AI ethics has progressed from being centered around academic research and non-profit organizations. Today, big tech companies like IBM, Google, and Meta have assembled teams to tackle ethical issues that arise from collecting massive amounts of data. At the same time, government and intergovernmental entities have begun to devise regulations and ethics policy based on academic research.
    
                            <p class="fw-bold">Stakeholders in AI ethics</p>
    
                            Developing ethical principles for responsible AI use and development requires industry actors to work together. Stakeholders must examine how social, economic, and political issues intersect with AI, and determine how machines and humans can coexist harmoniously.
    
                            Each of these actors play an important role in ensuring less bias and risk for AI technologies.
    
    
                            Academics: Researchers and professors are responsible for developing theory-based statistics, research, and ideas that can support governments, corporations, and non-profit organizations.
    
    
                            Government: Agencies and committees within a government can help facilitate AI ethics in a nation. A good example of this is the Preparing for the Future of Artificial Intelligence report that was developed by the National Science and Technology Council (NSTC) in 2016, which outlines AI and its relationship to public outreach, regulation, governance, economy, and security.
    
    
                            Intergovernmental entities: Entities like the United Nations and the World Bank are responsible for raising awareness and drafting agreements for AI ethics globally. For example, UNESCO’s 193 member states adopted the first ever global agreement on the Ethics of AI in November 2021 to promote human rights and dignity.
    
    
                            Non-profit organizations: Non-profit organizations like Black in AI and Queer in AI help diverse groups gain representation within AI technology. The Future of Life Institute created 23 guidelines that are now the Asilomar AI Principles, which outline specific risks, challenges, outcomes for AI technologies.
    
    
                            Private companies: Executives at Google, Meta, and other tech companies, as well as banking, consulting, health care, and other industries within the private sector that uses AI technology, are responsible for creating ethics teams and codes of conduct. This often creates a standard for companies to follow suit. <br> </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 03. Algorithm -->
                    <div class="card" id="algorithm">
                        <h3>03. Algorithm</h3>
                        <img class="toggleImg p-3 align-center" src="https://www.techtarget.com/rms/onlineimages/types_of_algorithms-f_mobile.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">An algorithm is a sequence of rules given to an AI machine to perform a task or solve a problem. Common algorithms include classification, regression, and clustering.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>At the core of machine learning are algorithms, which are trained to become the machine learning models used to power some of the most impactful innovations in the world today. In this article, you'll learn about 10 of the most popular machine learning algorithms that you'll want to know, and explore the different learning styles used to turn machine learning algorithms into functioning machine learning models. <br>
    
                            <p class="fw-bold">10 machine learning algorithms to know</p>
    
    
    
                            1. Linear regression
                            Linear regression is a supervised machine learning technique used for predicting and forecasting values that fall within a continuous range, such as sales numbers or housing prices. It is a technique derived from statistics and is commonly used to establish a relationship between an input variable (X) and an output variable (Y) that can be represented by a straight line.
    
                            In simple terms, linear regression takes a set of data points with known input and output values and finds the line that best fits those points. This line, known as the "regression line," serves as a predictive model. By using this line, we can estimate or predict the output value (Y) for a given input value (X).
    
                            Linear regression is primarily used for predictive modeling rather than categorization. It is useful when we want to understand how changes in the input variable affect the output variable. By analyzing the slope and intercept of the regression line, we can gain insights into the relationship between the variables and make predictions based on this understanding.
    
                            2. Logistic regression
                            Logistic regression, also known as "logit regression," is a supervised learning algorithm primarily used for binary classification tasks. It is commonly employed when we want to determine whether an input belongs to one class or another, such as deciding whether an image is a cat or not a cat.
    
                            Logistic regression predicts the probability that an input can be categorized into a single primary class. However, in practice, it is commonly used to group outputs into two categories: the primary class and not the primary class. To accomplish this, logistic regression creates a threshold or boundary for binary classification. For example, any output value between 0 and 0.49 might be classified as one group, while values between 0.50 and 1.00 would be classified as the other group.
    
                            Consequently, logistic regression is typically used for binary categorization rather than predictive modeling. It enables us to assign input data to one of two classes based on the probability estimate and a defined threshold. This makes logistic regression a powerful tool for tasks such as image recognition, spam email detection, or medical diagnosis where we need to categorize data into distinct classes.
    
                            3. Naive Bayes
                            Naive Bayes is a set of supervised learning algorithms used to create predictive models for binary or multi-classification tasks. It is based on Bayes' Theorem and operates on conditional probabilities, which estimate the likelihood of a classification based on the combined factors while assuming independence between them.
    
                            Let's consider a program that identifies plants using a Naive Bayes algorithm. The algorithm takes into account specific factors such as perceived size, color, and shape to categorize images of plants. Although each of these factors is considered independently, the algorithm combines them to assess the probability of an object being a particular plant.
    
                            Naive Bayes leverages the assumption of independence among the factors, which simplifies the calculations and allows the algorithm to work efficiently with large datasets. It is particularly well-suited for tasks like document classification, email spam filtering, sentiment analysis, and many other applications where the factors can be considered separately but still contribute to the overall classification.
    
                            4. Decision tree
                            A decision tree is a supervised learning algorithm used for classification and predictive modeling tasks. It resembles a flowchart, starting with a root node that asks a specific question about the data. Based on the answer, the data is directed down different branches to subsequent internal nodes, which ask further questions and guide the data to subsequent branches. This process continues until the data reaches an end node, also known as a leaf node, where no further branching occurs.
    
                            Decision tree algorithms are popular in machine learning because they can handle complex datasets with ease and simplicity. The algorithm's structure makes it straightforward to understand and interpret the decision-making process. By asking a sequence of questions and following the corresponding branches, decision trees enable us to classify or predict outcomes based on the data's characteristics.
    
                            This simplicity and interpretability make decision trees valuable for various applications in machine learning, especially when dealing with complex datasets.
    
                            5. Random forest
                            A random forest algorithm is an ensemble of decision trees used for classification and predictive modeling. Instead of relying on a single decision tree, a random forest combines the predictions from multiple decision trees to make more accurate predictions.
    
                            In a random forest, numerous decision tree algorithms (sometimes hundreds or even thousands) are individually trained using different random samples from the training dataset. This sampling method is called "bagging." Each decision tree is trained independently on its respective random sample.
    
                            Once trained, the random forest takes the same data and feeds it into each decision tree. Each tree produces a prediction, and the random forest tallies the results. The most common prediction among all the decision trees is then selected as the final prediction for the dataset.
    
                            Random forests address a common issue called "overfitting" that can occur with individual decision trees. Overfitting happens when a decision tree becomes too closely aligned with its training data, making it less accurate when presented with new data.
    
                            6. K-nearest neighbor (KNN)
                            K-nearest neighbor (KNN) is a supervised learning algorithm commonly used for classification and predictive modeling tasks. The name "K-nearest neighbor" reflects the algorithm's approach of classifying an output based on its proximity to other data points on a graph.
    
                            Let's say we have a dataset with labeled points, some marked as blue and others as red. When we want to classify a new data point, KNN looks at its nearest neighbors in the graph. The "K" in KNN refers to the number of nearest neighbors considered. For example, if K is set to 5, the algorithm looks at the 5 closest points to the new data point.
    
                            Based on the majority of the labels among the K nearest neighbors, the algorithm assigns a classification to the new data point. For instance, if most of the nearest neighbors are blue points, the algorithm classifies the new point as belonging to the blue group.
    
                            Additionally, KNN can also be used for prediction tasks. Instead of assigning a class label, KNN can estimate the value of an unknown data point based on the average or median of its K nearest neighbors.
    
    
                            7. K-means
                            K-means is an unsupervised algorithm commonly used for clustering and pattern recognition tasks. It aims to group data points based on their proximity to one another. Similar to K-nearest neighbor (KNN), K-means clustering utilizes the concept of proximity to identify patterns in data.
    
                            Each of the clusters is defined by a centroid, a real or imaginary center point for the cluster. K-means is useful on large data sets, especially for clustering, though it can falter when handling outliers.
    
                            Clustering algorithms are particularly useful for large datasets and can provide insights into the inherent structure of the data by grouping similar points together. It has applications in various fields such as customer segmentation, image compression, and anomaly detection.
    
                            Read more: What is Big Data? A Layperson's Guide
    
                            8. Support vector machine (SVM)
                            A support vector machine (SVM) is a supervised learning algorithm commonly used for classification and predictive modeling tasks. SVM algorithms are popular because they are reliable and can work well even with a small amount of data. SVM algorithms work by creating a decision boundary called a "hyperplane." In two-dimensional space, this hyperplane is like a line that separates two sets of labeled data.
    
                            The goal of SVM is to find the best possible decision boundary by maximizing the margin between the two sets of labeled data. It looks for the widest gap or space between the classes. Any new data point that falls on either side of this decision boundary is classified based on the labels in the training dataset.
    
                            It's important to note that hyperplanes can take on different shapes when plotted in three-dimensional space, allowing SVM to handle more complex patterns and relationships in the data.
    
                            9. Apriori
                            Apriori is an unsupervised learning algorithm used for predictive modeling, particularly in the field of association rule mining.
    
                            The Apriori algorithm was initially proposed in the early 1990s as a way to discover association rules between item sets. It is commonly used in pattern recognition and prediction tasks, such as understanding a consumer's likelihood of purchasing one product after buying another.
    
                            The Apriori algorithm works by examining transactional data stored in a relational database. It identifies frequent itemsets, which are combinations of items that often occur together in transactions. These itemsets are then used to generate association rules. For example, if customers frequently buy product A and product B together, an association rule can be generated to suggest that purchasing A increases the likelihood of buying B.
    
                            By applying the Apriori algorithm, analysts can uncover valuable insights from transactional data, enabling them to make predictions or recommendations based on observed patterns of itemset associations.
    
                            10. Gradient boosting
                            Gradient boosting algorithms employ an ensemble method, which means they create a series of "weak" models that are iteratively improved upon to form a strong predictive model. The iterative process gradually reduces the errors made by the models, leading to the generation of an optimal and accurate final model.
    
                            The algorithm starts with a simple, naive model that may make basic assumptions, such as classifying data based on whether it is above or below the mean. This initial model serves as a starting point.
    
                            In each iteration, the algorithm builds a new model that focuses on correcting the mistakes made by the previous models. It identifies the patterns or relationships that the previous models struggled to capture and incorporates them into the new model.
    
                            Gradient boosting is effective in handling complex problems and large datasets. It can capture intricate patterns and dependencies that may be missed by a single model. By combining the predictions from multiple models, gradient boosting produces a powerful predictive model.
                            </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 04. API -->
                    <div class="card" id="api">
                        <h3>04. API</h3>
                        <img class="toggleImg p-3 align-center" src="https://aviowiki.com/wp-content/uploads/2022/01/API-Info-graphic.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">An API, or application programming interface, is a set of protocols that determine how two software applications will interact with each other. APIs tend to be written in programming languages such as C++ or JavaScript.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            <p class="fw-bold">What is an API?</p>
                            API stands for application programming interface. An API is a set of protocols and instructions written in programming languages such as C++ or JavaScript that determine how two software components will communicate with each other. Unlike a user interface that is visible to everyone, APIs work behind the scenes to allow users to locate and retrieve the requested information. Think of APIs like contracts that determine how two software systems will interact.
    
                            Interested in learning how to work with APIs? You can get hands-on practice for free by enrolling in DeepLearning.AI's beginner-friendly online project, Building Systems with the ChatGPT API. Some experience with Python is recommended.
    
                            <p class="fw-bold">API Examples</p>
    
                            As an internet user, you’ve most likely experienced the convenience API technology enables when browsing a website or using a mobile app. Application programming interfaces are a crucial behind-the-scenes aspect of user experience (UX). Consider a few familiar examples of APIs and how a website owner or administrator might use them:
    
    
                            The YouTube API allows you to add videos to your website or app, as well as manage your playlists and subscriptions.
    
    
                            The Facebook API for conversions allows you to track page visits and conversions, as well as provide data for ad targeting and reporting.
    
    
                            The Google Maps API allows you to embed static and dynamic maps, as well as street view imagery, on your website.
    
    
                            Paypal's public simple object access protocol (SOAP) API. SOAP APIs are often used for identity management and payment gateways, especially at the enterprise level. This type of API can be more challenging to integrate than REST APIs, but typically offer more advanced features.
    
                            Any time you land on a site and watch a video, see an ad on Facebook related to a website you recently visited, or use the map on a business’s website to find its physical location, chances are an API has been at work to make this experience possible.<br>
    
                            <p class="fw-bold">Types of API</p>
    
                            Now that you have an API definition, the next step is to become familiar with the different types of APIs.
    
    
                            Open APIs - also known as external or public APIs, are available for anyone to use and integrate with their sites or apps.
    
    
                            Partner APIs - are also considered external, but you can use them only if you have a business relationship with the companies providing them.
    
    
                            Internal APIs - also called private APIs, are used by people within a company and help to transfer data between teams or connect different systems and apps. Third parties do not access internal APIs like they do with open or partner APIs.
    
    
                            Composite APIs - combine multiple APIs from different servers or data sources to create a unified connection to a single system.
    
    
                            Web Service API (or Web API) - an application interface between a web browser and a web server
    
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 05. Big Data -->
                    <div class="card" id="bigData">
                        <h3>05. Big Data</h3>
                        <img class="toggleImg p-3 align-center" src="https://i0.wp.com/www.techcheers.com/wp-content/uploads/2015/07/4-Vs-of-big-data.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Big data refers to the large data sets that can be studied to reveal patterns and trends to support business decisions. It’s called “big” data because organizations can now gather massive amounts of complex data using data collection tools and systems. Big data can be collected very quickly and stored in a variety of formats.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Big data is the vast amount of data that can be studied to show patterns, trends, and associations. Explore the basics of big data, how it's used, the industries that use it most, and how you can pursue a career in big data. <br>
    
                                Big data refers to large data sets that can be studied to reveal patterns, trends, and associations. The vast number of data collection avenues means that data can now come in larger quantities, be gathered much more quickly, and exist in a greater variety of formats than ever before. This new, larger, and more complex data is collectively called big data.
    
                            <p class="fw-bold">The three Vs of big data</p>
                            Big data is broadly defined by the three Vs: volume, velocity, and variety.
    
    
                            Volume refers to the amount of data. Big data deals with high volumes of data.
    
    
                            Velocity refers to the rate at which the data is received. Big data streams at a high velocity, often directly into memory rather than being stored on a disk.
    
    
                            Variety refers to the wide range of data formats. Big data may be structured, semi-structured, or unstructured and can be presented as numbers, text, images, audio, and more.
    
    
                            Companies that process big data may also focus on other Vs, such as value, veracity, and variability.<br>
    
                            <p class="fw-bold">What’s driving big data growth?</p>
    
                            Emerging information technology has allowed data to be collected, stored, and analyzed at unprecedented scales. The internet continues to be adopted by new users in the US and across the globe, and developing technologies have allowed the internet to be integrated into many different products, creating numerous new sources of data. The millions of people watching Netflix, using Google, and buying products online daily contribute to the increasing volume and sophistication of big data.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 06. Chatbot -->
                    <div class="card" id="chatbot">
                        <h3>06. Chatbot</h3>
                        <img class="toggleImg p-3 align-center" src="https://klizos.com/wp-content/uploads/image-1-3.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A chatbot is a computer program that simulates human conversation with an end user. Not all chatbots are equipped with artificial intelligence (AI), but modern chatbots increasingly use conversational AI techniques such as natural language processing (NLP) to understand user questions and automate responses to them.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
    
                            <p class="fw-bold">Generative AI-powered chatbots</p>
                            The next generation of chatbots with generative AI capabilities will offer even more enhanced functionality with their understanding of common language and complex queries, their ability to adapt to a user’s style of conversation and use of empathy when answering users’ questions. Business leaders can clearly see this future: 85% of execs say generative AI will be interacting directly with customers in the next two years, as reported in The CEO’s guide to generative AI study, from IBV. An enterprise-grade artificial intelligence solution can empower companies to automate self-service and accelerate the development of exceptional user experiences. 
    
                            FAQ chatbots no longer need to be pre-programmed with answers to set questions: It’s easier and faster to use generative AI in combination with an organization’s’ knowledge base to automatically generate answers in response to the wider range of questions.
    
                            While conversational AI chatbots can digest a users’ questions or comments and generate a human-like response, generative AI chatbots can take this a step further by generating new content as the output. This new content can include high-quality text, images and sound based on the LLMs they are trained on. Chatbot interfaces with generative AI can recognize, summarize, translate, predict and create content in response to a user’s query without the need for human interaction.
    
                            Enterprise-grade, self-learning generative AI chatbots built on a conversational AI platform are continually and automatically improving. They employ algorithms that automatically learn from past interactions how best to answer questions and improve conversation flow routing.<br>
    
                            <p class="fw-bold">The value of chatbots</p>
    
                            Chatbots can make it easy for users to find information by instantaneously responding to questions and requests—through text input, audio input, or both—without the need for human intervention or manual research.
    
                            Chatbot technology is now commonplace, found everywhere from smart speakers at home and consumer-facing instances of SMS, WhatsApp and Facebook Messenger, to workplace messaging applications including Slack. The latest evolution of AI chatbots, often referred to as “intelligent virtual assistants” or “virtual agents,” can not only understand free-flowing conversation through use of sophisticated language models, but even automate relevant tasks. Alongside well-known consumer-facing intelligent virtual assistants—such as Apple's Siri, Amazon Alexa, Google’s Gemini and OpenAI’s ChatGPT—virtual agents are also increasingly used in an enterprise context to assist customers and employees.
    
                            To increase the power of apps already in use, well-designed chatbots can be integrated into the software an organization is already using. For example, a chatbot can be added to Microsoft Teams to create and customize a productive hub where content, tools, and members come together to chat, meet and collaborate.
    
                            To get the most from an organization’s existing data, enterprise-grade chatbots can be integrated with critical systems and orchestrate workflows inside and outside of a CRM system. Chatbots can handle real-time actions as routine as a password change, all the way through a complex multi-step workflow spanning multiple applications. In addition, conversational analytics can analyze and extract insights from natural language conversations, typically between customers interacting with businesses through chatbots and virtual assistants.
    
                            Artificial intelligence can also be a powerful tool for developing conversational marketing strategies. AI chatbots are available to deliver customer care 24/7 and can discover insights into your customer’s engagement and buying patterns to drive more compelling conversations, and deliver more consistent and personalized digital experiences across your web and messaging channels.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 07. Cognitive computing -->
                    <div class="card" id="cognitiveComputing">
                        <h3>07. Cognitive Computing</h3>
                        <img class="toggleImg p-3 align-center" src="https://www.predictiveanalyticstoday.com/wp-content/uploads/2016/05/What-is-Cognitive-Computing-Top-10-Cognitive-Computing-Companies.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Cognitive computing is essentially the same as AI. It’s a computerized model that focuses on mimicking human thought processes such as pattern recognition and learning. Marketing teams sometimes use this term to eliminate the sci-fi mystique of AI.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
    
                            <p class="fw-bold">What is cognitive computing?</p>
                            Cognitive computing is the use of computerized models to simulate the human thought process in complex situations where the answers might be ambiguous and uncertain. The phrase is closely associated with IBM's cognitive computer system, Watson.
    
                            Computers are faster than humans at processing and calculating, but they've yet to master some tasks, such as understanding natural language and recognizing objects in an image. Cognitive computing is an attempt to have computers mimic the way the human brain works.
    
                            To accomplish this, cognitive computing uses artificial intelligence (AI) and other underlying technologies, including the following:
    
                            Expert systems.
                            Neural networks.
                            Machine learning.
                            Deep learning.
                            Natural language processing (NLP).
                            Speech recognition.
                            Object recognition.
                            Robotics.
                            Cognitive computing uses these processes in conjunction with self-learning algorithms, data analysis and pattern recognition to teach computing systems. The learning technology can be used for sentiment analysis, risk assessments and face detection. In addition, cognitive computing is particularly useful in fields such as healthcare, banking, finance and retail.<br>
    
                            <p class="fw-bold">How cognitive computing works</p>
    
                            Systems used in the cognitive sciences combine data from various sources while weighing context and conflicting evidence to suggest the best possible answers. To achieve this, cognitive systems include self-learning technologies that use data mining, pattern recognition and NLP to mimic human intelligence.
    
                            Using computer systems to solve the types of problems that humans are typically tasked with requires vast amounts of structured and unstructured data fed to machine learning algorithms. Over time, cognitive systems can refine the way they identify patterns and process data. They become capable of anticipating new problems and modeling possible solutions.
    
                            For example, by storing thousands of pictures of dogs in a database, an AI system can be taught how to identify pictures of dogs. The more data a system is exposed to, the more it's able to learn and the more accurate it becomes over time.
    
                            To achieve those capabilities, cognitive computing systems must have the following attributes:
    
                            Adaptive. Systems must be flexible enough to learn as information changes and goals evolve. They must digest dynamic data in real time and adjust as the data and environments change.
                            Interactive. Human-computer interaction is a critical component in cognitive systems. Users must be able to interact with cognitive machines and define their needs as they change. The technologies must also be able to interact with other processors, devices and cloud platforms.
                            Iterative and stateful. Cognitive computing technologies can ask questions and pull in additional data to identify or clarify a problem. They must be stateful in that they keep information about similar situations that have occurred previously.
                            Contextual. Understanding context is critical in thought processes. Cognitive systems must understand, identify and mine contextual data, such as syntax, time, location, domain, user requirements, user profiles, tasks and goals. The systems can draw on multiple sources of information, including structured and unstructured data and visual, auditory and sensor data.
    
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 08. Computer vision -->
                    <div class="card" id="computerVision">
                        <h3>08. Computer vision</h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn.prod.website-files.com/5d7b77b063a9066d83e1209c/60c12a6e084dab67d440a268_VuZKpISJC3C5ZjJ-jKPCFCDu5xiJJ595iCYtMGJguD6R6K5IDWBYm0tbMsBZwViACf73_AZrk9tAjVXemVNM-_ypZILqzf-7mRDS0x_4Nrr9DfIHv02-lD8qJCCNmWic4jaF63sO.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Computer vision is an interdisciplinary field of science and technology that focuses on how computers can gain understanding from images and videos. For AI engineers, computer vision allows them to automate activities that the human visual system typically performs.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Computer vision is a field of artificial intelligence (AI) that uses machine learning and neural networks to teach computers and systems to derive meaningful information from digital images, videos and other visual inputs—and to make recommendations or take actions when they see defects or issues.
    
                                Computer vision works much the same as human vision, except humans have a head start. Human sight has the advantage of lifetimes of context to train how to tell objects apart, how far away they are, whether they are moving or something is wrong with an image.
    
                                Computer vision trains machines to perform these functions, but it must do it in much less time with cameras, data and algorithms rather than retinas, optic nerves and a visual cortex. Because a system trained to inspect products or watch a production asset can analyze thousands of products or processes a minute, noticing imperceptible defects or issues, it can quickly surpass human capabilities.
    
                                Computer vision is used in industries that range from energy and utilities to manufacturing and automotive—and the market is continuing to grow. It is expected to reach USD 48.6 billion by 2022. <br>
    
                            <p class="fw-bold">How does computer vision work?</p>
                            Computer vision needs lots of data. It runs analyses of data over and over until it discerns distinctions and ultimately recognize images. For example, to train a computer to recognize automobile tires, it needs to be fed vast quantities of tire images and tire-related items to learn the differences and recognize a tire, especially one with no defects.
    
                            Two essential technologies are used to accomplish this: a type of machine learning called deep learning and a convolutional neural network (CNN).
    
                            Machine learning uses algorithmic models that enable a computer to teach itself about the context of visual data. If enough data is fed through the model, the computer will “look” at the data and teach itself to tell one image from another. Algorithms enable the machine to learn by itself, rather than someone programming it to recognize an image.
    
                            A CNN helps a machine learning or deep learning model “look” by breaking images down into pixels that are given tags or labels. It uses the labels to perform convolutions (a mathematical operation on two functions to produce a third function) and makes predictions about what it is “seeing.” The neural network runs convolutions and checks the accuracy of its predictions in a series of iterations until the predictions start to come true. It is then recognizing or seeing images in a way similar to humans.
    
                            Much like a human making out an image at a distance, a CNN first discerns hard edges and simple shapes, then fills in information as it runs iterations of its predictions. A CNN is used to understand single images. A recurrent neural network (RNN) is used in a similar way for video applications to help computers understand how pictures in a series of frames are related to one another.<br>
    
    
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 09. Data mining -->
                    <div class="card" id="dataVision">
                        <h3>09. Data Mining</h3>
                        <img class="toggleImg p-3 align-center" src="https://en.mdv.co.jp/ebm/wp-content/uploads/2022/08/column-17-01-1024x576.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Data mining is the process of sorting through large data sets to identify patterns that can improve models or solve problems. Data mining is the use of machine learning and statistical analysis to uncover patterns and other valuable information from large data sets.
                        </p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Given the evolution of machine learning (ML), data warehousing, and the growth of big data, the adoption of data mining, also known as knowledge discovery in databases (KDD), has rapidly accelerated over the last decades. However, while this technology continuously evolves to handle data at a large scale, leaders still might face challenges with scalability and automation.
    
                                The data mining techniques that underpin data analyses can be deployed for two main purposes. They can either describe the target data set or they can predict outcomes by using machine learning algorithms.
    
                                These methods are used to organize and filter data, surfacing the most useful information, from fraud to user behaviors, bottlenecks and even security breaches. Using ML algorithms and artificial intelligence (AI) enables automation of the analysis, which can greatly speed up the process.
    
                                When combined with data analytics and visualization tools, such as Apache Spark, data mining software is becoming more straightforward and extracting relevant insights can be gained more quickly than ever. Advances in AI continue to expedite adoption across industries.
                            <p class="fw-bold">Benefits and challenges</p>
                            Benefits
    
                            Discover hidden insights and trends: Data mining takes raw data and finds order in the chaos: seeing the forest for the trees. This can result in better-informed planning across corporate functions and industries, including advertising, finance, government, healthcare, human resources (HR), manufacturing, marketing, research, sales and supply chain management (SCM).
    
                            Save budget: By analyzing performance data from multiple sources, bottlenecks in business processes can be identified to speed resolution and increase efficiency.
    
                            Solve multiple challenges: Data mining is a versatile tool. Data from almost any source and any aspect of an organization can be analyzed to discover patterns and better ways of conducting business. Almost every department in an organization that collects and analyzes data can benefit from data mining.
    
                            Challenges
    
                            Complexity and risk: Useful insights require valid data, plus experts with coding experience. Knowledge of data mining languages including Python, R and SQL is helpful. An insufficiently cautious approach to data mining might result in misleading or dangerous results. Some consumer data used in data mining might be personally identifiable information (PII) which should be handled carefully to avoid legal or public relations issues.
    
                            Cost: For the best results, a wide and deep collection of data sets is often needed. If new information is to be gathered by an organization, setting up a data pipeline might represent a new expense. If data needs to be purchased from an outside source, that also imposes a cost.
    
                            Uncertainty: First, a major data mining effort might be well run, but produce unclear results, with no major benefit. Or inaccurate data can lead to incorrect insights, whether incorrect data was selected or the preprocessing was mishandled. Other risks include modeling errors or outdated data from a rapidly changing market.
    
                            Another potential problem is results might appear valid but are in fact random and not to be trusted. It’s important to remember that “correlation is not causation.” A famous example of “data dredging”—seeing an apparent correlation and overstating its importance—was recently presented by blogger Tyler Vigen: “The price of Amazon.com stock closely matches the number of children named ‘Stevie’ from 2002 to 2022.”1 But, of course, the naming of Stevies did not influence the stock price or vice versa. Data mining applications find the patterns, but human judgment is still significant.
    
                            Data mini<br>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 10. Data science -->
                    <div class="card" id="dataScience">
                        <h3>10. Data Science</h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn1.vectorstock.com/i/1000x1000/64/65/data-science-infographic-10-option-concept-vector-27906465.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Data science is an interdisciplinary field of technology that uses algorithms and processes to gather and analyze large amounts of data to uncover patterns and insights that inform business decisions.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Data science is an in-demand career path for people with an aptitude for research, programming, math, and computers. Discover real-world applications and job opportunities in data science and what it takes to work in this exciting field.
    
                                Data science is an interdisciplinary field that uses algorithms, procedures, and processes to examine large amounts of data in order to uncover hidden patterns, generate insights, and direct decision-making. To create prediction models, data scientists use advanced machine learning algorithms to sort through, organize, and learn from structured and unstructured data.
    
                                As a fast-growing field with applications across numerous industries, data science offers a variety of job opportunities—from researching to computing. Explore how to use data science in the real world, the job outlook for the field, its required skills, and what credentials you need to land a job.
    
                            <p class="fw-bold">Applications of Data Science [Impact Across all Industries]</p>
                            Data scientists are having an impact in almost every industry.
    
                            Health care
                            eCommerce
                            Law enforcement
                            Marketing/advertising
                            Transportation
                            Sports
                            As expected, different sectors are using data science in different ways.<br>
    
                            Health care:
    
                            Identifying and predicting disease
                            Personalized health care recommendations
                            eCommerce
    
                            Automated “smart” ad placement
                            Personalized product recommendations
                            Law enforcement
    
                            Data-driven crime predictions
                            Facial recognition tools
                            Tax fraud enforcement
                            Transportation
    
                            Optimized shipping routes
                            Modeling the most effective traffic patterns and streetlight usage
                            Getting hot food delivered quickly
                            Chances are your favorite sports team may be dabbling in data science to help put together the best, most cost-effective team.
    
                            Why, data science is even at the heart of helping people find love — through online dating platforms powered by complex algorithms. <br>
    
                            <p class="fw-bold">Why is Data Science Important?</p>
    
                            Big Data may have the potential to change the world for the better but data science is essential because, according to training provider SimpliLearn, because “without the expertise of professionals who turn cutting-edge technology into actionable insights, Big Data is nothing.”
    
                            In “Why Data Science Matters And How It Powers Business Value,” the company details eight ways that data scientists can add value to business.
    
                            Empowering management to make better decisions
                            Directing actions and defining goals based on trends
                            Challenging staff to adopt best practices and focus on issues that matter
                            Identifying business opportunities
                            Decision making with quantifiable, data-driven evidence
                            Testing these decisions
                            Identifying and refining of target audiences
                            Recruiting the right talent
                            Data science has the potential to help nearly all organizations, according to Damien Deighan, CEO of Data Science Talent.
    
                            “With the ability to uncover hidden patterns, unknown correlations and build models that can make accurate predictions, data science can be used to help you make better business decisions for your organization,” says Deighan. “You can now analyze almost anything and everything in relation to your organization. Anything that can be logged via computer or network use can be analyzed and organized and turned into actionable insights. When applied and used correctly, data analytics can play a pivotal role in driving profitability and productivity.”
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 11. Deep Learning -->
                    <div class="card" id="dataScience">
                        <h3>11. Deep Learning</h3>
                        <img class="toggleImg p-3 align-center" src="https://img.freepik.com/premium-vector/deep-learning-algorithm-neural-network-ai-machine-learning-icons-infographic-design-layout-template-creative-presentation-concept-with-5-steps_159242-17895.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Deep learning is a function of AI that imitates the human brain by learning from how it structures and processes information to make decisions. Instead of relying on an algorithm that can only perform one specific task, this subset of machine learning can learn from unstructured data without supervision.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Deep learning is a method that trains computers to process information in a way that mimics human neural processes. Learn more about deep learning examples and applications in this article.
    
                            The field of artificial intelligence (AI) and machine learning (ML) is rapidly evolving, generating both fear and excitement. While many people have a general understanding of ML and AI, deep learning is a special type of machine learning that can be more challenging to describe.

You can learn more about deep learning systems and how to work with them in the following article, or start your journey with the popular course, Deep Learning Specialization from DeepLearning.AI.

    
    
                            
    
                            <p class="fw-bold">What is deep learning?</p>
                            Deep learning is a branch of machine learning that is made up of a neural network with three or more layers: <br>

Input layer: Data enters through the input layer. <br>

Hidden layers: Hidden layers process and transport data to other layers. <br>

Output layer: The final result or prediction is made in the output layer.<br>

Neural networks attempt to model human learning by digesting and analyzing massive amounts of information, also known as training data. They perform a given task with that data repeatedly, improving in accuracy each time. It's similar to the way we study and practice to improve skills. <br>
    
                            <p class="fw-bold">Deep learning models</p>
    
                            Deep learning models are files that data scientists train to perform tasks with minimal human intervention. Deep learning models include predefined sets of steps (algorithms) that tell the file how to treat certain data. This training method enables deep learning models to recognize more complicated patterns in text, images, or sounds.
                            </p>


                            <p class="fw-bold">Examples of deep learning</p>
                            Deep learning is a subset of machine learning that is made up of a neural network with three or more layers. A neural network attempts to model the human brain's behavior by learning from large data sets. Deep learning drives many AI applications that improve the way systems and tools deliver services, such as voice-enabled technology and credit card fraud detection.

Self-driving cars
Autonomous vehicles are already on our roadways. Deep learning algorithms help determine whether there are other cars, debris, or humans around and react accordingly.

Chatbots
Deep learning chatbots designed to mimic human intelligence (like Chat-GPT) have gained recent popularity due to their ability to respond to natural-language questions quickly and often accurately. The deeper the data pool from which deep learning occurs, the more rapidly deep learning can produce the desired results.

Facial recognition
Facial recognition plays an essential role in everything from tagging people on social media to crucial security measures. Deep learning allows algorithms to function accurately despite cosmetic changes such as hairstyles, beards, or poor lighting.

Medical science
The human genome consists of approximately three billion DNA base pairs of chromosomes. Machine learning is helping scientists and other medical professionals to create personalized medicines, and diagnose tumors, and is undergoing research and utilization for other pharmaceutical and medical purposes.

Speech recognition
Similar to facial recognition, deep learning uses millions of audio clips to learn and recognize speech. It can then power algorithms to understand what someone said and differentiate different tones, as well as detect a specific person's voice.


                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>




                    <!-- 12. Emergent Behaviour -->
                    <div class="card" id="dataScience">
                        <h3>12. Emergent Behavior</h3>
                        <img class="toggleImg p-3 align-center" src="https://www.digital-adoption.com/wp-content/uploads/2023/12/Risks-of-Emergent-AI-1024x497.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Emergent behavior, also called emergence, is when an AI system shows unpredictable or unintended capabilities.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Emergent behavior refers to actions or patterns that weren’t explicitly programmed into an AI system but developed as a natural outcome of its complexity and interactions.
                            Imagine a colony of ants. No single ant has the blueprint for the colony’s intricate behavior. Yet, collectively, they demonstrate complex activities like building nests or finding food. Similarly, in AI, emergent behavior occurs when simple rules or algorithms interact in a complex system, leading to outcomes that might surprise even the creators of the AI.

                            This phenomenon is significant in AI for many reasons. First, it pushes the boundaries of what AI can achieve, often leading to more efficient and adaptable systems.

It’s like giving AI a mind of its own but within the confines of its programming. Emergent behaviors can lead to AI systems solving problems in ways that were not preconceived, which can be both exciting and a bit scary.

Secondly, understanding and harnessing this emergent behavior is necessary for advancing AI technology. It’s about learning from the unexpected and using it to enhance AI capabilities. This could mean more advanced robotic systems, smarter AI in video games, or even more effective data analysis tools.

It’s not all smooth sailing, though. Emergent behavior can also pose challenges, especially in predicting and controlling AI systems. As AI becomes more integrated into important areas like healthcare, transportation, and security, we need to make sure that these emergent behaviors are understood and managed.
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 13. Generative AI -->
                    <div class="card" id="dataScience">
                        <h3>13. Generative AI</h3>
                        <img class="toggleImg p-3 align-center" src="https://ediscoverytoday.com/wp-content/uploads/2023/11/2023GenerativeAILegalUseCases.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Generative AI is a type of technology that uses AI to create content, including text, video, code and images. A generative AI system is trained using large amounts of data, so that it can find patterns for generating new content.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Generative AI refers to deep-learning models that can generate high-quality text, images, and other content based on the data they were trained on.
                            Artificial intelligence has gone through many cycles of hype, but even to skeptics, the release of ChatGPT seems to mark a turning point. OpenAI’s chatbot, powered by its latest large language model, can write poems, tell jokes, and churn out essays that look like a human created them. Prompt ChatGPT with a few words, and out comes love poems in the form of Yelp reviews, or song lyrics in the style of Nick Cave.

                            The last time generative AI loomed this large, the breakthroughs were in computer vision. Selfies transformed into Renaissance-style portraits and prematurely aged faces filled social media feeds. Five years later, it’s the leap forward in natural language processing, and the ability of large language models to riff on just about any theme, that has seized the popular imagination. And it’s not just language: Generative models can also learn the grammar of software code, molecules, natural images, and a variety of other data types.

                            The applications for this technology are growing every day, and we’re just starting to explore the possibilities. At IBM Research, we’re working to help our customers use generative models to write high-quality software code faster, discover new molecules, and train trustworthy conversational chatbots grounded on enterprise data. We’re even using generative AI to create synthetic data to build more robust and trustworthy AI models and to stand-in for real data protected by privacy and copyright laws.

                            As the field continues to evolve, we thought we’d take a step back and explain what we mean by generative AI, how we got here, and how these models work.

                            <p class="fw-bold">The rise of deep generative models</p>
                            Generative AI refers to deep-learning models that can take raw data — say, all of Wikipedia or the collected works of Rembrandt — and “learn” to generate statistically probable outputs when prompted. At a high level, generative models encode a simplified representation of their training data and draw from it to create a new work that’s similar, but not identical, to the original data.

Generative models have been used for years in statistics to analyze numerical data. The rise of deep learning, however, made it possible to extend them to images, speech, and other complex data types. Among the first class of models to achieve this cross-over feat were variational autoencoders, or VAEs, introduced in 2013. VAEs were the first deep-learning models to be widely used for generating realistic images and speech.

“VAEs opened the floodgates to deep generative modeling by making models easier to scale,” said Akash Srivastava, an expert on generative AI at the MIT-IBM Watson AI Lab. “Much of what we think of today as generative AI started here.”

Autoencoders work by encoding unlabeled data into a compressed representation, and then decoding the data back into its original form. “Plain” autoencoders were used for a variety of purposes, including reconstructing corrupted or blurry images. Variational autoencoders added the critical ability to not just reconstruct data, but to output variations on the original data.

This ability to generate novel data ignited a rapid-fire succession of new technologies, from generative adversarial networks (GANs) to diffusion models, capable of producing ever more realistic — but fake — images. In this way, VAEs set the stage for today’s generative AI.

They are built out of blocks of encoders and decoders, an architecture that also underpins today’s large language models. Encoders compress a dataset into a dense representation, arranging similar data points closer together in an abstract space. Decoders sample from this space to create something new while preserving the dataset’s most important features.

Transformers, introduced by Google in 2017 in a landmark paper “Attention Is All You Need,” combined the encoder-decoder architecture with a text-processing mechanism called attention to change how language models were trained. An encoder converts raw unannotated text into representations known as embeddings; the decoder takes these embeddings together with previous outputs of the model, and successively predicts each word in a sentence.

Through fill-in-the-blank guessing games, the encoder learns how words and sentences relate to each other, building up a powerful representation of language without anyone having to label parts of speech and other grammatical features. Transformers, in fact, can be pre-trained at the outset without a particular task in mind. Once these powerful representations are learned, the models can later be specialized — with much less data — to perform a given task.

Several innovations made this possible. Transformers processed words in a sentence all at once, allowing text to be processed in parallel, speeding up training. Earlier techniques like recurrent neural networks (RNNs) and Long Short-Term Memory (LSTM) networks processed words one by one. Transformers also learned the positions of words and their relationships, context that allowed them to infer meaning and disambiguate words like “it” in long sentences.

By eliminating the need to define a task upfront, transformers made it practical to pre-train language models on vast amounts of raw text, allowing them to grow dramatically in size. Previously, people gathered and labeled data to train one model on a specific task. With transformers, you could train one model on a massive amount of data and then adapt it to multiple tasks by fine-tuning it on a small amount of labeled task-specific data.

Transformers have come to be known as foundation models for their versatility. “If you wanted to improve a classifier, you used to have to feed it more labeled data,” said Srivastava. “Now, with foundation models, you can feed the model large amounts of unlabeled data to learn a representation that generalizes well to many tasks.”

Language transformers today are used for non-generative tasks like classification and entity extraction as well as generative tasks like translation, summarization, and question answering. More recently, transformers have stunned the world with their capacity to generate convincing dialogue, essays, and other content.

Language transformers fall into three main categories: encoder-only models, decoder-only models, and encoder-decoder models.

Encoder-only models like BERT power search engines and customer-service chatbots, including IBM’s Watson Assistant. Encoder-only models are widely used for non-generative tasks like classifying customer feedback and extracting information from long documents. In a project with NASA, IBM is building an encoder-only model to mine millions of earth-science journals for new knowledge.

Decoder-only models like the GPT family of models are trained to predict the next word without an encoded representation. GPT-3, at 175 billion parameters, was the largest language model of its kind when OpenAI released it in 2020. Other massive models — Google’s PaLM (540 billion parameters) and open-access BLOOM (176 billion parameters), among others, have since joined the scene.

Encoder-decoder models, like Google’s Text-to-Text Transfer Transformer, or T5, combine features of both BERT and GPT-style models. They can do many of the generative tasks that decoder-only models can, but their compact size makes them faster and cheaper to tune and serve.

Generative AI and large language models have been progressing at a dizzying pace, with new models, architectures, and innovations appearing almost daily.

    
            
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 14. Guardrails -->
                    <div class="card" id="dataScience">
                        <h3>14. Guardrails</h3>
                        <img class="toggleImg p-3 align-center" src="https://media.licdn.com/dms/image/v2/D4D12AQEowMU_fw0Jtw/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1708006841675?e=2147483647&v=beta&t=LGAiZ_AB2hgGNYrRHsDgogjO92AA5dTOWlqxxee3s4M" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">An AI guardrail is a safeguard that is put in place to prevent artificial intelligence (AI) from causing harm. AI guardrails are a lot like highway guardrails – they are both created to keep people safe and guide positive outcomes.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                DAs AI evolves, guardrails are becoming increasingly important for maintaining public trust in AI and ensuring that AI-enabled technology operates safely within ethical and legal boundaries.

                                Creating AI guardrails that everyone agrees with is challenging, however, due to the rapid pace of technological advancement, different legal systems around the world, and the difficulty of balancing AI innovation with the need for privacy, fairness, and public safety.
    
                            <p class="fw-bold">Why Do We Need AI Guardrails?</p>
                            AI guardrails are a critical component of AI governance and the development, deployment and use of responsible AI.

                            In the past year, guardrails have often been mentioned in the context of generative AI, but it’s important to remember that safeguards are vital considerations for any type of AI system that can make a decision autonomously. 

                            This includes relatively simple machine learning (ML) algorithms that decide between two choices, as well as multimodal AI systems whose decisions can have literally billions of potential outcomes.

                            As the world has seen, when guardrails don’t exist, AI technology can perpetuate biases, create new concerns about privacy, make erroneous or unethical decisions that directly impact people’s lives, and get misused for harmful purposes. 

                            It’s no surprise that this, in turn, has led many people to mistrust AI. 
    
                            <p class="fw-bold">Who Is Responsible For Creating AI Guardrails?</p>
    
                            The creation and implementation of AI guardrails is a collaborative effort that involves a diverse group of stakeholders. This includes:

                            Big Tech companies and AI startups;
                            Business leaders;
                            AI researchers;
                            Civic organizations;
                            Ethicists;
                            Professional organizations;
                            Government agencies;
                            Prominent global organizations;
                            Legal experts. 
                            Each of these stakeholders brings a unique perspective and skill set to the table, which in theory, should contribute to a more holistic approach to the development of guardrails for specific types of artificial intelligence.

                            The challenge is that when stakeholder interests are too diverse, it can be difficult to reconcile competing priorities and find a balance that everyone can live with. Too many guardrails can stifle innovation, and too few guardrails can leave the door open for harmful consequences that undermine safety and public trust.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 15. Hallucination -->
                    <div class="card" id="dataScience">
                        <h3>15. Hallucination</h3>
                        <img class="toggleImg p-3 align-center" src="https://www.aporia.com/wp-content/webp-express/webp-images/uploads/2024/02/image-32.png.webp" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">An AI hallucination is when an artificial intelligence (AI) model produces incorrect or misleading results. This can happen when an AI model: Has insufficient training data, Makes incorrect assumptions, Uses biased data for training, Processes data incorrectly, and Misapplies learned patterns.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            AI hallucinations are incorrect or misleading results that AI models generate. These errors can be caused by a variety of factors, including insufficient training data, incorrect assumptions made by the model, or biases in the data used to train the model. AI hallucinations can be a problem for AI systems that are used to make important decisions, such as medical diagnoses or financial trading.
                            <p class="fw-bold">How do AI hallucinations occur?</p>
                            AI models are trained on data, and they learn to make predictions by finding patterns in the data. However, the accuracy of these predictions often depends on the quality and completeness of the training data. If the training data is incomplete, biased, or otherwise flawed, the AI model may learn incorrect patterns, leading to inaccurate predictions or hallucinations.

For example, an AI model that is trained on a dataset of medical images may learn to identify cancer cells. However, if the dataset does not include any images of healthy tissue, the AI model may incorrectly predict that healthy tissue is cancerous. 

Flawed training data is just one reason why AI hallucinations can occur. Another factor that may contribute is a lack of proper grounding. An AI model may struggle to accurately understand real-world knowledge, physical properties, or factual information. This lack of grounding can cause the model to generate outputs that, while seemingly plausible, are actually factually incorrect, irrelevant, or nonsensical. This can even extend to fabricating links to web pages that never existed.

An example of this would be an AI model designed to generate summaries of news articles may produce a summary that includes details not present in the original article, or even fabricates information entirely. 

Understanding these potential causes of AI hallucinations is important for developers working with AI models. By carefully considering the quality and completeness of training data, as well as ensuring proper grounding, developers may minimize the risk of AI hallucinations and ensure the accuracy and reliability of their models. <br>
    
                            <p class="fw-bold">Examples of AI hallucinations</p>
    
                            AI hallucinations can take many different forms. Some common examples include:

Incorrect predictions: An AI model may predict that an event will occur when it is unlikely to happen. For example, an AI model that is used to predict the weather may predict that it will rain tomorrow when there is no rain in the forecast.
False positives: When working with an AI model, it may identify something as being a threat when it is not. For example, an AI model that is used to detect fraud may flag a transaction as fraudulent when it is not.
False negatives: An AI model may fail to identify something as being a threat when it is. For example, an AI model that is used to detect cancer may fail to identify a cancerous tumor.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 16. Hyperparameter -->
                    <div class="card" id="dataScience">
                        <h3>16. Hyperparameter</h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn.labellerr.com/ML%20Process/Everything%20you%20need%20to%20know%20about%20AI%20Model%20Training/Hyperparameter%20Tuning.webp" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A hyperparameter is a parameter whose value is set before the machine learning process begins. In contrast, the values of other parameters are derived via training. Algorithm hyperparameters affect the speed and quality of the learning process.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Hyperparameters in Machine learning are those parameters that are explicitly defined by the user to control the learning process. These hyperparameters are used to improve the learning of the model, and their values are set before starting the learning process of the model.
                            <p class="fw-bold">What are hyperparameters?</p>
                            In Machine Learning/Deep Learning, a model is represented by its parameters. In contrast, a training process involves selecting the best/optimal hyperparameters that are used by learning algorithms to provide the best result. So, what are these hyperparameters? The answer is, "Hyperparameters are defined as the parameters that are explicitly defined by the user to control the learning process."

                            Here the prefix "hyper" suggests that the parameters are top-level parameters that are used in controlling the learning process. The value of the Hyperparameter is selected and set by the machine learning engineer before the learning algorithm begins training the model. Hence, these are external to the model, and their values cannot be changed during the training process.

                            <p class="fw-bold">Some examples of Hyperparameters in Machine Learning</p>
    
                            The k in kNN or K-Nearest Neighbour algorithm
                            Learning rate for training a neural network
                            Train-test split ratio
                            Batch Size
                            Number of Epochs
                            Branches in Decision Tree
                            Number of clusters in Clustering Algorithm
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 17. Image recognition -->
                    <div class="card" id="dataScience">
                        <h3>17. Image recognition</h3>
                        <img class="toggleImg p-3 align-center" src="https://azati.ai/wp-content/uploads/2020/04/object-detection-800x400-1.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A hyperparameter is a parameter whose value is set before the machine learning process begins. In contrast, the values of other parameters are derived via training. Algorithm hyperparameters affect the speed and quality of the learning process.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Hyperparameters in Machine learning are those parameters that are explicitly defined by the user to control the learning process. These hyperparameters are used to improve the learning of the model, and their values are set before starting the learning process of the model.
                            <p class="fw-bold">What are hyperparameters?</p>
                            In Machine Learning/Deep Learning, a model is represented by its parameters. In contrast, a training process involves selecting the best/optimal hyperparameters that are used by learning algorithms to provide the best result. So, what are these hyperparameters? The answer is, "Hyperparameters are defined as the parameters that are explicitly defined by the user to control the learning process."

                            Here the prefix "hyper" suggests that the parameters are top-level parameters that are used in controlling the learning process. The value of the Hyperparameter is selected and set by the machine learning engineer before the learning algorithm begins training the model. Hence, these are external to the model, and their values cannot be changed during the training process.

                            <p class="fw-bold">Some examples of Hyperparameters in Machine Learning</p>
    
                            The k in kNN or K-Nearest Neighbour algorithm
                            Learning rate for training a neural network
                            Train-test split ratio
                            Batch Size
                            Number of Epochs
                            Branches in Decision Tree
                            Number of clusters in Clustering Algorithm
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 18. Large language model -->
                    <div class="card" id="dataScience">
                        <h3>18. Large language model</h3>
                        <img class="toggleImg p-3 align-center" src="https://pbs.twimg.com/media/GM9UJKOXIAAMINO.jpg:large" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A large language model (LLM) is an AI model that has been trained on large amounts of text so that it can understand language and generate human-like text.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Hyperparameters in Machine learning are those parameters that are explicitly defined by the user to control the learning process. These hyperparameters are used to improve the learning of the model, and their values are set before starting the learning process of the model.
                            <p class="fw-bold">What are LLMs?</p>
                            Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks.

                            LLMs have become a household name thanks to the role they have played in bringing generative AI to the forefront of the public interest, as well as the point on which organizations are focusing to adopt artificial intelligence across numerous business functions and use cases.

                            Outside of the enterprise context, it may seem like LLMs have arrived out of the blue along with new developments in generative AI. However, many companies, including IBM, have spent years implementing LLMs at different levels to enhance their natural language understanding (NLU) and natural language processing (NLP) capabilities. This has occurred alongside advances in machine learning, machine learning models, algorithms, neural networks and the transformer models that provide the architecture for these AI systems.

                            LLMs are a class of foundation models, which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark contrast to the idea of building and training domain specific models for each of these use cases individually, which is prohibitive under many criteria (most importantly cost and infrastructure), stifles synergies and can even lead to inferior performance.

                            LLMs represent a significant breakthrough in NLP and artificial intelligence, and are easily accessible to the public through interfaces like Open AI’s Chat GPT-3 and GPT-4, which have garnered the support of Microsoft. Other examples include Meta’s Llama models and Google’s bidirectional encoder representations from transformers (BERT/RoBERTa) and PaLM models. IBM has also recently launched its Granite model series on watsonx.ai, which has become the generative AI backbone for other IBM products like watsonx Assistant and watsonx Orchestrate. 

                            In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or code generation tasks. 

                            They are able to do this thanks to billions of parameters that enable them to capture intricate patterns in language and perform a wide array of language-related tasks. LLMs are revolutionizing applications in various fields, from chatbots and virtual assistants to content generation, research assistance and language translation.

                            As they continue to evolve and improve, LLMs are poised to reshape the way we interact with technology and access information, making them a pivotal part of the modern digital landscape.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 19. Machine learning -->
                    <div class="card" id="dataScience">
                        <h3>19. Machine learning</h3>
                        <img class="toggleImg p-3 align-center" src="https://pbs.twimg.com/media/CLyZCTBW8AA2qsV.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Machine learning is a subset of AI that incorporates aspects of computer science, mathematics, and coding. Machine learning focuses on developing algorithms and models that help machines learn from data and predict trends and behaviors, without human assistance.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Machine learning is one of the most trendy fields in technology today. It fuels the technology behind Netflix recommendations and the speech-to-text recognition on your smartphone. A mix of math, computer science, and coding, a career in machine learning requires extensive education and training to land a job as an engineer.

So, is machine learning hard to learn? You'll need to learn programming languages like Python, practice using and modifying algorithms, and keeping up with trends in AI. There are plenty of educational resources online, such as courses and specializations, to gain the skills and experience you need for a career in machine learning.

Use this guide to decide if machine learning is right for you and if it's "hard" to learn. We'll help you map out your career path in machine learning.


                            <p class="fw-bold">What is machine learning?</p>
                            Machine learning is a branch of artificial intelligence that imitates how humans learn. It is also a division of computer science that uses algorithms and data to adjust its actions as it gathers more information.

Machine learning is used in many applications we use daily. Voice-to-text technology, which iPhones and Androids use, is created with machine learning—specifically deep learning—because it analyzes speech and translates to text based on the software’s established knowledge of how audio can be interpreted as language.

                            <p class="fw-bold">The importance of machine learning</p>
    
                            Machine learning can automate simple tasks, such as data entry or compiling contact information lists into a particular format. It can also make significant technological changes, such as dynamic pricing for event tickets or public transportation delay alerts. The following explains in more detail the benefits and advantages of machine learning.

Automating manual tasks: Machine learning programs aim to automate tasks and draw conclusions from data sets more quickly than humans could by manually analyzing it. It also saves us a lot of time.

Spotting trends and patterns: Machine learning detects patterns in data and recommends actions based on those patterns. Netflix's algorithm spots patterns in your TV watching to recommend shows that you will like based on your preferences.

Range of applications: From "smart homes" to self-driving cars, machine learning informs many recent groundbreaking innovations in technology.

Constant improvement: Careful attention to an algorithm and the data sets fed into it, as well as the use of programming languages such as Python, can identify areas of improvement for a machine learning application to offer quality assurance. Adjusting an algorithm as often as possible helps uphold AI ethics to establish avoidable bias.

Rapid handling of multi-dimensional data: Machine learning applications allow us to analyze data and draw conclusions at a faster pace and a higher level of sophistication than humans can do on their own. For example, banks use AI to detect money laundering or fraud. To achieve this without machines would require too many employees, who would likely miss a significant amount of illicit activity.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 20. Natural language processing -->
                    <div class="card" id="dataScience">
                        <h3>20. Natural language processing</h3>
                        <img class="toggleImg p-3 align-center" src="https://f5b623aa.rocketcdn.me/wp-content/uploads/2022/10/How-NLP-Works-760px.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Natural language processing (NLP) is a type of AI that enables computers to understand spoken and written human language. NLP enables features like text and speech recognition on devices.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Natural language processing (NLP) is a form of artificial intelligence (AI) that allows computers to understand human language, whether it be written, spoken, or even scribbled. As AI-powered devices and services become increasingly more intertwined with our daily lives and world, so too does the impact that NLP has on ensuring a seamless human-computer experience.
                            <p class="fw-bold">Natural language techniques </p>
                            NLP encompasses a wide range of techniques to analyze human language. Some of the most common techniques you will likely encounter in the field include:

                            Sentiment analysis: An NLP technique that analyzes text to identify its sentiments, such as “positive,” “negative,” or “neutral.” Sentiment analysis is commonly used by businesses to better understand customer feedback. 

                            Summarization: An NLP technique that summarizes a longer text, in order to make it more manageable for time-sensitive readers. Some common texts that are summarized include reports and articles. 

                            Keyword extraction: An NLP technique that analyzes a text to identify the most important keywords or phrases. Keyword extraction is commonly used for search engine optimization (SEO), social media monitoring, and business intelligence purposes. 

                            Tokenization: The process of breaking characters, words, or subwords down into “tokens” that can be analyzed by a program. Tokenization undergirds common NLP tasks like word modeling, vocabulary building, and frequent word occurrence. 
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 21. Neural network -->
                    <div class="card" id="dataScience">
                        <h3>21. Neural network</h3>
                        <img class="toggleImg p-3 align-center" src="https://www.shutterstock.com/image-vector/ai-infographic-template-artificial-intelligence-260nw-2415784423.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A neural network is a deep learning technique designed to resemble the human brain’s structure. Neural networks require large data sets to perform calculations and create outputs, which enables features like speech and vision recognition.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            A neural network is a method in artificial intelligence (AI) that teaches computers to process data in a way that is inspired by the human brain. It is a type of machine learning (ML) process, called deep learning, that uses interconnected nodes or neurons in a layered structure that resembles the human brain. It creates an adaptive system that computers use to learn from their mistakes and improve continuously. Thus, artificial neural networks attempt to solve complicated problems, like summarizing documents or recognizing faces, with greater accuracy.
                            <p class="fw-bold">What are neural networks used for?</p>
                            Neural networks have several use cases across many industries, such as the following:

Medical diagnosis by medical image classification
Targeted marketing by social network filtering and behavioral data analysis
Financial predictions by processing historical data of financial instruments
Electrical load and energy demand forecasting
Process and quality control
Chemical compound identification
We give four of the important applications of neural networks below.

Computer vision
Computer vision is the ability of computers to extract information and insights from images and videos. With neural networks, computers can distinguish and recognize images similar to humans. Computer vision has several applications, such as the following:

Visual recognition in self-driving cars so they can recognize road signs and other road users
Content moderation to automatically remove unsafe or inappropriate content from image and video archives
Facial recognition to identify faces and recognize attributes like open eyes, glasses, and facial hair
Image labeling to identify brand logos, clothing, safety gear, and other image details
Speech recognition
Neural networks can analyze human speech despite varying speech patterns, pitch, tone, language, and accent. Virtual assistants like Amazon Alexa and automatic transcription software use speech recognition to do tasks like these:

Assist call center agents and automatically classify calls
Convert clinical conversations into documentation in real time
Accurately subtitle videos and meeting recordings for wider content reach
Natural language processing
Natural language processing (NLP) is the ability to process natural, human-created text. Neural networks help computers gather insights and meaning from text data and documents. NLP has several use cases, including in these functions:

Automated virtual agents and chatbots
Automatic organization and classification of written data
Business intelligence analysis of long-form documents like emails and forms
Indexing of key phrases that indicate sentiment, like positive and negative comments on social media
Document summarization and article generation for a given topic
Recommendation engines
Neural networks can track user activity to develop personalized recommendations. They can also analyze all user behavior and discover new products or services that interest a specific user. For example, Curalate, a Philadelphia-based startup, helps brands convert social media posts into sales. Brands use Curalate’s intelligent product tagging (IPT) service to automate the collection and curation of user-generated social content. IPT uses neural networks to automatically find and recommend products relevant to the user’s social media activity. Consumers don't have to hunt through online catalogs to find a specific product from a social media image. Instead, they can use Curalate’s auto product tagging to purchase the product with ease.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 22. Overfitting -->
                    <div class="card" id="dataScience">
                        <h3>22. Overfitting</h3>
                        <img class="toggleImg p-3 align-center" src="https://365datascience.com/resources/blog/x671k7dla1f-overfitting-vs-underfitting-classification-examples.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Overfitting occurs in machine learning training when the algorithm can only work on specific examples within the training data. A typical functioning AI model should be able to generalize patterns in the data to tackle new tasks.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Overfitting is an undesirable machine learning behavior that occurs when the machine learning model gives accurate predictions for training data but not for new data. When data scientists use machine learning models for making predictions, they first train the model on a known data set. Then, based on this information, the model tries to predict outcomes for new data sets. An overfit model can give inaccurate predictions and cannot perform well for all types of new data.
                            <p class="fw-bold">Why does overfitting occur?</p>
                            You only get accurate predictions if the machine learning model generalizes to all types of data within its domain. Overfitting occurs when the model cannot generalize and fits too closely to the training dataset instead. Overfitting happens due to several reasons, such as:
•    The training data size is too small and does not contain enough data samples to accurately represent all possible input data values.
•    The training data contains large amounts of irrelevant information, called noisy data.
•    The model trains for too long on a single sample set of data.
•    The model complexity is high, so it learns the noise within the training data.

Overfitting examples
Consider a use case where a machine learning model has to analyze photos and identify the ones that contain dogs in them. If the machine learning model was trained on a data set that contained majority photos showing dogs outside in parks , it may may learn to use grass as a feature for classification, and may not recognize a dog inside a room.
Another overfitting example is a machine learning algorithm that predicts a university student's academic performance and graduation outcome by analyzing several factors like family income, past academic performance, and academic qualifications of parents. However, the test data only includes candidates from a specific gender or ethnic group. In this case, overfitting causes the algorithm's prediction accuracy to drop for candidates with gender or ethnicity outside of the test dataset.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 23. Pattern recognition -->
                    <div class="card" id="dataScience">
                        <h3>23. Pattern recognition</h3>
                        <img class="toggleImg p-3 align-center" src="https://aiperceiver.com/wp-content/uploads/2024/06/Benefits-of-Pattern-Recognition-in-AI-1024x1024.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Pattern is everything around in this digital world. A pattern can either be seen physically or it can be observed mathematically by applying algorithms. 

                        Example: The colors on the clothes, speech pattern, etc. In computer science, a pattern is represented using vector feature values. </p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Pattern recognition is the method of using computer algorithms to analyze, detect, and label regularities in data. This informs how the data gets classified into different categories.
                            <p class="fw-bold">What is Pattern Recognition? </p>
                            Pattern recognition is the process of recognizing patterns by using a machine learning algorithm. Pattern recognition can be defined as the classification of data based on knowledge already gained or on statistical information extracted from patterns and/or their representation. One of the important aspects of pattern recognition is its application potential. 

Examples: Speech recognition, speaker identification, multimedia document recognition (MDR), automatic medical diagnosis. 
In a typical pattern recognition application, the raw data is processed and converted into a form that is amenable for a machine to use. Pattern recognition involves the classification and cluster of patterns. 

In classification, an appropriate class label is assigned to a pattern based on an abstraction that is generated using a set of training patterns or domain knowledge. Classification is used in supervised learning.
Clustering generated a partition of the data which helps decision making, the specific decision-making activity of interest to us. Clustering is used in unsupervised learning.
Features may be represented as continuous, discrete, or discrete binary variables. A feature is a function of one or more measurements, computed so that it quantifies some significant characteristics of the object. 

Example: consider our face then eyes, ears, nose, etc are features of the face. 
A set of features that are taken together, forms the features vector. 

Example: In the above example of a face, if all the features (eyes, ears, nose, etc) are taken together then the sequence is a feature vector([eyes, ears, nose]). The feature vector is the sequence of a feature represented as a d-dimensional column vector. In the case of speech, MFCC (Mel-frequency Cepstral Coefficient) is the spectral feature of the speech. The sequence of the first 13 features forms a feature vector. 
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 24. Predictive analytics -->
                    <div class="card" id="dataScience">
                        <h3>24. Predictive analytics</h3>
                        <img class="toggleImg p-3 align-center" src="https://www.tierpoint.com/wp-content/uploads/2024/02/Predictive-AI-Techniques-1024x627.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Predictive analytics is a type of analytics that uses technology to predict what will happen in a specific time frame based on historical data and patterns.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Predictive analytics is one of the four key types of data analytics, and typically forecasts what will happen in the future, such as how sales will shift during different seasons or how consumers will respond to a change in price. Businesses often use predictive analytics to make data-driven decisions and optimize outcomes.
                            <p class="fw-bold">What is predictive analytics?</p>
                            Businesses use data to understand what's happening—both now and in the future. Predictive analytics falls under the latter category. It uses historical data to predict potential future events or behaviors so companies can better position themselves in the present.

In order to calculate the future, predictive analytics relies on a number of techniques from statistics, data analytics, artificial intelligence (AI), and machine learning. Some common business applications include detecting fraud, predicting customer behavior, and forecasting demand.
                            <p class="fw-bold">Benefits of predictive analytics</p>
    
                            Predictive analytics can help businesses make stronger, more informed decisions. It can help you identify patterns and trends within data that enable different business functions to make a probabilistic determination about future events. Other benefits include:

Decision making: Improve how a business function makes decisions by relying on data to determine potential outcomes

Risk management: Develop risk management strategies for potential risks and prioritize the most detrimental risks

Customer insights: Better understand potential customers and what they need so that you can develop more specific marketing campaigns to reach them

Operational efficiency: Make companies operate more efficiently by turning to historical data to understand resources and better manage them
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 25. Prompt Engineering -->
                    <div class="card" id="dataScience">
                        <h3>25. Prompt Engineering</h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn-infographic.pressidium.com/wp-content/uploads/2024/06/Types-Of-Prompts-For-AI-Creativity-960x720.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Prompt engineering is the art and science of designing and optimizing prompts to guide AI models, particularly LLMs, towards generating the desired responses.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            In the context of AI, a prompt is the input you provide to the model to elicit a specific response. This can take various forms, ranging from simple questions or keywords to complex instructions, code snippets, or even creative writing samples. The effectiveness of your prompt directly influences the quality and relevance of the AI's output.

                            <p class="fw-bold">What do you need for prompt engineering?</p>
                            Several key elements contribute to effective prompt engineering. Mastering these allows you to communicate effectively with AI models and unlock their full potential.

Prompt format
The structure and style of your prompt play a significant role in guiding the AI's response. Different models may respond better to specific formats, such as:

The format of your prompt plays a significant role in how the AI interprets your request. Different models may respond better to specific formats, such as natural language questions, direct commands, or structured inputs with specific fields. Understanding the model's capabilities and preferred format is essential for crafting effective prompts.

Context and examples
Providing context and relevant examples within your prompt helps the AI understand the desired task and generate more accurate and relevant outputs. For instance, if you're looking for a creative story, including a few sentences describing the desired tone or theme can significantly improve the results.

Fine-tuning and adapting
Fine-tuning the AI model on specific tasks or domains using tailored prompts can enhance its performance. Additionally, adapting prompts based on user feedback or model outputs can further improve the model's responses over time.

Multi-turn conversations
Designing prompts for multi-turn conversations allows users to engage in continuous and context-aware interactions with the AI model, enhancing the overall user experience.
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 26. Quantum computing -->
                    <div class="card" id="dataScience">
                        <h3>26. Quantum computing</h3>
                        <img class="toggleImg p-3 align-center" src="https://img.freepik.com/premium-vector/quantum-computing-with-engineers-physics-infographics_1268-7705.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Quantum computing is the process of using quantum-mechanical phenomena such as entanglement and superposition to perform calculations. Quantum machine learning uses these algorithms on quantum computers to expedite work because it performs much faster than a classic machine learning program and computer.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <!-- <p>Quantum computing is the process of using quantum-mechanical phenomena such as entanglement and superposition to perform calculations. Quantum machine learning uses these algorithms on quantum computers to expedite work because it performs much faster than a classic machine learning program and computer. -->
                            <p class="fw-bold">What is quantum machine learning? </p>
                            Quantum machine learning uses algorithms run on quantum devices, such as quantum computers, to supplement, expedite, or support the work performed by a classical machine learning program. Also called quantum-enhanced machine learning, quantum machine learning leverages the information processing power of quantum technologies to enhance and speed up the work performed by a machine learning model.

While classical computers are constrained by limited storage and processing capacities, quantum-enabled ones allow for exponentially more storage and processing power. This ability to store and process huge amounts of information means that quantum computers can analyze massive data sets that would take classical methods significantly longer to perform. As a result, quantum machine learning leverages this out-sized processing power to expedite and improve the development of machine learning models, neural networks, and other forms of artificial intelligence (AI).
                            <p class="fw-bold">Quantum machine learning uses </p>
    
                            From crunching massive amounts of big data to powering transformative technological advances, both quantum computing and machine learning stand to make waves in the future. While quantum machine learning is still in its infancy, researchers and professionals are already using it in numerous ways. Some of these applications include to: 

Develop new machine learning algorithms
Speed up already existing machine learning algorithms 

Employ quantum-enhanced reinforcement learning, in which a machine learning algorithm learns based on its interactions within a quantum environment 

Create quantum neural networks, which can operate at fewer steps and with greater processing speed than traditional neural networks.

Despite these intriguing applications, though, the field of quantum computing and machine learning is still growing and changing. As a result, many other applications used to solve real-world problems will likely develop in the near and distant future. 
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 27. Reinforcement learning -->
                    <div class="card" id="dataScience">
                        <h3>27. Reinforcement learning</h3>
                        <img class="toggleImg p-3 align-center" src="https://pbs.twimg.com/media/CLyZCTBW8AA2qsV.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Reinforcement learning is a type of machine learning in which an algorithm learns by interacting with its environment and then is either rewarded or penalized based on its actions.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals. Software actions that work towards your goal are reinforced, while actions that detract from the goal are ignored. 

                            RL algorithms use a reward-and-punishment paradigm as they process data. They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes. The algorithms are also capable of delayed gratification. The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way. RL is a powerful method to help artificial intelligence (AI) systems achieve optimal outcomes in unseen environments.
                            <p class="fw-bold">What are the benefits of reinforcement learning?</p>
                            There are many benefits to using reinforcement learning (RL). However, these three often stand out.

                            Excels in complex environments
                            RL algorithms can be used in complex environments with many rules and dependencies. In the same environment, a human may not be capable of determining the best path to take, even with superior knowledge of the environment. Instead, model-free RL algorithms adapt quickly to continuously changing environments and find new strategies to optimize results.

                            Requires less human interaction
                            In traditional ML algorithms, humans must label data pairs to direct the algorithm. When you use an RL algorithm, this isn’t necessary. It learns by itself. At the same time, it offers mechanisms to integrate human feedback, allowing for systems that adapt to human preferences, expertise, and corrections.

                            Optimizes for long-term goals
                            RL inherently focuses on long-term reward maximization, which makes it apt for scenarios where actions have prolonged consequences. It is particularly well-suited for real-world situations where feedback isn't immediately available for every step, since it can learn from delayed rewards.

                            For example, decisions about energy consumption or storage might have long-term consequences. RL can be used to optimize long-term energy efficiency and cost. With appropriate architectures, RL agents can also generalize their learned strategies across similar but not identical tasks.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 28. Sentiment analysis -->
                    <div class="card" id="dataScience">
                        <h3>28. Sentiment analysis</h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn.shopify.com/s/files/1/1905/9639/files/Benefits_of_AI-based_sentiment_analysis_-_Lucent_innovation_2048x2048.png?v=1705062130" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Sentiment analysis is a popular task in natural language processing. The goal of sentiment analysis is to classify the text based on the mood or mentality expressed in the text, which can be positive negative, or neutral.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Sentiment analysis is the process of classifying whether a block of text is positive, negative, or neutral. The goal that Sentiment mining tries to gain is to be analysed people’s opinions in a way that can help businesses expand. It focuses not only on polarity (positive, negative & neutral) but also on emotions (happy, sad, angry, etc.). It uses various Natural Language Processing algorithms such as Rule-based, Automatic, and Hybrid.
                            <p class="fw-bold">Why is Sentiment Analysis Important?</p>
                            Sentiment analysis is the contextual meaning of words that indicates the social sentiment of a brand and also helps the business to determine whether the product they are manufacturing is going to make a demand in the market or not.

According to the survey,80% of the world’s data is unstructured. The data needs to be analyzed and be in a structured manner whether it is in the form of emails, texts, documents, articles, and many more.

Sentiment Analysis is required as it stores data in an efficient, cost friendly.
Sentiment analysis solves real-time issues and can help you solve all real-time scenarios.
Here are some key reasons why sentiment analysis is important for business:

Customer Feedback Analysis: Businesses can analyze customer reviews, comments, and feedback to understand the sentiment behind them helping in identifying areas for improvement and addressing customer concerns, ultimately enhancing customer satisfaction.
Brand Reputation Management: Sentiment analysis allows businesses to monitor their brand reputation in real-time.
By tracking mentions and sentiments on social media, review platforms, and other online channels, companies can respond promptly to both positive and negative sentiments, mitigating potential damage to their brand.
Product Development and Innovation: Understanding customer sentiment helps identify features and aspects of their products or services that are well-received or need improvement. This information is invaluable for product development and innovation, enabling companies to align their offerings with customer preferences.
Competitor Analysis: Sentiment Analysis can be used to compare the sentiment around a company’s products or services with those of competitors.
Businesses identify their strengths and weaknesses relative to competitors, allowing for strategic decision-making.
Marketing Campaign Effectiveness
Businesses can evaluate the success of their marketing campaigns by analyzing the sentiment of online discussions and social media mentions.
Positive sentiment indicates that the campaign is resonating with the target audience, while negative sentiment may signal the need for adjustments.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    
                    <!-- 29. Structured data -->
                    <div class="card" id="dataScience">
                        <h3>29. Structured data</h3>
                        <img class="toggleImg p-3 align-center" src="https://www.expert.ai/wp-content/uploads/2020/10/StructuredVsUnstructuredData-300x234.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Structured data is data that has a standardized format for efficient access by software and humans alike. It is typically tabular with rows and columns that clearly define data attributes. Computers can effectively process structured data for insights due to its quantitative nature.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            When it comes to data, files can come in many different forms. There are two main types of data—structured and unstructured. Each is sourced and collected in different ways, living on different types of databases, so their differences are important for data professionals.

But, how do they differ from one another exactly? And, why would you want to use one over the other?
                            <p class="fw-bold">Structured vs. unstructured data</p>
                            The main difference is that structured data is defined and searchable. This includes data like dates, phone numbers, and product SKUs. Unstructured data is everything else, which is more difficult to categorize or search, like photos, videos, podcasts, social media posts, and emails. Most of the data in the world is unstructured data.
                            <p class="fw-bold">What is structured data?</p>
                            TStructured data is typically quantitative data that is organized and easily searchable. The programming language Structured Query Language (SQL) is used in a relational database to “query” to input and search within structured data. 

Common types of structured data include names, addresses, credit card numbers, telephone numbers, star ratings from customers, bank information, and other data that can be easily searched using SQL. 

Structured data examples
In the real world, structured data could be used for things like:

Booking a flight: Flight and reservation data, such as dates, prices, and destinations, fit neatly within the Excel spreadsheet format. When you book a flight, this information is stored in a database.

Customer relationship management (CRM): CRM software such as Salesforce runs structured data through analytical tools to create new data sets for businesses to analyze customer behavior and preferences.

                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 30. Supervised learning -->
                    <div class="card" id="dataScience">
                        <h3>30. Supervised learning</h3>
                        <img class="toggleImg p-3 align-center" src="https://i.redd.it/tyjjqjswi5621.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Supervised learning is a type of machine learning in which classified output data is used to train the machine and produce the correct algorithms. It is much more common than unsupervised learning.
</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Supervised learning is a category of machine learning that uses labeled datasets to train algorithms to predict outcomes and recognize patterns. Unlike unsupervised learning, supervised learning algorithms are given labeled training to learn the relationship between the input and the outputs. 

Supervised machine learning algorithms make it easier for organizations to create complex models that can make accurate predictions. As a result, they are widely used across various industries and fields, including healthcare, marketing, financial services, and more. 
                            <p class="fw-bold">How does supervised learning work?</p>
                            The data used in supervised learning is labeled — meaning that it contains examples of both inputs (called features) and correct outputs (labels). The algorithms analyze a large dataset of these training pairs to infer what a desired output value would be when asked to make a prediction on new data.

For instance, let’s pretend you want to teach a model to identify pictures of trees. You provide a labeled dataset that contains many different examples of types of trees and the names of each species. You let the algorithm try to define what set of characteristics belongs to each tree based on the labeled outputs. You can then test the model by showing it a tree picture and asking it to guess what species it is. If the model provides an incorrect answer, you can continue training it and adjusting its parameters with more examples to improve its accuracy and minimize errors. 

Once the model has been trained and tested, you can use it to make predictions on unknown data based on the previous knowledge it has learned.

                            <p class="fw-bold">Types of supervised learning</p>
                            Supervised learning in machine learning is generally divided into two categories: classification and regression. 

Classification
Classification algorithms are used to group data by predicting a categorical label or output variable based on the input data. Classification is used when output variables are categorical, meaning there are two or more classes.

One of the most common examples of classification algorithms in use is the spam filter in your email inbox. Here, a supervised learning model is trained to predict whether an email is spam or not with a dataset that contains labeled examples of both spam and legitimate emails. The algorithm extracts information about each email, including the sender, the subject line, body copy, and more. It then uses these features and corresponding output labels to learn patterns and assign a score that indicates whether an email is real or spam.

Regression
Regression algorithms are used to predict a real or continuous value, where the algorithm detects a relationship between two or more variables. 

A common example of a regression task might be predicting a salary based on work experience. For instance, a supervised learning algorithm would be fed inputs related to work experience (e.g., length of time, the industry or field, location, etc.) and the corresponding assigned salary amount. After the model is trained, it could be used to predict the average salary based on work experience.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 31. Token -->
                    <div class="card" id="dataScience">
                        <h3>31. Token</h3>
                        <img class="toggleImg p-3 align-center" src="https://d230m64oxp1vr8.cloudfront.net/blogs/codeZeros954hb9sh1712984901598.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A token is a basic unit of text that an LLM uses to understand and generate language. A token may be an entire word or parts of a word.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            It's a term that floats around the realm of Generative AI, often leaving many scratching their heads. Far from the realm of cryptocurrency or reward systems, in the world of Artificial Intelligence, tokens play a pivotal role in understanding and generating human-like text. <br>
                            At its core, a token is the smallest unit into which text data can be broken down for an AI model to process. Think of it as similar to how we might break sentences into words or characters. However, for AI, especially in the context of language models, these tokens can represent a character, a word, or even larger chunks of text like phrases, depending on the model and its configuration.

                            <p class="fw-bold">Why Are Tokens Important?</p>
                            Data Representation: Tokens serve as the bridge between raw human language and a format that AI models can process. Every token is converted into a numerical format (often a high-dimensional vector) using embeddings. These numerical representations capture the semantic essence of the token and can be processed by neural networks.

Memory and Computation: AI models, especially large ones, have a fixed number of tokens they can handle in one go, known as their "context window" or "attention span". By understanding the nature and number of tokens, developers can effectively interact with the model and structure the input to ensure optimal performance.

Granularity and Flexibility: Since tokens can represent varying sizes of text chunks, they provide flexibility. For example, a model designed to operate on word-level tokens might be ideal for certain languages or applications, while character-level tokens might be more suited for others.


                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 32. Training data -->
                    <div class="card" id="dataScience">
                        <h3>32. Training data</h3>
                        <img class="toggleImg p-3 align-center" src="https://www.cognilytica.com/wp-content/uploads/2022/08/The-Steps-for-an-AI-Project.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Training data is the information or examples given to an AI system to enable it to learn, find patterns, and create new content.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            In the world of artificial intelligence and machine learning, data training is inevitable. This is the process that makes machine learning modules accurate, efficient and fully functional. In this post, we explore in detail what AI training data is, training data quality, data collection & licensing and more.

It is estimated that on average adult makes decisions on life and everyday things based on past learning. These, in turn, come from life experiences shaped by situations and people. In the literal sense, situations, instances, and people are nothing but data that gets fed into our minds. As we accumulate years of data in the form of experience, the human mind tends to make seamless decisions.
                            <p class="fw-bold">What does this convey? That data is inevitable in learning.</p>
                            Similar to how a child needs a label called an alphabet to understand the letters A, B, C, D a machine also needs to understand the data it is receiving.

That’s exactly what Artificial Intelligence (AI) training is all about. A machine is no different than a child who has yet to learn things from what they are about to be taught. The machine does not know to differentiate between a cat and a dog or a bus and a car because they haven’t yet experienced those items or been taught what they look like.

So, for someone building a self-driving car, the primary function that needs to be added is the system’s ability to understand all the everyday elements the car may encounter, so the vehicle can identify them and make appropriate driving decisions. This is where AI training data comes into play. 

Today, artificial intelligence modules offer us many conveniences in the form of recommendation engines, navigation, automation, and more. All of that happens due to AI data training that was used to train the algorithms while they were built.

AI training data is a fundamental process in building machine learning and AI algorithms. If you are developing an app that is based on these tech concepts, you need to train your systems to understand data elements for optimized processing. Without training, your AI model will be inefficient, flawed and potentially pointless.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 33. Transfer learning -->
                    <div class="card" id="dataScience">
                        <h3>33. Transfer learning</h3>
                        <img class="toggleImg p-3 align-center" src="https://d1krbhyfejrtpz.cloudfront.net/blog/wp-content/uploads/2024/02/19161749/Infographic2-10-1024x526.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Transfer learning is a machine learning system that takes existing, previously learned data and applies it to new tasks and activities.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Transfer learning is a machine learning technique where a model trained on one task is repurposed as the foundation for a second task. This approach is beneficial when the second task is related to the first or when data for the second task is limited. Leveraging learned features from the initial task, the model can adapt more efficiently to the new task, accelerating learning and improving performance. Transfer learning also reduces overfitting risk, as the model already incorporates generalizable features useful for the second task.
                            <p class="fw-bold">Why is Transfer Learning Important?</p>
                            Transfer learning is a critical technique in machine learning, offering solutions to key challenges:

                            Limited Data: Acquiring extensive labeled data is often challenging and costly. Transfer learning enables us to use pre-trained models, reducing the dependency on large datasets.
                            Enhanced Performance: Starting with a pre-trained model, which has already learned from substantial data, allows for faster and more accurate results on new tasks—ideal for applications needing high accuracy and efficiency.
                            Time and Cost Efficiency: Transfer learning shortens training time and conserves resources by utilizing existing models, eliminating the need for training from scratch.
                            Adaptability: Models trained on one task can be fine-tuned for related tasks, making transfer learning versatile for various applications, from image recognition to natural language processing.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 28. Topic -->
                    <div class="card" id="dataScience">
                        <h3>28. Topic</h3>
                        <img class="toggleImg p-3 align-center" src="" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">initial short</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            info
                            <p class="fw-bold">What are the benefits of reinforcement learning?</p>
                            content
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 28. Topic -->
                    <div class="card" id="dataScience">
                        <h3>28. Topic</h3>
                        <img class="toggleImg p-3 align-center" src="" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">initial short</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            info
                            <p class="fw-bold">What are the benefits of reinforcement learning?</p>
                            content
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 28. Topic -->
                    <div class="card" id="dataScience">
                        <h3>28. Topic</h3>
                        <img class="toggleImg p-3 align-center" src="" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">initial short</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            info
                            <p class="fw-bold">What are the benefits of reinforcement learning?</p>
                            content
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 28. Topic -->
                    <div class="card" id="dataScience">
                        <h3>28. Topic</h3>
                        <img class="toggleImg p-3 align-center" src="" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">initial short</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            info
                            <p class="fw-bold">What are the benefits of reinforcement learning?</p>
                            content
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 28. Topic -->
                    <div class="card" id="dataScience">
                        <h3>28. Topic</h3>
                        <img class="toggleImg p-3 align-center" src="" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">initial short</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            info
                            <p class="fw-bold">What are the benefits of reinforcement learning?</p>
                            content
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 28. Topic -->
                    <div class="card" id="dataScience">
                        <h3>28. Topic</h3>
                        <img class="toggleImg p-3 align-center" src="" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">initial short</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            info
                            <p class="fw-bold">What are the benefits of reinforcement learning?</p>
                            content
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>



                </div>
            </div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    <script src="script.js"></script>
</body>

</html>