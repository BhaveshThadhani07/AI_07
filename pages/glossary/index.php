<?php
?>
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AINexus</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="style.css">
</head>

<body>

    <?php include '../../partials/_navbar/navbar.php'; ?>
    <div class="container">

        <div class="main">
            
        
            <div class="middle">
                <div class="above-title">
                    <h1 class="text-dark">Artificial Intelligence Terms: A to Z<br> Glossary</h1>
                    <img src="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://images.ctfassets.net/wp1lcwdav1p1/Otc07JBoEZ2f0QA7E4k17/0cd37a4956703c4cac5188e5e7503b03/AI_Glossary_Terms.png?w=1500&h=680&q=60&fit=fill&f=faces&fm=jpg&fl=progressive&auto=format%2Ccompress&dpr=1&w=1000" alt="">
                    <p>
                        Embarking on the journey into artificial intelligence (AI) opens up a world of innovation and potential. As a student exploring AI, understanding core terms and concepts is a powerful first step. This glossary provides an easy-to-understand guide to key AI topics, covering everything from machine learning basics to advanced topics like neural networks and natural language processing (NLP). <br> <br>
    
                        This list is designed to help you build a strong foundation in AI vocabulary. Whether you're preparing for an academic project, drafting your resume, or simply satisfying your curiosity about AI, these terms will help you feel confident and well-prepared. <br> <br>
    
                        AI is shaping our future, and learning its language is a great way to start contributing to the field. Dive into these essential AI terms and get ready for an exciting journey into technology’s next frontier!
                    </p>
                    <h2>AI Terms</h2>
                </div>
                <div class="main-content">
    
                    <!-- 01. AI -->
                    <div class="card" id="artificialIntelligence">
                        <h3>01. Artificial Intelligence</h3>
                        <img src="https://fpf.org/wp-content/uploads/2020/12/AI-inforgraphic.jpg" alt="AI Infographic" class="toggleImg p-3 align-center" height="600px" width="100%" style="display: none;">
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">AI stands for artificial intelligence, which is the simulation of human intelligence processes by machines or computer systems. AI can mimic human capabilities such as communication, learning, and decision-making.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>Artificial intelligence is a field of science concerned with building computers and machines that can reason, learn, and act in such a way that would normally require human intelligence or that involves data whose scale exceeds what humans can analyze. <br>
    
                                AI is a broad field that encompasses many different disciplines, including computer science, data analytics and statistics, hardware and software engineering, linguistics, neuroscience, and even philosophy and psychology. <br>
    
                                On an operational level for business use, AI is a set of technologies that are based primarily on machine learning and deep learning, used for data analytics, predictions and forecasting, object categorization, natural language processing, recommendations, intelligent data retrieval, and more. <br>
    
                            <p class="fw-bold">How does AI work?</p>
                            While the specifics vary across different AI techniques, the core principle revolves around data. AI systems learn and improve through exposure to vast amounts of data, identifying patterns and relationships that humans may miss. <br>
    
                            This learning process often involves algorithms, which are sets of rules or instructions that guide the AI's analysis and decision-making. In machine learning, a popular subset of AI, algorithms are trained on labeled or unlabeled data to make predictions or categorize information. <br>
    
                            Deep learning, a further specialization, utilizes artificial neural networks with multiple layers to process information, mimicking the structure and function of the human brain. Through continuous learning and adaptation, AI systems become increasingly adept at performing specific tasks, from recognizing images to translating languages and beyond. <br>
    
                            <p class="fw-bold">Types of artificial intelligence
                                Artificial intelligence can be organized in several ways, depending on stages of development or actions being performed.</p>
    
                            For instance, four stages of AI development are commonly recognized. <br>
    
                            Reactive machines: Limited AI that only reacts to different kinds of stimuli based on preprogrammed rules. Does not use memory and thus cannot learn with new data. IBM’s Deep Blue that beat chess champion Garry Kasparov in 1997 was an example of a reactive machine. <br>
                            Limited memory: Most modern AI is considered to be limited memory. It can use memory to improve over time by being trained with new data, typically through an artificial neural network or other training model. Deep learning, a subset of machine learning, is considered limited memory artificial intelligence. <br>
                            Theory of mind: Theory of mind AI does not currently exist, but research is ongoing into its possibilities. It describes AI that can emulate the human mind and has decision-making capabilities equal to that of a human, including recognizing and remembering emotions and reacting in social situations as a human would. <br>
                            Self aware: A step above theory of mind AI, self-aware AI describes a mythical machine that is aware of its own existence and has the intellectual and emotional capabilities of a human. Like theory of mind AI, self-aware AI does not currently exist. <br>
                            A more useful way of broadly categorizing types of artificial intelligence is by what the machine can do. All of what we currently call artificial intelligence is considered artificial “narrow” intelligence, in that it can perform only narrow sets of actions based on its programming and training. For instance, an AI algorithm that is used for object classification won’t be able to perform natural language processing. Google Search is a form of narrow AI, as is predictive analytics, or virtual assistants. <br>
    
                            Artificial general intelligence (AGI) would be the ability for a machine to “sense, think, and act” just like a human. AGI does not currently exist. The next level would be artificial superintelligence (ASI), in which the machine would be able to function in all ways superior to a human. <br> </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 02. AIEthics -->
                    <div class="card" id="aiEthics">
                        <h3>02. AI Ethics</h3>
                        <img class="toggleImg p-3 align-center" src="https://www.orientsoftware.com/Themes/Content/Images/blog/2022-04-14/ethics-in-ai-2.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">AI ethics refers to the issues that AI stakeholders such as engineers and government officials must consider to ensure that the technology is developed and used responsibly. This means adopting and implementing systems that support a safe, secure, unbiased, and environmentally friendly approach to artificial intelligence.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>As artificial intelligence (AI) becomes increasingly important to society, experts in the field have identified a need for ethical boundaries when it comes to creating and implementing new AI tools. Although there's currently no wide-scale governing body to write and enforce these rules, many technology companies have adopted their own version of AI ethics or an AI code of conduct.
    
                                AI ethics are the moral principles that companies use to guide responsible and fair development and use of AI. In this article, we'll explore what ethics in AI are, why they matter, and some challenges and benefits of developing an AI code of conduct. <br>
    
                            <p class="fw-bold">What are AI ethics?</p>
                            AI ethics are the set of guiding principles that stakeholders (from engineers to government officials) use to ensure artificial intelligence technology is developed and used responsibly. This means taking a safe, secure, humane, and environmentally friendly approach to AI.
    
                            A strong AI code of ethics can include avoiding bias, ensuring privacy of users and their data, and mitigating environmental risks. Codes of ethics in companies and government-led regulatory frameworks are two main ways that AI ethics can be implemented. By covering global and national ethical AI issues, and laying the policy groundwork for ethical AI in companies, both approaches help regulate AI technology.
    
                            More broadly, discussion around AI ethics has progressed from being centered around academic research and non-profit organizations. Today, big tech companies like IBM, Google, and Meta have assembled teams to tackle ethical issues that arise from collecting massive amounts of data. At the same time, government and intergovernmental entities have begun to devise regulations and ethics policy based on academic research.
    
                            <p class="fw-bold">Stakeholders in AI ethics</p>
    
                            Developing ethical principles for responsible AI use and development requires industry actors to work together. Stakeholders must examine how social, economic, and political issues intersect with AI, and determine how machines and humans can coexist harmoniously.
    
                            Each of these actors play an important role in ensuring less bias and risk for AI technologies.
    
    
                            Academics: Researchers and professors are responsible for developing theory-based statistics, research, and ideas that can support governments, corporations, and non-profit organizations.
    
    
                            Government: Agencies and committees within a government can help facilitate AI ethics in a nation. A good example of this is the Preparing for the Future of Artificial Intelligence report that was developed by the National Science and Technology Council (NSTC) in 2016, which outlines AI and its relationship to public outreach, regulation, governance, economy, and security.
    
    
                            Intergovernmental entities: Entities like the United Nations and the World Bank are responsible for raising awareness and drafting agreements for AI ethics globally. For example, UNESCO’s 193 member states adopted the first ever global agreement on the Ethics of AI in November 2021 to promote human rights and dignity.
    
    
                            Non-profit organizations: Non-profit organizations like Black in AI and Queer in AI help diverse groups gain representation within AI technology. The Future of Life Institute created 23 guidelines that are now the Asilomar AI Principles, which outline specific risks, challenges, outcomes for AI technologies.
    
    
                            Private companies: Executives at Google, Meta, and other tech companies, as well as banking, consulting, health care, and other industries within the private sector that uses AI technology, are responsible for creating ethics teams and codes of conduct. This often creates a standard for companies to follow suit. <br> </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 03. Algorithm -->
                    <div class="card" id="algorithm">
                        <h3>03. Algorithm</h3>
                        <img class="toggleImg p-3 align-center" src="https://www.techtarget.com/rms/onlineimages/types_of_algorithms-f_mobile.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">An algorithm is a sequence of rules given to an AI machine to perform a task or solve a problem. Common algorithms include classification, regression, and clustering.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>At the core of machine learning are algorithms, which are trained to become the machine learning models used to power some of the most impactful innovations in the world today. In this article, you'll learn about 10 of the most popular machine learning algorithms that you'll want to know, and explore the different learning styles used to turn machine learning algorithms into functioning machine learning models. <br>
    
                            <p class="fw-bold">10 machine learning algorithms to know</p>
    
    
    
                            1. Linear regression
                            Linear regression is a supervised machine learning technique used for predicting and forecasting values that fall within a continuous range, such as sales numbers or housing prices. It is a technique derived from statistics and is commonly used to establish a relationship between an input variable (X) and an output variable (Y) that can be represented by a straight line.
    
                            In simple terms, linear regression takes a set of data points with known input and output values and finds the line that best fits those points. This line, known as the "regression line," serves as a predictive model. By using this line, we can estimate or predict the output value (Y) for a given input value (X).
    
                            Linear regression is primarily used for predictive modeling rather than categorization. It is useful when we want to understand how changes in the input variable affect the output variable. By analyzing the slope and intercept of the regression line, we can gain insights into the relationship between the variables and make predictions based on this understanding.
    
                            2. Logistic regression
                            Logistic regression, also known as "logit regression," is a supervised learning algorithm primarily used for binary classification tasks. It is commonly employed when we want to determine whether an input belongs to one class or another, such as deciding whether an image is a cat or not a cat.
    
                            Logistic regression predicts the probability that an input can be categorized into a single primary class. However, in practice, it is commonly used to group outputs into two categories: the primary class and not the primary class. To accomplish this, logistic regression creates a threshold or boundary for binary classification. For example, any output value between 0 and 0.49 might be classified as one group, while values between 0.50 and 1.00 would be classified as the other group.
    
                            Consequently, logistic regression is typically used for binary categorization rather than predictive modeling. It enables us to assign input data to one of two classes based on the probability estimate and a defined threshold. This makes logistic regression a powerful tool for tasks such as image recognition, spam email detection, or medical diagnosis where we need to categorize data into distinct classes.
    
                            3. Naive Bayes
                            Naive Bayes is a set of supervised learning algorithms used to create predictive models for binary or multi-classification tasks. It is based on Bayes' Theorem and operates on conditional probabilities, which estimate the likelihood of a classification based on the combined factors while assuming independence between them.
    
                            Let's consider a program that identifies plants using a Naive Bayes algorithm. The algorithm takes into account specific factors such as perceived size, color, and shape to categorize images of plants. Although each of these factors is considered independently, the algorithm combines them to assess the probability of an object being a particular plant.
    
                            Naive Bayes leverages the assumption of independence among the factors, which simplifies the calculations and allows the algorithm to work efficiently with large datasets. It is particularly well-suited for tasks like document classification, email spam filtering, sentiment analysis, and many other applications where the factors can be considered separately but still contribute to the overall classification.
    
                            4. Decision tree
                            A decision tree is a supervised learning algorithm used for classification and predictive modeling tasks. It resembles a flowchart, starting with a root node that asks a specific question about the data. Based on the answer, the data is directed down different branches to subsequent internal nodes, which ask further questions and guide the data to subsequent branches. This process continues until the data reaches an end node, also known as a leaf node, where no further branching occurs.
    
                            Decision tree algorithms are popular in machine learning because they can handle complex datasets with ease and simplicity. The algorithm's structure makes it straightforward to understand and interpret the decision-making process. By asking a sequence of questions and following the corresponding branches, decision trees enable us to classify or predict outcomes based on the data's characteristics.
    
                            This simplicity and interpretability make decision trees valuable for various applications in machine learning, especially when dealing with complex datasets.
    
                            5. Random forest
                            A random forest algorithm is an ensemble of decision trees used for classification and predictive modeling. Instead of relying on a single decision tree, a random forest combines the predictions from multiple decision trees to make more accurate predictions.
    
                            In a random forest, numerous decision tree algorithms (sometimes hundreds or even thousands) are individually trained using different random samples from the training dataset. This sampling method is called "bagging." Each decision tree is trained independently on its respective random sample.
    
                            Once trained, the random forest takes the same data and feeds it into each decision tree. Each tree produces a prediction, and the random forest tallies the results. The most common prediction among all the decision trees is then selected as the final prediction for the dataset.
    
                            Random forests address a common issue called "overfitting" that can occur with individual decision trees. Overfitting happens when a decision tree becomes too closely aligned with its training data, making it less accurate when presented with new data.
    
                            6. K-nearest neighbor (KNN)
                            K-nearest neighbor (KNN) is a supervised learning algorithm commonly used for classification and predictive modeling tasks. The name "K-nearest neighbor" reflects the algorithm's approach of classifying an output based on its proximity to other data points on a graph.
    
                            Let's say we have a dataset with labeled points, some marked as blue and others as red. When we want to classify a new data point, KNN looks at its nearest neighbors in the graph. The "K" in KNN refers to the number of nearest neighbors considered. For example, if K is set to 5, the algorithm looks at the 5 closest points to the new data point.
    
                            Based on the majority of the labels among the K nearest neighbors, the algorithm assigns a classification to the new data point. For instance, if most of the nearest neighbors are blue points, the algorithm classifies the new point as belonging to the blue group.
    
                            Additionally, KNN can also be used for prediction tasks. Instead of assigning a class label, KNN can estimate the value of an unknown data point based on the average or median of its K nearest neighbors.
    
    
                            7. K-means
                            K-means is an unsupervised algorithm commonly used for clustering and pattern recognition tasks. It aims to group data points based on their proximity to one another. Similar to K-nearest neighbor (KNN), K-means clustering utilizes the concept of proximity to identify patterns in data.
    
                            Each of the clusters is defined by a centroid, a real or imaginary center point for the cluster. K-means is useful on large data sets, especially for clustering, though it can falter when handling outliers.
    
                            Clustering algorithms are particularly useful for large datasets and can provide insights into the inherent structure of the data by grouping similar points together. It has applications in various fields such as customer segmentation, image compression, and anomaly detection.
    
                            Read more: What is Big Data? A Layperson's Guide
    
                            8. Support vector machine (SVM)
                            A support vector machine (SVM) is a supervised learning algorithm commonly used for classification and predictive modeling tasks. SVM algorithms are popular because they are reliable and can work well even with a small amount of data. SVM algorithms work by creating a decision boundary called a "hyperplane." In two-dimensional space, this hyperplane is like a line that separates two sets of labeled data.
    
                            The goal of SVM is to find the best possible decision boundary by maximizing the margin between the two sets of labeled data. It looks for the widest gap or space between the classes. Any new data point that falls on either side of this decision boundary is classified based on the labels in the training dataset.
    
                            It's important to note that hyperplanes can take on different shapes when plotted in three-dimensional space, allowing SVM to handle more complex patterns and relationships in the data.
    
                            9. Apriori
                            Apriori is an unsupervised learning algorithm used for predictive modeling, particularly in the field of association rule mining.
    
                            The Apriori algorithm was initially proposed in the early 1990s as a way to discover association rules between item sets. It is commonly used in pattern recognition and prediction tasks, such as understanding a consumer's likelihood of purchasing one product after buying another.
    
                            The Apriori algorithm works by examining transactional data stored in a relational database. It identifies frequent itemsets, which are combinations of items that often occur together in transactions. These itemsets are then used to generate association rules. For example, if customers frequently buy product A and product B together, an association rule can be generated to suggest that purchasing A increases the likelihood of buying B.
    
                            By applying the Apriori algorithm, analysts can uncover valuable insights from transactional data, enabling them to make predictions or recommendations based on observed patterns of itemset associations.
    
                            10. Gradient boosting
                            Gradient boosting algorithms employ an ensemble method, which means they create a series of "weak" models that are iteratively improved upon to form a strong predictive model. The iterative process gradually reduces the errors made by the models, leading to the generation of an optimal and accurate final model.
    
                            The algorithm starts with a simple, naive model that may make basic assumptions, such as classifying data based on whether it is above or below the mean. This initial model serves as a starting point.
    
                            In each iteration, the algorithm builds a new model that focuses on correcting the mistakes made by the previous models. It identifies the patterns or relationships that the previous models struggled to capture and incorporates them into the new model.
    
                            Gradient boosting is effective in handling complex problems and large datasets. It can capture intricate patterns and dependencies that may be missed by a single model. By combining the predictions from multiple models, gradient boosting produces a powerful predictive model.
                            </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 04. API -->
                    <div class="card" id="api">
                        <h3>04. API</h3>
                        <img class="toggleImg p-3 align-center" src="https://aviowiki.com/wp-content/uploads/2022/01/API-Info-graphic.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">An API, or application programming interface, is a set of protocols that determine how two software applications will interact with each other. APIs tend to be written in programming languages such as C++ or JavaScript.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            <p class="fw-bold">What is an API?</p>
                            API stands for application programming interface. An API is a set of protocols and instructions written in programming languages such as C++ or JavaScript that determine how two software components will communicate with each other. Unlike a user interface that is visible to everyone, APIs work behind the scenes to allow users to locate and retrieve the requested information. Think of APIs like contracts that determine how two software systems will interact.
    
                            Interested in learning how to work with APIs? You can get hands-on practice for free by enrolling in DeepLearning.AI's beginner-friendly online project, Building Systems with the ChatGPT API. Some experience with Python is recommended.
    
                            <p class="fw-bold">API Examples</p>
    
                            As an internet user, you’ve most likely experienced the convenience API technology enables when browsing a website or using a mobile app. Application programming interfaces are a crucial behind-the-scenes aspect of user experience (UX). Consider a few familiar examples of APIs and how a website owner or administrator might use them:
    
    
                            The YouTube API allows you to add videos to your website or app, as well as manage your playlists and subscriptions.
    
    
                            The Facebook API for conversions allows you to track page visits and conversions, as well as provide data for ad targeting and reporting.
    
    
                            The Google Maps API allows you to embed static and dynamic maps, as well as street view imagery, on your website.
    
    
                            Paypal's public simple object access protocol (SOAP) API. SOAP APIs are often used for identity management and payment gateways, especially at the enterprise level. This type of API can be more challenging to integrate than REST APIs, but typically offer more advanced features.
    
                            Any time you land on a site and watch a video, see an ad on Facebook related to a website you recently visited, or use the map on a business’s website to find its physical location, chances are an API has been at work to make this experience possible.<br>
    
                            <p class="fw-bold">Types of API</p>
    
                            Now that you have an API definition, the next step is to become familiar with the different types of APIs.
    
    
                            Open APIs - also known as external or public APIs, are available for anyone to use and integrate with their sites or apps.
    
    
                            Partner APIs - are also considered external, but you can use them only if you have a business relationship with the companies providing them.
    
    
                            Internal APIs - also called private APIs, are used by people within a company and help to transfer data between teams or connect different systems and apps. Third parties do not access internal APIs like they do with open or partner APIs.
    
    
                            Composite APIs - combine multiple APIs from different servers or data sources to create a unified connection to a single system.
    
    
                            Web Service API (or Web API) - an application interface between a web browser and a web server
    
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 05. Big Data -->
                    <div class="card" id="bigData">
                        <h3>05. Big Data</h3>
                        <img class="toggleImg p-3 align-center" src="https://i0.wp.com/www.techcheers.com/wp-content/uploads/2015/07/4-Vs-of-big-data.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Big data refers to the large data sets that can be studied to reveal patterns and trends to support business decisions. It’s called “big” data because organizations can now gather massive amounts of complex data using data collection tools and systems. Big data can be collected very quickly and stored in a variety of formats.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Big data is the vast amount of data that can be studied to show patterns, trends, and associations. Explore the basics of big data, how it's used, the industries that use it most, and how you can pursue a career in big data. <br>
    
                                Big data refers to large data sets that can be studied to reveal patterns, trends, and associations. The vast number of data collection avenues means that data can now come in larger quantities, be gathered much more quickly, and exist in a greater variety of formats than ever before. This new, larger, and more complex data is collectively called big data.
    
                            <p class="fw-bold">The three Vs of big data</p>
                            Big data is broadly defined by the three Vs: volume, velocity, and variety.
    
    
                            Volume refers to the amount of data. Big data deals with high volumes of data.
    
    
                            Velocity refers to the rate at which the data is received. Big data streams at a high velocity, often directly into memory rather than being stored on a disk.
    
    
                            Variety refers to the wide range of data formats. Big data may be structured, semi-structured, or unstructured and can be presented as numbers, text, images, audio, and more.
    
    
                            Companies that process big data may also focus on other Vs, such as value, veracity, and variability.<br>
    
                            <p class="fw-bold">What’s driving big data growth?</p>
    
                            Emerging information technology has allowed data to be collected, stored, and analyzed at unprecedented scales. The internet continues to be adopted by new users in the US and across the globe, and developing technologies have allowed the internet to be integrated into many different products, creating numerous new sources of data. The millions of people watching Netflix, using Google, and buying products online daily contribute to the increasing volume and sophistication of big data.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 06. Chatbot -->
                    <div class="card" id="chatbot">
                        <h3>06. Chatbot</h3>
                        <img class="toggleImg p-3 align-center" src="https://klizos.com/wp-content/uploads/image-1-3.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A chatbot is a computer program that simulates human conversation with an end user. Not all chatbots are equipped with artificial intelligence (AI), but modern chatbots increasingly use conversational AI techniques such as natural language processing (NLP) to understand user questions and automate responses to them.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
    
                            <p class="fw-bold">Generative AI-powered chatbots</p>
                            The next generation of chatbots with generative AI capabilities will offer even more enhanced functionality with their understanding of common language and complex queries, their ability to adapt to a user’s style of conversation and use of empathy when answering users’ questions. Business leaders can clearly see this future: 85% of execs say generative AI will be interacting directly with customers in the next two years, as reported in The CEO’s guide to generative AI study, from IBV. An enterprise-grade artificial intelligence solution can empower companies to automate self-service and accelerate the development of exceptional user experiences. 
    
                            FAQ chatbots no longer need to be pre-programmed with answers to set questions: It’s easier and faster to use generative AI in combination with an organization’s’ knowledge base to automatically generate answers in response to the wider range of questions.
    
                            While conversational AI chatbots can digest a users’ questions or comments and generate a human-like response, generative AI chatbots can take this a step further by generating new content as the output. This new content can include high-quality text, images and sound based on the LLMs they are trained on. Chatbot interfaces with generative AI can recognize, summarize, translate, predict and create content in response to a user’s query without the need for human interaction.
    
                            Enterprise-grade, self-learning generative AI chatbots built on a conversational AI platform are continually and automatically improving. They employ algorithms that automatically learn from past interactions how best to answer questions and improve conversation flow routing.<br>
    
                            <p class="fw-bold">The value of chatbots</p>
    
                            Chatbots can make it easy for users to find information by instantaneously responding to questions and requests—through text input, audio input, or both—without the need for human intervention or manual research.
    
                            Chatbot technology is now commonplace, found everywhere from smart speakers at home and consumer-facing instances of SMS, WhatsApp and Facebook Messenger, to workplace messaging applications including Slack. The latest evolution of AI chatbots, often referred to as “intelligent virtual assistants” or “virtual agents,” can not only understand free-flowing conversation through use of sophisticated language models, but even automate relevant tasks. Alongside well-known consumer-facing intelligent virtual assistants—such as Apple's Siri, Amazon Alexa, Google’s Gemini and OpenAI’s ChatGPT—virtual agents are also increasingly used in an enterprise context to assist customers and employees.
    
                            To increase the power of apps already in use, well-designed chatbots can be integrated into the software an organization is already using. For example, a chatbot can be added to Microsoft Teams to create and customize a productive hub where content, tools, and members come together to chat, meet and collaborate.
    
                            To get the most from an organization’s existing data, enterprise-grade chatbots can be integrated with critical systems and orchestrate workflows inside and outside of a CRM system. Chatbots can handle real-time actions as routine as a password change, all the way through a complex multi-step workflow spanning multiple applications. In addition, conversational analytics can analyze and extract insights from natural language conversations, typically between customers interacting with businesses through chatbots and virtual assistants.
    
                            Artificial intelligence can also be a powerful tool for developing conversational marketing strategies. AI chatbots are available to deliver customer care 24/7 and can discover insights into your customer’s engagement and buying patterns to drive more compelling conversations, and deliver more consistent and personalized digital experiences across your web and messaging channels.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 07. Cognitive computing -->
                    <div class="card" id="cognitiveComputing">
                        <h3>07. Cognitive Computing</h3>
                        <img class="toggleImg p-3 align-center" src="https://www.predictiveanalyticstoday.com/wp-content/uploads/2016/05/What-is-Cognitive-Computing-Top-10-Cognitive-Computing-Companies.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Cognitive computing is essentially the same as AI. It’s a computerized model that focuses on mimicking human thought processes such as pattern recognition and learning. Marketing teams sometimes use this term to eliminate the sci-fi mystique of AI.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
    
                            <p class="fw-bold">What is cognitive computing?</p>
                            Cognitive computing is the use of computerized models to simulate the human thought process in complex situations where the answers might be ambiguous and uncertain. The phrase is closely associated with IBM's cognitive computer system, Watson.
    
                            Computers are faster than humans at processing and calculating, but they've yet to master some tasks, such as understanding natural language and recognizing objects in an image. Cognitive computing is an attempt to have computers mimic the way the human brain works.
    
                            To accomplish this, cognitive computing uses artificial intelligence (AI) and other underlying technologies, including the following:
    
                            Expert systems.
                            Neural networks.
                            Machine learning.
                            Deep learning.
                            Natural language processing (NLP).
                            Speech recognition.
                            Object recognition.
                            Robotics.
                            Cognitive computing uses these processes in conjunction with self-learning algorithms, data analysis and pattern recognition to teach computing systems. The learning technology can be used for sentiment analysis, risk assessments and face detection. In addition, cognitive computing is particularly useful in fields such as healthcare, banking, finance and retail.<br>
    
                            <p class="fw-bold">How cognitive computing works</p>
    
                            Systems used in the cognitive sciences combine data from various sources while weighing context and conflicting evidence to suggest the best possible answers. To achieve this, cognitive systems include self-learning technologies that use data mining, pattern recognition and NLP to mimic human intelligence.
    
                            Using computer systems to solve the types of problems that humans are typically tasked with requires vast amounts of structured and unstructured data fed to machine learning algorithms. Over time, cognitive systems can refine the way they identify patterns and process data. They become capable of anticipating new problems and modeling possible solutions.
    
                            For example, by storing thousands of pictures of dogs in a database, an AI system can be taught how to identify pictures of dogs. The more data a system is exposed to, the more it's able to learn and the more accurate it becomes over time.
    
                            To achieve those capabilities, cognitive computing systems must have the following attributes:
    
                            Adaptive. Systems must be flexible enough to learn as information changes and goals evolve. They must digest dynamic data in real time and adjust as the data and environments change.
                            Interactive. Human-computer interaction is a critical component in cognitive systems. Users must be able to interact with cognitive machines and define their needs as they change. The technologies must also be able to interact with other processors, devices and cloud platforms.
                            Iterative and stateful. Cognitive computing technologies can ask questions and pull in additional data to identify or clarify a problem. They must be stateful in that they keep information about similar situations that have occurred previously.
                            Contextual. Understanding context is critical in thought processes. Cognitive systems must understand, identify and mine contextual data, such as syntax, time, location, domain, user requirements, user profiles, tasks and goals. The systems can draw on multiple sources of information, including structured and unstructured data and visual, auditory and sensor data.
    
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 08. Computer vision -->
                    <div class="card" id="computerVision">
                        <h3>08. Computer vision</h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn.prod.website-files.com/5d7b77b063a9066d83e1209c/60c12a6e084dab67d440a268_VuZKpISJC3C5ZjJ-jKPCFCDu5xiJJ595iCYtMGJguD6R6K5IDWBYm0tbMsBZwViACf73_AZrk9tAjVXemVNM-_ypZILqzf-7mRDS0x_4Nrr9DfIHv02-lD8qJCCNmWic4jaF63sO.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Computer vision is an interdisciplinary field of science and technology that focuses on how computers can gain understanding from images and videos. For AI engineers, computer vision allows them to automate activities that the human visual system typically performs.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Computer vision is a field of artificial intelligence (AI) that uses machine learning and neural networks to teach computers and systems to derive meaningful information from digital images, videos and other visual inputs—and to make recommendations or take actions when they see defects or issues.
    
                                Computer vision works much the same as human vision, except humans have a head start. Human sight has the advantage of lifetimes of context to train how to tell objects apart, how far away they are, whether they are moving or something is wrong with an image.
    
                                Computer vision trains machines to perform these functions, but it must do it in much less time with cameras, data and algorithms rather than retinas, optic nerves and a visual cortex. Because a system trained to inspect products or watch a production asset can analyze thousands of products or processes a minute, noticing imperceptible defects or issues, it can quickly surpass human capabilities.
    
                                Computer vision is used in industries that range from energy and utilities to manufacturing and automotive—and the market is continuing to grow. It is expected to reach USD 48.6 billion by 2022. <br>
    
                            <p class="fw-bold">How does computer vision work?</p>
                            Computer vision needs lots of data. It runs analyses of data over and over until it discerns distinctions and ultimately recognize images. For example, to train a computer to recognize automobile tires, it needs to be fed vast quantities of tire images and tire-related items to learn the differences and recognize a tire, especially one with no defects.
    
                            Two essential technologies are used to accomplish this: a type of machine learning called deep learning and a convolutional neural network (CNN).
    
                            Machine learning uses algorithmic models that enable a computer to teach itself about the context of visual data. If enough data is fed through the model, the computer will “look” at the data and teach itself to tell one image from another. Algorithms enable the machine to learn by itself, rather than someone programming it to recognize an image.
    
                            A CNN helps a machine learning or deep learning model “look” by breaking images down into pixels that are given tags or labels. It uses the labels to perform convolutions (a mathematical operation on two functions to produce a third function) and makes predictions about what it is “seeing.” The neural network runs convolutions and checks the accuracy of its predictions in a series of iterations until the predictions start to come true. It is then recognizing or seeing images in a way similar to humans.
    
                            Much like a human making out an image at a distance, a CNN first discerns hard edges and simple shapes, then fills in information as it runs iterations of its predictions. A CNN is used to understand single images. A recurrent neural network (RNN) is used in a similar way for video applications to help computers understand how pictures in a series of frames are related to one another.<br>
    
    
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 09. Data mining -->
                    <div class="card" id="dataVision">
                        <h3>09. Data Mining</h3>
                        <img class="toggleImg p-3 align-center" src="https://en.mdv.co.jp/ebm/wp-content/uploads/2022/08/column-17-01-1024x576.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Data mining is the process of sorting through large data sets to identify patterns that can improve models or solve problems. Data mining is the use of machine learning and statistical analysis to uncover patterns and other valuable information from large data sets.
                        </p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Given the evolution of machine learning (ML), data warehousing, and the growth of big data, the adoption of data mining, also known as knowledge discovery in databases (KDD), has rapidly accelerated over the last decades. However, while this technology continuously evolves to handle data at a large scale, leaders still might face challenges with scalability and automation.
    
                                The data mining techniques that underpin data analyses can be deployed for two main purposes. They can either describe the target data set or they can predict outcomes by using machine learning algorithms.
    
                                These methods are used to organize and filter data, surfacing the most useful information, from fraud to user behaviors, bottlenecks and even security breaches. Using ML algorithms and artificial intelligence (AI) enables automation of the analysis, which can greatly speed up the process.
    
                                When combined with data analytics and visualization tools, such as Apache Spark, data mining software is becoming more straightforward and extracting relevant insights can be gained more quickly than ever. Advances in AI continue to expedite adoption across industries.
                            <p class="fw-bold">Benefits and challenges</p>
                            Benefits
    
                            Discover hidden insights and trends: Data mining takes raw data and finds order in the chaos: seeing the forest for the trees. This can result in better-informed planning across corporate functions and industries, including advertising, finance, government, healthcare, human resources (HR), manufacturing, marketing, research, sales and supply chain management (SCM).
    
                            Save budget: By analyzing performance data from multiple sources, bottlenecks in business processes can be identified to speed resolution and increase efficiency.
    
                            Solve multiple challenges: Data mining is a versatile tool. Data from almost any source and any aspect of an organization can be analyzed to discover patterns and better ways of conducting business. Almost every department in an organization that collects and analyzes data can benefit from data mining.
    
                            Challenges
    
                            Complexity and risk: Useful insights require valid data, plus experts with coding experience. Knowledge of data mining languages including Python, R and SQL is helpful. An insufficiently cautious approach to data mining might result in misleading or dangerous results. Some consumer data used in data mining might be personally identifiable information (PII) which should be handled carefully to avoid legal or public relations issues.
    
                            Cost: For the best results, a wide and deep collection of data sets is often needed. If new information is to be gathered by an organization, setting up a data pipeline might represent a new expense. If data needs to be purchased from an outside source, that also imposes a cost.
    
                            Uncertainty: First, a major data mining effort might be well run, but produce unclear results, with no major benefit. Or inaccurate data can lead to incorrect insights, whether incorrect data was selected or the preprocessing was mishandled. Other risks include modeling errors or outdated data from a rapidly changing market.
    
                            Another potential problem is results might appear valid but are in fact random and not to be trusted. It’s important to remember that “correlation is not causation.” A famous example of “data dredging”—seeing an apparent correlation and overstating its importance—was recently presented by blogger Tyler Vigen: “The price of Amazon.com stock closely matches the number of children named ‘Stevie’ from 2002 to 2022.”1 But, of course, the naming of Stevies did not influence the stock price or vice versa. Data mining applications find the patterns, but human judgment is still significant.
    
                            Data mini<br>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 10. Data science -->
                    <div class="card" id="dataScience">
                        <h3>10. Data Science</h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn1.vectorstock.com/i/1000x1000/64/65/data-science-infographic-10-option-concept-vector-27906465.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Data science is an interdisciplinary field of technology that uses algorithms and processes to gather and analyze large amounts of data to uncover patterns and insights that inform business decisions.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Data science is an in-demand career path for people with an aptitude for research, programming, math, and computers. Discover real-world applications and job opportunities in data science and what it takes to work in this exciting field.
    
                                Data science is an interdisciplinary field that uses algorithms, procedures, and processes to examine large amounts of data in order to uncover hidden patterns, generate insights, and direct decision-making. To create prediction models, data scientists use advanced machine learning algorithms to sort through, organize, and learn from structured and unstructured data.
    
                                As a fast-growing field with applications across numerous industries, data science offers a variety of job opportunities—from researching to computing. Explore how to use data science in the real world, the job outlook for the field, its required skills, and what credentials you need to land a job.
    
                            <p class="fw-bold">Applications of Data Science [Impact Across all Industries]</p>
                            Data scientists are having an impact in almost every industry.
    
                            Health care
                            eCommerce
                            Law enforcement
                            Marketing/advertising
                            Transportation
                            Sports
                            As expected, different sectors are using data science in different ways.<br>
    
                            Health care:
    
                            Identifying and predicting disease
                            Personalized health care recommendations
                            eCommerce
    
                            Automated “smart” ad placement
                            Personalized product recommendations
                            Law enforcement
    
                            Data-driven crime predictions
                            Facial recognition tools
                            Tax fraud enforcement
                            Transportation
    
                            Optimized shipping routes
                            Modeling the most effective traffic patterns and streetlight usage
                            Getting hot food delivered quickly
                            Chances are your favorite sports team may be dabbling in data science to help put together the best, most cost-effective team.
    
                            Why, data science is even at the heart of helping people find love — through online dating platforms powered by complex algorithms. <br>
    
                            <p class="fw-bold">Why is Data Science Important?</p>
    
                            Big Data may have the potential to change the world for the better but data science is essential because, according to training provider SimpliLearn, because “without the expertise of professionals who turn cutting-edge technology into actionable insights, Big Data is nothing.”
    
                            In “Why Data Science Matters And How It Powers Business Value,” the company details eight ways that data scientists can add value to business.
    
                            Empowering management to make better decisions
                            Directing actions and defining goals based on trends
                            Challenging staff to adopt best practices and focus on issues that matter
                            Identifying business opportunities
                            Decision making with quantifiable, data-driven evidence
                            Testing these decisions
                            Identifying and refining of target audiences
                            Recruiting the right talent
                            Data science has the potential to help nearly all organizations, according to Damien Deighan, CEO of Data Science Talent.
    
                            “With the ability to uncover hidden patterns, unknown correlations and build models that can make accurate predictions, data science can be used to help you make better business decisions for your organization,” says Deighan. “You can now analyze almost anything and everything in relation to your organization. Anything that can be logged via computer or network use can be analyzed and organized and turned into actionable insights. When applied and used correctly, data analytics can play a pivotal role in driving profitability and productivity.”
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                </div>
            </div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    <script src="script.js"></script>
</body>

</html>