<?php
?>
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AINexus</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="style.css">
    <link rel="icon" href="../../AINexus.png" type="image/x-icon">
</head>

<body>

    <?php include '../../partials/_navbar/navbar.php'; ?>
    <div class="container">

        <div class="main">
            <div class="middle">
                <div class="above-title">
                    <h1 class="text-dark">Artificial Intelligence Terms: A to Z<br> Glossary</h1>
                    <img src="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://images.ctfassets.net/wp1lcwdav1p1/Otc07JBoEZ2f0QA7E4k17/0cd37a4956703c4cac5188e5e7503b03/AI_Glossary_Terms.png?w=1500&h=680&q=60&fit=fill&f=faces&fm=jpg&fl=progressive&auto=format%2Ccompress&dpr=1&w=1000" alt="">
                    <p>
                        Embarking on the journey into artificial intelligence (AI) opens up a world of innovation and potential. As a student exploring AI, understanding core terms and concepts is a powerful first step. This glossary provides an easy-to-understand guide to key AI topics, covering everything from machine learning basics to advanced topics like neural networks and natural language processing (NLP). <br> <br>
    
                        This list is designed to help you build a strong foundation in AI vocabulary. Whether you're preparing for an academic project, drafting your resume, or simply satisfying your curiosity about AI, these terms will help you feel confident and well-prepared. <br> <br>
    
                        AI is shaping our future, and learning its language is a great way to start contributing to the field. Dive into these essential AI terms and get ready for an exciting journey into technology’s next frontier!
                    </p>
                    <h2>AI Terms</h2>
                </div>
                <div class="main-content">
                    <ol>
                        <!-- 01. AI -->
                        <div class="card" id="artificialIntelligence">
                            <h3><li>Artificial Intelligence</li></h3>
                            <img src="https://fpf.org/wp-content/uploads/2020/12/AI-inforgraphic.jpg" alt="AI Infographic" class="toggleImg p-3 align-center" height="600px" width="100%" style="display: none;">
                            <!-- Main content that is always shown -->
                            <p id="shortContent" class="fw-bold">AI stands for artificial intelligence, which is the simulation of human intelligence processes by machines or computer systems. AI can mimic human capabilities such as communication, learning, and decision-making.</p>
        
                            <!-- Additional content that will be shown upon clicking "Read More" -->
                            <div class="moreContent" style="display: none;">
                                <p>Artificial intelligence is a field of science concerned with building computers and machines that can reason, learn, and act in such a way that would normally require human intelligence or that involves data whose scale exceeds what humans can analyze. <br>
        
                                    AI is a broad field that encompasses many different disciplines, including computer science, data analytics and statistics, hardware and software engineering, linguistics, neuroscience, and even philosophy and psychology. <br>
        
                                    On an operational level for business use, AI is a set of technologies that are based primarily on machine learning and deep learning, used for data analytics, predictions and forecasting, object categorization, natural language processing, recommendations, intelligent data retrieval, and more. <br>
        
                                <p class="fw-bold">How does AI work?</p>
                                While the specifics vary across different AI techniques, the core principle revolves around data. AI systems learn and improve through exposure to vast amounts of data, identifying patterns and relationships that humans may miss. <br>
        
                                This learning process often involves algorithms, which are sets of rules or instructions that guide the AI's analysis and decision-making. In machine learning, a popular subset of AI, algorithms are trained on labeled or unlabeled data to make predictions or categorize information. <br>
        
                                Deep learning, a further specialization, utilizes artificial neural networks with multiple layers to process information, mimicking the structure and function of the human brain. Through continuous learning and adaptation, AI systems become increasingly adept at performing specific tasks, from recognizing images to translating languages and beyond. <br>
        
                                <p class="fw-bold">Types of artificial intelligence
                                    Artificial intelligence can be organized in several ways, depending on stages of development or actions being performed.</p>
        
                                For instance, four stages of AI development are commonly recognized. <br>
        
                                Reactive machines: Limited AI that only reacts to different kinds of stimuli based on preprogrammed rules. Does not use memory and thus cannot learn with new data. IBM’s Deep Blue that beat chess champion Garry Kasparov in 1997 was an example of a reactive machine. <br>
                                Limited memory: Most modern AI is considered to be limited memory. It can use memory to improve over time by being trained with new data, typically through an artificial neural network or other training model. Deep learning, a subset of machine learning, is considered limited memory artificial intelligence. <br>
                                Theory of mind: Theory of mind AI does not currently exist, but research is ongoing into its possibilities. It describes AI that can emulate the human mind and has decision-making capabilities equal to that of a human, including recognizing and remembering emotions and reacting in social situations as a human would. <br>
                                Self aware: A step above theory of mind AI, self-aware AI describes a mythical machine that is aware of its own existence and has the intellectual and emotional capabilities of a human. Like theory of mind AI, self-aware AI does not currently exist. <br>
                                A more useful way of broadly categorizing types of artificial intelligence is by what the machine can do. All of what we currently call artificial intelligence is considered artificial “narrow” intelligence, in that it can perform only narrow sets of actions based on its programming and training. For instance, an AI algorithm that is used for object classification won’t be able to perform natural language processing. Google Search is a form of narrow AI, as is predictive analytics, or virtual assistants. <br>
        
                                Artificial general intelligence (AGI) would be the ability for a machine to “sense, think, and act” just like a human. AGI does not currently exist. The next level would be artificial superintelligence (ASI), in which the machine would be able to function in all ways superior to a human. <br> </p>
                            </div>
                            <!-- Link to toggle visibility -->
                            <a href="#" class="readMoreLink">Read More</a>
                        </div>

                    
    
                    <!-- 02. AIEthics -->
                    <div class="card" id="aiEthics">
                        <h3><li>AIEthics</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.orientsoftware.com/Themes/Content/Images/blog/2022-04-14/ethics-in-ai-2.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">AI ethics refers to the issues that AI stakeholders such as engineers and government officials must consider to ensure that the technology is developed and used responsibly. This means adopting and implementing systems that support a safe, secure, unbiased, and environmentally friendly approach to artificial intelligence.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>As artificial intelligence (AI) becomes increasingly important to society, experts in the field have identified a need for ethical boundaries when it comes to creating and implementing new AI tools. Although there's currently no wide-scale governing body to write and enforce these rules, many technology companies have adopted their own version of AI ethics or an AI code of conduct.
    
                                AI ethics are the moral principles that companies use to guide responsible and fair development and use of AI. In this article, we'll explore what ethics in AI are, why they matter, and some challenges and benefits of developing an AI code of conduct. <br>
    
                            <p class="fw-bold">What are AI ethics?</p>
                            AI ethics are the set of guiding principles that stakeholders (from engineers to government officials) use to ensure artificial intelligence technology is developed and used responsibly. This means taking a safe, secure, humane, and environmentally friendly approach to AI.
    
                            A strong AI code of ethics can include avoiding bias, ensuring privacy of users and their data, and mitigating environmental risks. Codes of ethics in companies and government-led regulatory frameworks are two main ways that AI ethics can be implemented. By covering global and national ethical AI issues, and laying the policy groundwork for ethical AI in companies, both approaches help regulate AI technology.
    
                            More broadly, discussion around AI ethics has progressed from being centered around academic research and non-profit organizations. Today, big tech companies like IBM, Google, and Meta have assembled teams to tackle ethical issues that arise from collecting massive amounts of data. At the same time, government and intergovernmental entities have begun to devise regulations and ethics policy based on academic research.
    
                            <p class="fw-bold">Stakeholders in AI ethics</p>
    
                            Developing ethical principles for responsible AI use and development requires industry actors to work together. Stakeholders must examine how social, economic, and political issues intersect with AI, and determine how machines and humans can coexist harmoniously.
    
                            Each of these actors play an important role in ensuring less bias and risk for AI technologies.
    
    
                            Academics: Researchers and professors are responsible for developing theory-based statistics, research, and ideas that can support governments, corporations, and non-profit organizations.
    
    
                            Government: Agencies and committees within a government can help facilitate AI ethics in a nation. A good example of this is the Preparing for the Future of Artificial Intelligence report that was developed by the National Science and Technology Council (NSTC) in 2016, which outlines AI and its relationship to public outreach, regulation, governance, economy, and security.
    
    
                            Intergovernmental entities: Entities like the United Nations and the World Bank are responsible for raising awareness and drafting agreements for AI ethics globally. For example, UNESCO’s 193 member states adopted the first ever global agreement on the Ethics of AI in November 2021 to promote human rights and dignity.
    
    
                            Non-profit organizations: Non-profit organizations like Black in AI and Queer in AI help diverse groups gain representation within AI technology. The Future of Life Institute created 23 guidelines that are now the Asilomar AI Principles, which outline specific risks, challenges, outcomes for AI technologies.
    
    
                            Private companies: Executives at Google, Meta, and other tech companies, as well as banking, consulting, health care, and other industries within the private sector that uses AI technology, are responsible for creating ethics teams and codes of conduct. This often creates a standard for companies to follow suit. <br> </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 03. Algorithm Section -->
                    <div class="card" id="algorithm">
                        <!-- Section Title -->
                        <h3>
                            <li>Algorithm</li>
                        </h3>

                        <!-- Infographic Image (Hidden by Default) -->
                        <img 
                            class="toggleImg p-3 align-center" 
                            src="https://www.techtarget.com/rms/onlineimages/types_of_algorithms-f_mobile.png" 
                            alt="AI Infographic" 
                            height="500px" 
                            width="100%" 
                            style="display: none;"
                        />

                        <!-- Main Content -->
                        <p id="shortContent" class="fw-bold">
                            An algorithm is a sequence of rules given to an AI machine to perform a task or solve a problem. Common algorithms include classification, regression, and clustering.
                        </p>

                        <!-- Additional Content (Hidden by Default) -->
                        <div class="moreContent" style="display: none;">
                            <!-- Introduction -->
                            <p>
                                At the core of machine learning are algorithms, which are trained to become machine learning models that power some of the most impactful innovations in the world today. Below, you’ll find 10 popular machine learning algorithms and their applications.
                            </p>

                            <!-- List of Algorithms -->
                            <p class="fw-bold">10 Machine Learning Algorithms to Know:</p>
                            <ol>
                                <li>
                                    <strong>Linear Regression:</strong> A supervised technique for predicting continuous values (e.g., sales or housing prices).
                                </li>
                                <li>
                                    <strong>Logistic Regression:</strong> Used for binary classification tasks (e.g., spam detection or medical diagnosis).
                                </li>
                                <li>
                                    <strong>Naive Bayes:</strong> Efficient for tasks like sentiment analysis and email spam filtering using conditional probabilities.
                                </li>
                                <li>
                                    <strong>Decision Tree:</strong> A flowchart-like structure for classification and prediction.
                                </li>
                                <li>
                                    <strong>Random Forest:</strong> Combines multiple decision trees to improve accuracy and reduce overfitting.
                                </li>
                                <li>
                                    <strong>K-Nearest Neighbor (KNN):</strong> Classifies data points based on their proximity to others.
                                </li>
                                <li>
                                    <strong>K-Means:</strong> An unsupervised clustering algorithm to group data based on similarities.
                                </li>
                                <li>
                                    <strong>Support Vector Machine (SVM):</strong> Finds the optimal boundary to separate classes for classification.
                                </li>
                                <li>
                                    <strong>Apriori:</strong> Used for association rule mining in transactional datasets.
                                </li>
                                <li>
                                    <strong>Gradient Boosting:</strong> An ensemble method that iteratively builds models to reduce errors.
                                </li>
                            </ol>

                            <!-- Conclusion -->
                            <p>
                                These algorithms are fundamental to machine learning and have diverse applications ranging from predictive modeling to clustering and classification.
                            </p>
                        </div>

                        <!-- Link to Toggle Content Visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>



                    <!-- 03. Activation Function -->
                    <div class="card" id="algorithm">
                        <h3><li>Activation Function</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://media.geeksforgeeks.org/wp-content/cdn-uploads/33-1-1.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A mathematical function in neural networks that determines the output of each neuron.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>In the process of building a neural network, one of the choices you get to make is what Activation Function to use in the hidden layer as well as at the output layer of the network. This article discusses Activation functions in Neural Networks. <br>
    
                            <p class="fw-bold">What is an Activation Function?</p>
    
                            An activation function in the context of neural networks is a mathematical function applied to the output of a neuron. The purpose of an activation function is to introduce non-linearity into the model, allowing the network to learn and represent complex patterns in the data. Without non-linearity, a neural network would essentially behave like a linear regression model, regardless of the number of layers it has.

                            The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. The purpose of the activation function is to introduce non-linearity into the output of a neuron. 

                            Explanation: We know, the neural network has neurons that work in correspondence with weight, bias, and their respective activation function. In a neural network, we would update the weights and biases of the neurons on the basis of the error at the output. This process is known as back-propagation. Activation functions make the back-propagation possible since the gradients are supplied along with the error to update the weights and biases. 


                            </p>

                            <p class="fw-bold">Elements of a Neural Network</p>
    
                            Input Layer: This layer accepts input features. It provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer. 

                            Hidden Layer: Nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network. The hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer. 

                            Output Layer: This layer bring up the information learned by the network to the outer world. 
                            </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 03. Agent-Based Modeling -->
                    <div class="card" id="algorithm">
                        <h3><li>Agent-Based Modeling</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.collidu.com/media/catalog/product/img/6/7/673d750016b6d23684554bd9254aa0969b8e0a36f56a6a768cb07d4672275f70/agent-based-modeling-slide3.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A simulation modeling technique to assess the actions and interactions of autonomous agents.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>Agent-based AI refers to a branch of artificial intelligence that focuses on creating autonomous entities known as agents. These agents can perceive their environment, make decisions based on that perception, and perform actions to achieve specific objectives. Essentially, each agent is an independent unit capable of intelligent behavior. <br>
    
                            <p class="fw-bold">Components</p>
    
                            Agents: These are the core units of agent-based AI. Agents can range from simple rule-based entities to complex systems with advanced learning capabilities. They can act independently and exhibit behaviors such as learning, adaptation, and cooperation.
                            Environments: The environment is the context within which agents operate. It can be a physical space, like a warehouse for robotics, or a virtual space, such as a financial market simulation. The environment provides the agents with the information they need to make decisions.
                            Interactions: Agents interact with their environment and with each other. These interactions can be cooperative, competitive, or neutral. The nature of these interactions significantly influences the agents’ behavior and the outcomes of their actions.

                            </p>

                            <p class="fw-bold">Comparison with Traditional AI Approaches</p>
    
                            Traditional AI: Typically involves centralized systems that process large amounts of data and make decisions based on predefined algorithms and models. These systems can be powerful but are often rigid and less adaptable to dynamic changes.
                            Agent-Based AI: In contrast, uses decentralized systems where multiple agents operate independently or collaboratively. This approach allows for greater flexibility and adaptability. For example, in a changing environment, agents can quickly adjust their behavior without needing a complete system overhaul.
                            </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 03. Attention Mechanism  -->
                    <div class="card" id="algorithm">
                        <h3><li>Attention Mechanism</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://techovedas.com/wp-content/uploads/2024/03/GBopIFaacAA__Ds.webp" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A neural network technique to improve the focus on relevant parts of input data, often used in NLP.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>An attention mechanism is a technique used in machine learning and artificial intelligence to improve the performance of models by focusing on relevant information. It allows models to selectively attend to different parts of the input data, assigning varying degrees of importance or weight to different elements. <br>
    
                            <p class="fw-bold">How Attention Mechanisms Work</p>
    
                            Attention mechanisms work by generating attention weights for different elements or features of the input data. These weights determine the level of importance each element contributes to the model's output. The attention weights are calculated based on the relevance or similarity between the elements and a query or context vector.

                            The attention mechanism typically involves three key components:

                            Query: Represents the current context or focus of the model.

                            Key: Represents the elements or features of the input data.

                            Value: Represents the values associated with the elements or features.

                            The attention mechanism computes the attention weights by measuring the similarity between the query and the keys. The values are then weighted by the attention weights and combined to produce the final output of the attention mechanism.

                            </p>

                            <p class="fw-bold">Why Attention Mechanisms are Important</p>
    
                            Attention mechanism is important in machine learning and artificial intelligence for several reasons:

                            Improved Model Performance: By focusing on relevant information, attention mechanism enables models to make more accurate predictions and capture important patterns or dependencies in the data.

                            Effective Handling of Variable-Length Inputs: Attention mechanism allows models to process inputs of variable lengths by attending to different parts of the input sequence dynamically.

                            Interpretability and Explainability: Attention weights provide insights into the model's decision-making process, making it easier to interpret and explain the model's predictions.
                            </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 03. Autoencoder -->
                    <div class="card" id="algorithm">
                        <h3><li>Autoencoder</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://media.geeksforgeeks.org/wp-content/cdn-uploads/33-1-1.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A type of neural network used for unsupervised learning, mainly in feature extraction and dimensionality reduction.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>At the heart of deep learning lies the neural network, an intricate interconnected system of nodes that mimics the human brain’s neural architecture. Neural networks excel at discerning intricate patterns and representations within vast datasets, allowing them to make predictions, classify information, and generate novel insights. Autoencoders emerge as a fascinating subset of neural networks, offering a unique approach to unsupervised learning. Autoencoders are an adaptable and strong class of architectures for the dynamic field of deep learning, where neural networks develop constantly to identify complicated patterns and representations. With their ability to learn effective representations of data, these unsupervised learning models have received considerable attention and are useful in a wide variety of areas, from image processing to anomaly detection. <br>
    
                            <p class="fw-bold">What are Autoencoders?</p>
    
                            Autoencoders are a specialized class of algorithms that can learn efficient representations of input data with no need for labels. It is a class of artificial neural networks designed for unsupervised learning. Learning to compress and effectively represent input data without specific labels is the essential principle of an automatic decoder. This is accomplished using a two-fold structure that consists of an encoder and a decoder. The encoder transforms the input data into a reduced-dimensional representation, which is often referred to as “latent space” or “encoding”. From that representation, a decoder rebuilds the initial input. For the network to gain meaningful patterns in data, a process of encoding and decoding facilitates the definition of essential features.

                            </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


    
                    <!-- 04. API -->
                    <div class="card" id="api">
                        <h3><li>API</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://aviowiki.com/wp-content/uploads/2022/01/API-Info-graphic.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">An API, or application programming interface, is a set of protocols that determine how two software applications will interact with each other. APIs tend to be written in programming languages such as C++ or JavaScript.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            <p class="fw-bold">What is an API?</p>
                            API stands for application programming interface. An API is a set of protocols and instructions written in programming languages such as C++ or JavaScript that determine how two software components will communicate with each other. Unlike a user interface that is visible to everyone, APIs work behind the scenes to allow users to locate and retrieve the requested information. Think of APIs like contracts that determine how two software systems will interact.
    
                            Interested in learning how to work with APIs? You can get hands-on practice for free by enrolling in DeepLearning.AI's beginner-friendly online project, Building Systems with the ChatGPT API. Some experience with Python is recommended.
    
                            <p class="fw-bold">API Examples</p>
    
                            As an internet user, you’ve most likely experienced the convenience API technology enables when browsing a website or using a mobile app. Application programming interfaces are a crucial behind-the-scenes aspect of user experience (UX). Consider a few familiar examples of APIs and how a website owner or administrator might use them:
    
    
                            The YouTube API allows you to add videos to your website or app, as well as manage your playlists and subscriptions.
    
    
                            The Facebook API for conversions allows you to track page visits and conversions, as well as provide data for ad targeting and reporting.
    
    
                            The Google Maps API allows you to embed static and dynamic maps, as well as street view imagery, on your website.
    
    
                            Paypal's public simple object access protocol (SOAP) API. SOAP APIs are often used for identity management and payment gateways, especially at the enterprise level. This type of API can be more challenging to integrate than REST APIs, but typically offer more advanced features.
    
                            Any time you land on a site and watch a video, see an ad on Facebook related to a website you recently visited, or use the map on a business’s website to find its physical location, chances are an API has been at work to make this experience possible.<br>
    
                            <p class="fw-bold">Types of API</p>
    
                            Now that you have an API definition, the next step is to become familiar with the different types of APIs.
    
    
                            Open APIs - also known as external or public APIs, are available for anyone to use and integrate with their sites or apps.
    
    
                            Partner APIs - are also considered external, but you can use them only if you have a business relationship with the companies providing them.
    
    
                            Internal APIs - also called private APIs, are used by people within a company and help to transfer data between teams or connect different systems and apps. Third parties do not access internal APIs like they do with open or partner APIs.
    
    
                            Composite APIs - combine multiple APIs from different servers or data sources to create a unified connection to a single system.
    
    
                            Web Service API (or Web API) - an application interface between a web browser and a web server
    
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 05. Backpropagation -->
                    <div class="card" id="bigData">
                        <h3><li>Backpropogation</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://serokell.io/files/a0/a05ov1m.Backpropagation_in_NN_pic1.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">The process of adjusting weights in a neural network through error correction during training.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Backpropagation (short for "Backward Propagation of Errors") is a method used to train artificial neural networks. Its goal is to reduce the difference between the model’s predicted output and the actual output by adjusting the weights and biases in the network.
                            <p class="fw-bold">What is Backpropagation?</p>
                            Backpropagation is a powerful algorithm in deep learning, primarily used to train artificial neural networks, particularly feed-forward networks. It works iteratively, minimizing the cost function by adjusting weights and biases.

                            In each epoch, the model adapts these parameters, reducing loss by following the error gradient. Backpropagation often utilizes optimization algorithms like gradient descent or stochastic gradient descent. The algorithm computes the gradient using the chain rule from calculus, allowing it to effectively navigate complex layers in the neural network to minimize the cost function. <br>
    
                            <p class="fw-bold">Why is Backpropagation Important?</p>
    
                            Backpropagation plays a critical role in how neural networks improve over time. Here's why:

                            Efficient Weight Update: It computes the gradient of the loss function with respect to each weight using the chain rule, making it possible to update weights efficiently.
                            Scalability: The backpropagation algorithm scales well to networks with multiple layers and complex architectures, making deep learning feasible.
                            Automated Learning: With backpropagation, the learning process becomes automated, and the model can adjust itself to optimize its performance.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 05. Bayesian Network -->
                    <div class="card" id="bigData">
                        <h3><li>Bayesian Nework</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://images.prismic.io/turing/659d7728531ac2845a2742b2_Applications_of_Bayesian_networks_in_AI_1_11zon_0d89bf7ece.webp?auto=format,compress" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A graphical model that represents probabilistic relationships among variables.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Bayesian Belief Network is a graphical representation of different probabilistic relationships among random variables in a particular set. It is a classifier with no dependency on attributes i.e it is condition independent. Due to its feature of joint probability, the probability in Bayesian Belief Network is derived, based on a condition — P(attribute/parent) i.e probability of an attribute, true over parent attribute.
                            <p class="fw-bold">Bayesian Belief Network in artificial intelligence</p>
                            Bayesian belief network is key computer technology for dealing with probabilistic events and to solve a problem which has uncertainty. We can define a Bayesian network as:

                            "A Bayesian network is a probabilistic graphical model which represents a set of variables and their conditional dependencies using a directed acyclic graph."

                            It is also called a Bayes network, belief network, decision network, or Bayesian model.

                            Bayesian networks are probabilistic, because these networks are built from a probability distribution, and also use probability theory for prediction and anomaly detection.
                            
                            Real world applications are probabilistic in nature, and to represent the relationship between multiple events, we need a Bayesian network. It can also be used in various tasks including prediction, anomaly detection, diagnostics, automated insight, reasoning, time series prediction, and decision making under uncertainty.

                            Bayesian Network can be used for building models from data and experts opinions, and it consists of two parts:

                            Directed Acyclic Graph
                            Table of conditional probabilities.
                            The generalized form of Bayesian network that represents and solve decision problems under uncertain knowledge is known as an Influence diagram.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 05. Big Data -->
                    <div class="card" id="bigData">
                        <h3><li>Big Data</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://i0.wp.com/www.techcheers.com/wp-content/uploads/2015/07/4-Vs-of-big-data.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Big data refers to the large data sets that can be studied to reveal patterns and trends to support business decisions. It’s called “big” data because organizations can now gather massive amounts of complex data using data collection tools and systems. Big data can be collected very quickly and stored in a variety of formats.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Big data is the vast amount of data that can be studied to show patterns, trends, and associations. Explore the basics of big data, how it's used, the industries that use it most, and how you can pursue a career in big data. <br>
    
                                Big data refers to large data sets that can be studied to reveal patterns, trends, and associations. The vast number of data collection avenues means that data can now come in larger quantities, be gathered much more quickly, and exist in a greater variety of formats than ever before. This new, larger, and more complex data is collectively called big data.
    
                            <p class="fw-bold">The three Vs of big data</p>
                            Big data is broadly defined by the three Vs: volume, velocity, and variety.
    
    
                            Volume refers to the amount of data. Big data deals with high volumes of data.
    
    
                            Velocity refers to the rate at which the data is received. Big data streams at a high velocity, often directly into memory rather than being stored on a disk.
    
    
                            Variety refers to the wide range of data formats. Big data may be structured, semi-structured, or unstructured and can be presented as numbers, text, images, audio, and more.
    
    
                            Companies that process big data may also focus on other Vs, such as value, veracity, and variability.<br>
    
                            <p class="fw-bold">What’s driving big data growth?</p>
    
                            Emerging information technology has allowed data to be collected, stored, and analyzed at unprecedented scales. The internet continues to be adopted by new users in the US and across the globe, and developing technologies have allowed the internet to be integrated into many different products, creating numerous new sources of data. The millions of people watching Netflix, using Google, and buying products online daily contribute to the increasing volume and sophistication of big data.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 05. Bias Term -->
                    <div class="card" id="bigData">
                        <h3><li>Bias Term</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://research.aimultiple.com/wp-content/uploads/2020/09/AI-bias-explained-612x311.jpeg.webp" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">AI bias, also called machine learning bias or algorithm bias, refers to the occurrence of biased results due to human biases that skew the original training data or AI algorithm—leading to distorted outputs and potentially harmful outcomes.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            When AI bias goes unaddressed, it can impact an organization’s success and hinder people’s ability to participate in the economy and society. Bias reduces AI’s accuracy, and therefore its potential.

                            Businesses are less likely to benefit from systems that produce distorted results. And scandals resulting from AI bias could foster mistrust among people of color, women, people with disabilities, the LGBTQ community, or other marginalized groups.


                            <p class="fw-bold">Real-world examples and risks</p>
                            When AI makes a mistake due to bias—such as groups of people denied opportunities, misidentified in photos or punished unfairly—the offending organization suffers damage to its brand and reputation. At the same time, the people in those groups and society as a whole can experience harm without even realizing it. Here are a few high-profile examples of disparities and bias in AI and the harm they can cause.

                            In healthcare, underrepresenting data of women or minority groups can skew predictive AI algorithms.2 For example, computer-aided diagnosis (CAD) systems have been found to return lower accuracy results for African-American patients than white patients.

                            While AI tools can streamline the automation of resume scanning during a search to help identify ideal candidates, the information requested and answers screened out can result in disproportionate outcomes across groups. For example, if a job ad uses the word “ninja,” it might attract more men than women, even though that is in no way a job requirement.3   

                            As a test of image generation, Bloomberg requested more than 5,000 AI images be created and found that, “The world according to Stable Diffusion is run by white male CEOs. Women are rarely doctors, lawyers or judges. Men with dark skin commit crimes, while women with dark skin flip burgers.”4  Midjourney conducted a similar study of AI art generation, requesting images of people in specialized professions. The result showed both younger and older people, but the older people were always men, reinforcing gender bias of the role of women in the workplace.5 

                            AI-powered predictive policing tools used by some organizations in the criminal justice system are supposed to identify areas where crime is likely to occur. However, they often rely on historical arrest data, which can reinforce existing patterns of racial profiling and disproportionate targeting of minority communities.
                            <p class="fw-bold">What’s driving big data growth?</p>
    
                            Emerging information technology has allowed data to be collected, stored, and analyzed at unprecedented scales. The internet continues to be adopted by new users in the US and across the globe, and developing technologies have allowed the internet to be integrated into many different products, creating numerous new sources of data. The millions of people watching Netflix, using Google, and buying products online daily contribute to the increasing volume and sophistication of big data.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 06. Chatbot -->
                    <div class="card" id="chatbot">
                        <h3><li>Chatbot</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://klizos.com/wp-content/uploads/image-1-3.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A chatbot is a computer program that simulates human conversation with an end user. Not all chatbots are equipped with artificial intelligence (AI), but modern chatbots increasingly use conversational AI techniques such as natural language processing (NLP) to understand user questions and automate responses to them.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
    
                            <p class="fw-bold">Generative AI-powered chatbots</p>
                            The next generation of chatbots with generative AI capabilities will offer even more enhanced functionality with their understanding of common language and complex queries, their ability to adapt to a user’s style of conversation and use of empathy when answering users’ questions. Business leaders can clearly see this future: 85% of execs say generative AI will be interacting directly with customers in the next two years, as reported in The CEO’s guide to generative AI study, from IBV. An enterprise-grade artificial intelligence solution can empower companies to automate self-service and accelerate the development of exceptional user experiences. 
    
                            FAQ chatbots no longer need to be pre-programmed with answers to set questions: It’s easier and faster to use generative AI in combination with an organization’s’ knowledge base to automatically generate answers in response to the wider range of questions.
    
                            While conversational AI chatbots can digest a users’ questions or comments and generate a human-like response, generative AI chatbots can take this a step further by generating new content as the output. This new content can include high-quality text, images and sound based on the LLMs they are trained on. Chatbot interfaces with generative AI can recognize, summarize, translate, predict and create content in response to a user’s query without the need for human interaction.
    
                            Enterprise-grade, self-learning generative AI chatbots built on a conversational AI platform are continually and automatically improving. They employ algorithms that automatically learn from past interactions how best to answer questions and improve conversation flow routing.<br>
    
                            <p class="fw-bold">The value of chatbots</p>
    
                            Chatbots can make it easy for users to find information by instantaneously responding to questions and requests—through text input, audio input, or both—without the need for human intervention or manual research.
    
                            Chatbot technology is now commonplace, found everywhere from smart speakers at home and consumer-facing instances of SMS, WhatsApp and Facebook Messenger, to workplace messaging applications including Slack. The latest evolution of AI chatbots, often referred to as “intelligent virtual assistants” or “virtual agents,” can not only understand free-flowing conversation through use of sophisticated language models, but even automate relevant tasks. Alongside well-known consumer-facing intelligent virtual assistants—such as Apple's Siri, Amazon Alexa, Google’s Gemini and OpenAI’s ChatGPT—virtual agents are also increasingly used in an enterprise context to assist customers and employees.
    
                            To increase the power of apps already in use, well-designed chatbots can be integrated into the software an organization is already using. For example, a chatbot can be added to Microsoft Teams to create and customize a productive hub where content, tools, and members come together to chat, meet and collaborate.
    
                            To get the most from an organization’s existing data, enterprise-grade chatbots can be integrated with critical systems and orchestrate workflows inside and outside of a CRM system. Chatbots can handle real-time actions as routine as a password change, all the way through a complex multi-step workflow spanning multiple applications. In addition, conversational analytics can analyze and extract insights from natural language conversations, typically between customers interacting with businesses through chatbots and virtual assistants.
    
                            Artificial intelligence can also be a powerful tool for developing conversational marketing strategies. AI chatbots are available to deliver customer care 24/7 and can discover insights into your customer’s engagement and buying patterns to drive more compelling conversations, and deliver more consistent and personalized digital experiences across your web and messaging channels.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 06. Clustering -->
                    <div class="card" id="chatbot">
                        <h3><li>Clustering</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://miro.medium.com/v2/resize:fit:1093/1*Ae45JULGgghix4EtjApmMg.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Clustering is an unsupervised machine learning technique designed to group unlabeled examples based on their similarity to each other.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
    
                            <p class="fw-bold">Clustering in Machine Learning</p>
                            In real world, not every data we work upon has a target variable. This kind of data cannot be analyzed using supervised learning algorithms. We need the help of unsupervised algorithms. One of the most popular type of analysis under unsupervised learning is Cluster analysis. When the goal is to group similar data points in a dataset, then we use cluster analysis. In practical situations, we can use cluster analysis for customer segmentation for targeted advertisements, or in medical imaging to find unknown or new infected areas and many more use cases. <br>
    
                            <p class="fw-bold">Uses of Clustering</p>
    
                            Clustering algorithms are majorly used for:

                            Market Segmentation – Businesses use clustering to group their customers and use targeted advertisements to attract more audience.
                            Market Basket Analysis – Shop owners analyze their sales and figure out which items are majorly bought together by the customers. For example, In USA, according to a study diapers and beers were usually bought together by fathers.
                            Social Network Analysis – Social media sites use your data to understand your browsing behaviour and provide you with targeted friend recommendations or content recommendations.
                            Medical Imaging – Doctors use Clustering to find out diseased areas in diagnostic images like X-rays.
                            Anomaly Detection – To find outliers in a stream of real-time dataset or forecasting fraudulent transactions we can use clustering to identify them.
                            Simplify working with large datasets – Each cluster is given a cluster ID after clustering is complete. Now, you may reduce a feature set’s whole feature set into its cluster ID. Clustering is effective when it can represent a complicated case with a straightforward cluster ID. Using the same principle, clustering data can make complex datasets simpler.
                            There are many more use cases for clustering but there are some of the major and common use cases of clustering. Moving forward we will be discussing Clustering Algorithms that will help you perform the above tasks.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 07. Convolutional Neural Network (CNN) -->
                    <div class="card" id="cnn">
                        <h3><li>Convolutional Neural Network (CNN)</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.quantamagazine.org/wp-content/uploads/2021/06/CNN04.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A neural network designed for image processing and computer vision tasks.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                A convolutional neural network (CNN) is a type of artificial neural network used primarily for image recognition and processing, due to its ability to recognize patterns in images. A CNN is a powerful tool but requires millions of labelled data points for training. CNNs must be trained with high-power processors, such as a GPU or an NPU, if they are to produce results quickly enough to be useful. <br>
    
                            <p class="fw-bold">What Is a Convolutional Neural Network?</p>
                            <p>A Convolutional Neural Network (CNN) is a type of deep learning algorithm that is particularly well-suited for image recognition and processing tasks. It is made up of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The architecture of CNNs is inspired by the visual processing in the human brain, and they are well-suited for capturing hierarchical patterns and spatial dependencies within images.

                                Key components of a Convolutional Neural Network include:

                                Convolutional Layers: These layers apply convolutional operations to input images, using filters (also known as kernels) to detect features such as edges, textures, and more complex patterns. Convolutional operations help preserve the spatial relationships between pixels.
                                Pooling Layers: Pooling layers downsample the spatial dimensions of the input, reducing the computational complexity and the number of parameters in the network. Max pooling is a common pooling operation, selecting the maximum value from a group of neighboring pixels.
                                Activation Functions: Non-linear activation functions, such as Rectified Linear Unit (ReLU), introduce non-linearity to the model, allowing it to learn more complex relationships in the data.
                                Fully Connected Layers: These layers are responsible for making predictions based on the high-level features learned by the previous layers. They connect every neuron in one layer to every neuron in the next layer.
                                CNNs are trained using a large dataset of labeled images, where the network learns to recognize patterns and features that are associated with specific objects or classes. Proven to be highly effective in image-related tasks, achieving state-of-the-art performance in various computer vision applications. Their ability to automatically learn hierarchical representations of features makes them well-suited for tasks where the spatial relationships and patterns in the data are crucial for accurate predictions. CNNs are widely used in areas such as image classification, object detection, facial recognition, and medical image analysis.

                                The convolutional layers are the key component of a CNN, where filters are applied to the input image to extract features such as edges, textures, and shapes.

                                The output of the convolutional layers is then passed through pooling layers, which are used to down-sample the feature maps, reducing the spatial dimensions while retaining the most important information. The output of the pooling layers is then passed through one or more fully connected layers, which are used to make a prediction or classify the image.</p>
    
                            <p class="fw-bold">Convolutional Neural Network Training</p>
    
                            CNNs are trained using a supervised learning approach. This means that the CNN is given a set of labeled training images. The CNN then learns to map the input images to their correct labels.

                                The training process for a CNN involves the following steps:

                                Data Preparation: The training images are preprocessed to ensure that they are all in the same format and size.
                                Loss Function: A loss function is used to measure how well the CNN is performing on the training data. The loss function is typically calculated by taking the difference between the predicted labels and the actual labels of the training images.
                                Optimizer: An optimizer is used to update the weights of the CNN in order to minimize the loss function.
                                Backpropagation: Backpropagation is a technique used to calculate the gradients of the loss function with respect to the weights of the CNN. The gradients are then used to update the weights of the CNN using the optimizer.

                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 07. Cognitive computing -->
                    <div class="card" id="cognitiveComputing">
                        <h3><li>Cognitive computing</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.predictiveanalyticstoday.com/wp-content/uploads/2016/05/What-is-Cognitive-Computing-Top-10-Cognitive-Computing-Companies.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Cognitive computing is essentially the same as AI. It’s a computerized model that focuses on mimicking human thought processes such as pattern recognition and learning. Marketing teams sometimes use this term to eliminate the sci-fi mystique of AI.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
    
                            <p class="fw-bold">What is cognitive computing?</p>
                            Cognitive computing is the use of computerized models to simulate the human thought process in complex situations where the answers might be ambiguous and uncertain. The phrase is closely associated with IBM's cognitive computer system, Watson.
    
                            Computers are faster than humans at processing and calculating, but they've yet to master some tasks, such as understanding natural language and recognizing objects in an image. Cognitive computing is an attempt to have computers mimic the way the human brain works.
    
                            To accomplish this, cognitive computing uses artificial intelligence (AI) and other underlying technologies, including the following:
    
                            Expert systems.
                            Neural networks.
                            Machine learning.
                            Deep learning.
                            Natural language processing (NLP).
                            Speech recognition.
                            Object recognition.
                            Robotics.
                            Cognitive computing uses these processes in conjunction with self-learning algorithms, data analysis and pattern recognition to teach computing systems. The learning technology can be used for sentiment analysis, risk assessments and face detection. In addition, cognitive computing is particularly useful in fields such as healthcare, banking, finance and retail.<br>
    
                            <p class="fw-bold">How cognitive computing works</p>
    
                            Systems used in the cognitive sciences combine data from various sources while weighing context and conflicting evidence to suggest the best possible answers. To achieve this, cognitive systems include self-learning technologies that use data mining, pattern recognition and NLP to mimic human intelligence.
    
                            Using computer systems to solve the types of problems that humans are typically tasked with requires vast amounts of structured and unstructured data fed to machine learning algorithms. Over time, cognitive systems can refine the way they identify patterns and process data. They become capable of anticipating new problems and modeling possible solutions.
    
                            For example, by storing thousands of pictures of dogs in a database, an AI system can be taught how to identify pictures of dogs. The more data a system is exposed to, the more it's able to learn and the more accurate it becomes over time.
    
                            To achieve those capabilities, cognitive computing systems must have the following attributes:
    
                            Adaptive. Systems must be flexible enough to learn as information changes and goals evolve. They must digest dynamic data in real time and adjust as the data and environments change.
                            Interactive. Human-computer interaction is a critical component in cognitive systems. Users must be able to interact with cognitive machines and define their needs as they change. The technologies must also be able to interact with other processors, devices and cloud platforms.
                            Iterative and stateful. Cognitive computing technologies can ask questions and pull in additional data to identify or clarify a problem. They must be stateful in that they keep information about similar situations that have occurred previously.
                            Contextual. Understanding context is critical in thought processes. Cognitive systems must understand, identify and mine contextual data, such as syntax, time, location, domain, user requirements, user profiles, tasks and goals. The systems can draw on multiple sources of information, including structured and unstructured data and visual, auditory and sensor data.
    
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 08. Computer vision -->
                    <div class="card" id="computerVision">
                        <h3><li>Computer Vision</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn.prod.website-files.com/5d7b77b063a9066d83e1209c/60c12a6e084dab67d440a268_VuZKpISJC3C5ZjJ-jKPCFCDu5xiJJ595iCYtMGJguD6R6K5IDWBYm0tbMsBZwViACf73_AZrk9tAjVXemVNM-_ypZILqzf-7mRDS0x_4Nrr9DfIHv02-lD8qJCCNmWic4jaF63sO.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Computer vision is an interdisciplinary field of science and technology that focuses on how computers can gain understanding from images and videos. For AI engineers, computer vision allows them to automate activities that the human visual system typically performs.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Computer vision is a field of artificial intelligence (AI) that uses machine learning and neural networks to teach computers and systems to derive meaningful information from digital images, videos and other visual inputs—and to make recommendations or take actions when they see defects or issues.
    
                                Computer vision works much the same as human vision, except humans have a head start. Human sight has the advantage of lifetimes of context to train how to tell objects apart, how far away they are, whether they are moving or something is wrong with an image.
    
                                Computer vision trains machines to perform these functions, but it must do it in much less time with cameras, data and algorithms rather than retinas, optic nerves and a visual cortex. Because a system trained to inspect products or watch a production asset can analyze thousands of products or processes a minute, noticing imperceptible defects or issues, it can quickly surpass human capabilities.
    
                                Computer vision is used in industries that range from energy and utilities to manufacturing and automotive—and the market is continuing to grow. It is expected to reach USD 48.6 billion by 2022. <br>
    
                            <p class="fw-bold">How does computer vision work?</p>
                            Computer vision needs lots of data. It runs analyses of data over and over until it discerns distinctions and ultimately recognize images. For example, to train a computer to recognize automobile tires, it needs to be fed vast quantities of tire images and tire-related items to learn the differences and recognize a tire, especially one with no defects.
    
                            Two essential technologies are used to accomplish this: a type of machine learning called deep learning and a convolutional neural network (CNN).
    
                            Machine learning uses algorithmic models that enable a computer to teach itself about the context of visual data. If enough data is fed through the model, the computer will “look” at the data and teach itself to tell one image from another. Algorithms enable the machine to learn by itself, rather than someone programming it to recognize an image.
    
                            A CNN helps a machine learning or deep learning model “look” by breaking images down into pixels that are given tags or labels. It uses the labels to perform convolutions (a mathematical operation on two functions to produce a third function) and makes predictions about what it is “seeing.” The neural network runs convolutions and checks the accuracy of its predictions in a series of iterations until the predictions start to come true. It is then recognizing or seeing images in a way similar to humans.
    
                            Much like a human making out an image at a distance, a CNN first discerns hard edges and simple shapes, then fills in information as it runs iterations of its predictions. A CNN is used to understand single images. A recurrent neural network (RNN) is used in a similar way for video applications to help computers understand how pictures in a series of frames are related to one another.<br>
    
    
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 09. Data mining -->
                    <div class="card" id="dataVision">
                        <h3><li>Data mining</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://en.mdv.co.jp/ebm/wp-content/uploads/2022/08/column-17-01-1024x576.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Data mining is the process of sorting through large data sets to identify patterns that can improve models or solve problems. Data mining is the use of machine learning and statistical analysis to uncover patterns and other valuable information from large data sets.
                        </p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Given the evolution of machine learning (ML), data warehousing, and the growth of big data, the adoption of data mining, also known as knowledge discovery in databases (KDD), has rapidly accelerated over the last decades. However, while this technology continuously evolves to handle data at a large scale, leaders still might face challenges with scalability and automation.
    
                                The data mining techniques that underpin data analyses can be deployed for two main purposes. They can either describe the target data set or they can predict outcomes by using machine learning algorithms.
    
                                These methods are used to organize and filter data, surfacing the most useful information, from fraud to user behaviors, bottlenecks and even security breaches. Using ML algorithms and artificial intelligence (AI) enables automation of the analysis, which can greatly speed up the process.
    
                                When combined with data analytics and visualization tools, such as Apache Spark, data mining software is becoming more straightforward and extracting relevant insights can be gained more quickly than ever. Advances in AI continue to expedite adoption across industries.
                            <p class="fw-bold">Benefits and challenges</p>
                            Benefits
    
                            Discover hidden insights and trends: Data mining takes raw data and finds order in the chaos: seeing the forest for the trees. This can result in better-informed planning across corporate functions and industries, including advertising, finance, government, healthcare, human resources (HR), manufacturing, marketing, research, sales and supply chain management (SCM).
    
                            Save budget: By analyzing performance data from multiple sources, bottlenecks in business processes can be identified to speed resolution and increase efficiency.
    
                            Solve multiple challenges: Data mining is a versatile tool. Data from almost any source and any aspect of an organization can be analyzed to discover patterns and better ways of conducting business. Almost every department in an organization that collects and analyzes data can benefit from data mining.
    
                            Challenges
    
                            Complexity and risk: Useful insights require valid data, plus experts with coding experience. Knowledge of data mining languages including Python, R and SQL is helpful. An insufficiently cautious approach to data mining might result in misleading or dangerous results. Some consumer data used in data mining might be personally identifiable information (PII) which should be handled carefully to avoid legal or public relations issues.
    
                            Cost: For the best results, a wide and deep collection of data sets is often needed. If new information is to be gathered by an organization, setting up a data pipeline might represent a new expense. If data needs to be purchased from an outside source, that also imposes a cost.
    
                            Uncertainty: First, a major data mining effort might be well run, but produce unclear results, with no major benefit. Or inaccurate data can lead to incorrect insights, whether incorrect data was selected or the preprocessing was mishandled. Other risks include modeling errors or outdated data from a rapidly changing market.
    
                            Another potential problem is results might appear valid but are in fact random and not to be trusted. It’s important to remember that “correlation is not causation.” A famous example of “data dredging”—seeing an apparent correlation and overstating its importance—was recently presented by blogger Tyler Vigen: “The price of Amazon.com stock closely matches the number of children named ‘Stevie’ from 2002 to 2022.”1 But, of course, the naming of Stevies did not influence the stock price or vice versa. Data mining applications find the patterns, but human judgment is still significant.
    
                            Data mini<br>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 09. Data Augmentation -->
                    <div class="card" id="dataVision">
                        <h3><li>Data Augmentation</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://research.aimultiple.com/wp-content/uploads/2021/04/data-augmentation-techniques.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Techniques for increasing the amount and diversity of training data, especially in image processing.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Data augmentation is the process of artificially generating new data from existing data, primarily to train new machine learning (ML) models. ML models require large and varied datasets for initial training, but sourcing sufficiently diverse real-world datasets can be challenging because of data silos, regulations, and other limitations. Data augmentation artificially increases the dataset by making small changes to the original data. Generative artificial intelligence (AI) solutions are now being used for high-quality and fast data augmentation in various industries.

                            <p class="fw-bold">Why is data augmentation important?</p>
                            Deep learning models rely on large volumes of diverse data to develop accurate predictions in various contexts. Data augmentation supplements the creation of data variations that can help a model improve the accuracy of its predictions. Augmented data is vital in training.

                            Here are some of the benefits of data augmentation.

                            Enhanced model performance
                            Data augmentation techniques help enrich datasets by creating many variations of existing data. This provides a larger dataset for training and enables a model to encounter more diverse features. The augmented data helps the model better generalize to unseen data and improve its overall performance in real-world environments. 

                            Reduced data dependency
                            The collection and preparation of large data volumes for training can be costly and time-consuming. Data augmentation techniques increase the effectiveness of smaller datasets, vastly reducing the dependency on large datasets in training environments. You can use smaller datasets to supplement the set with synthetic data points.

                            Mitigate overfitting in training data
                            Data augmentation helps prevent overfitting when you’re training ML models. Overfitting is the undesirable ML behavior where a model can accurately provide predictions for training data but it struggles with new data. If a model trains only with a narrow dataset, it can become overfit and can give predictions related to only that specific data type. In contrast, data augmentation provides a much larger and more comprehensive dataset for model training. It makes training sets appear unique to deep neural networks, preventing them from learning to work with only specific characteristics. 

                            Read about overfitting

                            Read about neural networks

                            Improved data privacy
                            If you need to train a deep learning model on sensitive data, you can use augmentation techniques on the existing data to create synthetic data. This augmented data retains the input data's statistical properties and weights while protecting and limiting access to the original.<br>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 10. Data science -->
                    <div class="card" id="dataScience">
                        <h3><li>Data Science</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn1.vectorstock.com/i/1000x1000/64/65/data-science-infographic-10-option-concept-vector-27906465.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Data science is an interdisciplinary field of technology that uses algorithms and processes to gather and analyze large amounts of data to uncover patterns and insights that inform business decisions.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Data science is an in-demand career path for people with an aptitude for research, programming, math, and computers. Discover real-world applications and job opportunities in data science and what it takes to work in this exciting field.
    
                                Data science is an interdisciplinary field that uses algorithms, procedures, and processes to examine large amounts of data in order to uncover hidden patterns, generate insights, and direct decision-making. To create prediction models, data scientists use advanced machine learning algorithms to sort through, organize, and learn from structured and unstructured data.
    
                                As a fast-growing field with applications across numerous industries, data science offers a variety of job opportunities—from researching to computing. Explore how to use data science in the real world, the job outlook for the field, its required skills, and what credentials you need to land a job.
    
                            <p class="fw-bold">Applications of Data Science [Impact Across all Industries]</p>
                            Data scientists are having an impact in almost every industry.
    
                            Health care
                            eCommerce
                            Law enforcement
                            Marketing/advertising
                            Transportation
                            Sports
                            As expected, different sectors are using data science in different ways.<br>
    
                            Health care:
    
                            Identifying and predicting disease
                            Personalized health care recommendations
                            eCommerce
    
                            Automated “smart” ad placement
                            Personalized product recommendations
                            Law enforcement
    
                            Data-driven crime predictions
                            Facial recognition tools
                            Tax fraud enforcement
                            Transportation
    
                            Optimized shipping routes
                            Modeling the most effective traffic patterns and streetlight usage
                            Getting hot food delivered quickly
                            Chances are your favorite sports team may be dabbling in data science to help put together the best, most cost-effective team.
    
                            Why, data science is even at the heart of helping people find love — through online dating platforms powered by complex algorithms. <br>
    
                            <p class="fw-bold">Why is Data Science Important?</p>
    
                            Big Data may have the potential to change the world for the better but data science is essential because, according to training provider SimpliLearn, because “without the expertise of professionals who turn cutting-edge technology into actionable insights, Big Data is nothing.”
    
                            In “Why Data Science Matters And How It Powers Business Value,” the company details eight ways that data scientists can add value to business.
    
                            Empowering management to make better decisions
                            Directing actions and defining goals based on trends
                            Challenging staff to adopt best practices and focus on issues that matter
                            Identifying business opportunities
                            Decision making with quantifiable, data-driven evidence
                            Testing these decisions
                            Identifying and refining of target audiences
                            Recruiting the right talent
                            Data science has the potential to help nearly all organizations, according to Damien Deighan, CEO of Data Science Talent.
    
                            “With the ability to uncover hidden patterns, unknown correlations and build models that can make accurate predictions, data science can be used to help you make better business decisions for your organization,” says Deighan. “You can now analyze almost anything and everything in relation to your organization. Anything that can be logged via computer or network use can be analyzed and organized and turned into actionable insights. When applied and used correctly, data analytics can play a pivotal role in driving profitability and productivity.”
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
    
                    <!-- 11. Deep Learning -->
                    <div class="card" id="dataScience">
                        <h3><li>Deep Learning</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://img.freepik.com/premium-vector/deep-learning-algorithm-neural-network-ai-machine-learning-icons-infographic-design-layout-template-creative-presentation-concept-with-5-steps_159242-17895.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Deep learning is a function of AI that imitates the human brain by learning from how it structures and processes information to make decisions. Instead of relying on an algorithm that can only perform one specific task, this subset of machine learning can learn from unstructured data without supervision.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Deep learning is a method that trains computers to process information in a way that mimics human neural processes. Learn more about deep learning examples and applications in this article.
    
                            The field of artificial intelligence (AI) and machine learning (ML) is rapidly evolving, generating both fear and excitement. While many people have a general understanding of ML and AI, deep learning is a special type of machine learning that can be more challenging to describe.

                            You can learn more about deep learning systems and how to work with them in the following article, or start your journey with the popular course, Deep Learning Specialization from DeepLearning.AI.

    
    
                            
    
                            <p class="fw-bold">What is deep learning?</p>
                            Deep learning is a branch of machine learning that is made up of a neural network with three or more layers: <br>

                            Input layer: Data enters through the input layer. <br>

                            Hidden layers: Hidden layers process and transport data to other layers. <br>

                            Output layer: The final result or prediction is made in the output layer.<br>

                            Neural networks attempt to model human learning by digesting and analyzing massive amounts of information, also known as training data. They perform a given task with that data repeatedly, improving in accuracy each time. It's similar to the way we study and practice to improve skills. <br>
    
                            <p class="fw-bold">Deep learning models</p>
    
                            Deep learning models are files that data scientists train to perform tasks with minimal human intervention. Deep learning models include predefined sets of steps (algorithms) that tell the file how to treat certain data. This training method enables deep learning models to recognize more complicated patterns in text, images, or sounds.
                            </p>


                            <p class="fw-bold">Examples of deep learning</p>
                            Deep learning is a subset of machine learning that is made up of a neural network with three or more layers. A neural network attempts to model the human brain's behavior by learning from large data sets. Deep learning drives many AI applications that improve the way systems and tools deliver services, such as voice-enabled technology and credit card fraud detection.

                            Self-driving cars
                            Autonomous vehicles are already on our roadways. Deep learning algorithms help determine whether there are other cars, debris, or humans around and react accordingly.

                            Chatbots
                            Deep learning chatbots designed to mimic human intelligence (like Chat-GPT) have gained recent popularity due to their ability to respond to natural-language questions quickly and often accurately. The deeper the data pool from which deep learning occurs, the more rapidly deep learning can produce the desired results.

                            Facial recognition
                            Facial recognition plays an essential role in everything from tagging people on social media to crucial security measures. Deep learning allows algorithms to function accurately despite cosmetic changes such as hairstyles, beards, or poor lighting.

                            Medical science
                            The human genome consists of approximately three billion DNA base pairs of chromosomes. Machine learning is helping scientists and other medical professionals to create personalized medicines, and diagnose tumors, and is undergoing research and utilization for other pharmaceutical and medical purposes.

                            Speech recognition
                            Similar to facial recognition, deep learning uses millions of audio clips to learn and recognize speech. It can then power algorithms to understand what someone said and differentiate different tones, as well as detect a specific person's voice.


                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Early Stopping -->
                    <div class="card" id="">
                        <h3><li>Early Stopping</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://images.contentstack.io/v3/assets/bltb654d1b96a72ddc4/blt8a878304ad914fd8/672d446b20ed6c508fa50a47/SPC-Blog-What-is-Early-Stopping-in-Deep-Learning.jpg?format=webp" alt="early_stopping" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Early stopping is a regularization technique used in training machine learning models to prevent overfitting. It halts training once the model’s performance on a validation set stops improving.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Early stopping is one of the most popular and effective techniques for improving model generalization in machine learning. During training, a model can start to memorize the training data, which leads to poor performance on new, unseen data—a phenomenon known as overfitting. Early stopping allows the training process to terminate before the model has overfitted, thus preserving its ability to generalize.
                            </p>
                            <p class="fw-bold">How Early Stopping Works</p>
                            <p>
                            Early stopping works by monitoring the performance of the model on a separate validation set during training. At each epoch (training cycle), the model's accuracy or loss on the validation data is calculated. If the model’s performance does not improve over a predetermined number of epochs, the training stops. This “patience” parameter controls how long the model can continue without improvement.
                            </p>
                            <p class="fw-bold">Benefits of Early Stopping</p>
                            <ul>
                                <li><strong>Prevents Overfitting</strong>: By stopping training early, the model avoids learning noise and irrelevant patterns in the training data.</li>
                                <li><strong>Saves Computational Resources</strong>: Early stopping reduces the overall training time and computing cost by halting the process as soon as performance peaks.</li>
                                <li><strong>Improves Model Generalization</strong>: Helps the model perform better on new data, as it doesn’t overfit to the training data.</li>
                            </ul>
                            <p class="fw-bold">Applications of Early Stopping</p>
                            <p>
                            Early stopping is widely used in deep learning for tasks such as image classification, natural language processing, and predictive modeling. It is particularly beneficial in complex models like deep neural networks where the risk of overfitting is high.
                            </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Embedding -->
                    <div class="card" id="">
                        <h3><li>Embedding</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn.infodiagram.com/c/e1c702/embedding-ai-technologies.png" alt="embedding" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">An embedding is a learned representation of data in a lower-dimensional space, making it easier to analyze and process. It's commonly used in natural language processing (NLP) and recommendation systems.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Embedding is a technique used to convert categorical or high-dimensional data into a dense, continuous vector space. By mapping similar data points closer together in this lower-dimensional space, embeddings capture semantic relationships, such as word similarities in NLP.
                            </p>
                            <p class="fw-bold">Why Embeddings are Important</p>
                            <p>
                            Embeddings simplify data representation, making it possible for machine learning models to perform better on complex tasks like language understanding or item recommendation. They reduce computational resources and enhance model interpretability by capturing meaningful relationships within a lower-dimensional structure.
                            </p>
                            <p class="fw-bold">Applications of Embeddings</p>
                            <ul>
                                <li><strong>NLP</strong>: Representing words and phrases in vector form to capture linguistic relationships.</li>
                                <li><strong>Recommendation Systems</strong>: Finding similarities between users and items for personalized recommendations.</li>
                                <li><strong>Image Processing</strong>: Mapping visual features into vectors for tasks like image retrieval.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 12. Emergent Behaviour -->
                    <div class="card" id="dataScience">
                        <h3><li>Emergent Behaviour</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.digital-adoption.com/wp-content/uploads/2023/12/Risks-of-Emergent-AI-1024x497.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Emergent behavior, also called emergence, is when an AI system shows unpredictable or unintended capabilities.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Emergent behavior refers to actions or patterns that weren’t explicitly programmed into an AI system but developed as a natural outcome of its complexity and interactions.
                            Imagine a colony of ants. No single ant has the blueprint for the colony’s intricate behavior. Yet, collectively, they demonstrate complex activities like building nests or finding food. Similarly, in AI, emergent behavior occurs when simple rules or algorithms interact in a complex system, leading to outcomes that might surprise even the creators of the AI.

                            This phenomenon is significant in AI for many reasons. First, it pushes the boundaries of what AI can achieve, often leading to more efficient and adaptable systems.

                            It’s like giving AI a mind of its own but within the confines of its programming. Emergent behaviors can lead to AI systems solving problems in ways that were not preconceived, which can be both exciting and a bit scary.

                            Secondly, understanding and harnessing this emergent behavior is necessary for advancing AI technology. It’s about learning from the unexpected and using it to enhance AI capabilities. This could mean more advanced robotic systems, smarter AI in video games, or even more effective data analysis tools.

                            It’s not all smooth sailing, though. Emergent behavior can also pose challenges, especially in predicting and controlling AI systems. As AI becomes more integrated into important areas like healthcare, transportation, and security, we need to make sure that these emergent behaviors are understood and managed.
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Encoder-Decoder Model -->
                    <div class="card" id="">
                        <h3><li>Encoder-Decoder Model</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.comet.com/site/wp-content/uploads/2023/07/Screen-Shot-2023-07-11-at-9.48.50-PM.png" alt="encoder_decoder" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">An encoder-decoder model is a neural network architecture used for sequence-to-sequence tasks, such as machine translation and summarization.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            The encoder-decoder model is designed to handle input-output pairs of varying lengths, especially useful for tasks where sequences of words, sentences, or actions must be translated or processed. The encoder transforms the input into a latent space, while the decoder uses this information to generate the output sequence.
                            </p>
                            <p class="fw-bold">How Encoder-Decoder Models Work</p>
                            <p>
                            The encoder processes the input data into a fixed-size context vector. The decoder then uses this vector to predict the output sequence, one step at a time, making this approach ideal for tasks with sequential data.
                            </p>
                            <p class="fw-bold">Applications of Encoder-Decoder Models</p>
                            <ul>
                                <li><strong>Machine Translation</strong>: Converting text from one language to another.</li>
                                <li><strong>Text Summarization</strong>: Condensing long texts into concise summaries.</li>
                                <li><strong>Image Captioning</strong>: Generating descriptive captions for images.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Epoch -->
                    <div class="card" id="">
                        <h3><li>Epoch</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://media.geeksforgeeks.org/wp-content/uploads/20241024155237307614/epoch-in-machine-learning_.webp" alt="epoch" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">An epoch represents one complete pass through the entire training dataset during the training of a machine learning model.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            In machine learning, an epoch is the unit of training where the model has processed the entire training dataset once. Each epoch allows the model to learn patterns and improve predictions based on the training data, progressively minimizing error.
                            </p>
                            <p class="fw-bold">Why Multiple Epochs are Used</p>
                            <p>
                            Repeating the dataset through multiple epochs helps the model learn better by gradually adjusting weights in response to data patterns. Too few epochs can lead to underfitting, while too many can cause overfitting.
                            </p>
                            <p class="fw-bold">Key Considerations with Epochs</p>
                            <ul>
                                <li><strong>Model Performance</strong>: More epochs usually result in higher accuracy until reaching a point of diminishing returns.</li>
                                <li><strong>Training Time</strong>: More epochs mean more training time, which requires computational resources.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Exploding Gradient -->
                    <div class="card" id="">
                        <h3><li>Exploding Gradient</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://miro.medium.com/max/700/0*pq5wlxZW4zvD9iJH" alt="exploding_gradient" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Exploding gradient is a deep learning issue where gradients grow too large during backpropagation, destabilizing model training.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            The exploding gradient problem occurs when the gradients in a neural network become excessively large, leading to unstable updates to model parameters. This problem is common in deep networks, especially recurrent neural networks (RNNs), where long sequences increase the gradient magnitude significantly.
                            </p>
                            <p class="fw-bold">Why Exploding Gradients Occur</p>
                            <p>
                            As the model propagates errors backward, gradients are calculated for each layer. In very deep networks, these gradients can grow exponentially, causing extreme parameter updates and preventing the model from learning effectively.
                            </p>
                            <p class="fw-bold">Solutions to Exploding Gradients</p>
                            <ul>
                                <li><strong>Gradient Clipping</strong>: Restricting the maximum gradient value to control growth.</li>
                                <li><strong>Weight Regularization</strong>: Adding penalties to discourage excessively large weights.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Exploration vs. Exploitation -->
                    <div class="card" id="">
                        <h3><li>Exploration vs. Exploitation</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://miro.medium.com/v2/resize:fit:1400/1*A3lcdoeusXJqU-0G84wdmQ.png" alt="exploration_exploitation" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A trade-off in reinforcement learning between trying new actions (exploration) and using known actions for rewards (exploitation).</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            In reinforcement learning, the agent must decide between exploring new actions to discover more reward opportunities or exploiting known actions that already yield high rewards. This balance is essential for optimal decision-making in uncertain environments.
                            </p>
                            <p class="fw-bold">Why Exploration vs. Exploitation is Important</p>
                            <p>
                            Too much exploration can waste resources, while too much exploitation can lead to suboptimal long-term results. An ideal balance helps the agent learn effectively, maximizing its cumulative reward over time.
                            </p>
                            <p class="fw-bold">Strategies for Managing the Trade-Off</p>
                            <ul>
                                <li><strong>ε-Greedy</strong>: Exploring randomly with a small probability and exploiting the rest of the time.</li>
                                <li><strong>Upper Confidence Bound (UCB)</strong>: Selecting actions based on a confidence interval that considers both exploration and exploitation.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- F1 Score -->
                    <div class="card" id="">
                        <h3><li>F1 Score</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.sharpsightlabs.com/wp-content/uploads/2023/11/f1-score_simple-explanation.png" alt="f1_score" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">The F1 score is a metric that combines precision and recall, making it especially useful for evaluating models on imbalanced datasets.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            The F1 score is the harmonic mean of precision and recall, balancing the two metrics to give a more comprehensive evaluation of model performance. It's particularly helpful when there's a class imbalance, ensuring the model performs well on both true positives and false positives.
                            </p>
                            <p class="fw-bold">How F1 Score is Calculated</p>
                            <p>
                            The formula for F1 score is: <code>F1 = 2 * (precision * recall) / (precision + recall)</code>. A higher F1 score indicates a better balance between precision and recall.
                            </p>
                            <p class="fw-bold">When to Use the F1 Score</p>
                            <ul>
                                <li><strong>Imbalanced Datasets</strong>: Where accuracy alone may be misleading.</li>
                                <li><strong>Binary Classification</strong>: For evaluating how well the model handles positive and negative classes.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Feature Engineering -->
                    <div class="card" id="">
                        <h3><li>Feature Engineering</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://sofy.ai/wp-content/uploads/2024/06/Feature-Engineering-in-Machine-Learning.png" alt="feature_engineering" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Feature engineering is the process of selecting and transforming variables to improve a machine learning model’s performance.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Feature engineering involves creating, modifying, or selecting the most relevant features (variables) for a model to use, helping it learn patterns more effectively. Effective feature engineering can significantly boost a model's accuracy and robustness.
                            </p>
                            <p class="fw-bold">Key Steps in Feature Engineering</p>
                            <ul>
                                <li><strong>Feature Selection</strong>: Identifying and removing irrelevant or redundant features.</li>
                                <li><strong>Feature Transformation</strong>: Scaling, encoding, or decomposing features to make them more informative.</li>
                            </ul>
                            <p class="fw-bold">Examples of Feature Engineering</p>
                            <ul>
                                <li>Creating new features from existing ones, such as "age" from "birth year."</li>
                                <li>Encoding categorical variables into numerical values for model compatibility.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- GAN (Generative Adversarial Network) -->
                    <div class="card" id="">
                        <h3><li>GAN (Generative Adversarial Network)</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.shutterstock.com/image-vector/generative-adversarial-network-illustration-260nw-1649688995.jpg" alt="gan" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A GAN is a network composed of a generator and a discriminator, working together to create realistic data samples.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Generative Adversarial Networks (GANs) are a class of machine learning models where two networks—a generator and a discriminator—compete in a game. The generator tries to create realistic data samples, while the discriminator tries to distinguish real samples from fake ones.
                            </p>
                            <p class="fw-bold">How GANs Work</p>
                            <p>
                            The generator learns to create increasingly realistic samples to fool the discriminator. The discriminator, in turn, improves at identifying fake samples, resulting in high-quality data generation over time.
                            </p>
                            <p class="fw-bold">Applications of GANs</p>
                            <ul>
                                <li><strong>Image Synthesis</strong>: Creating realistic images for art, game design, or virtual environments.</li>
                                <li><strong>Data Augmentation</strong>: Generating additional training data to improve model performance.</li>
                                <li><strong>Deepfake Creation</strong>: Crafting realistic media by altering images or videos.</
                                </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Gradient Descent -->
                    <div class="card" id="">
                        <h3><li>Gradient Descent</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://media.licdn.com/dms/image/v2/D4E22AQEEjxfwLdc7VQ/feedshare-shrink_800/feedshare-shrink_800/0/1713251082465?e=2147483647&v=beta&t=GH3oODJUgDw3DGCDzwY0ouB-ZpzSB-pNod26fwXA6hY" alt="gradient_descent" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Gradient Descent is an optimization algorithm used to minimize the loss function by iteratively adjusting weights in the model.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Gradient Descent works by calculating the gradient (slope) of the loss function with respect to each model parameter and moving in the direction that decreases the loss. This iterative process continues until the model converges to an optimal set of weights.
                            </p>
                            <p class="fw-bold">Types of Gradient Descent</p>
                            <ul>
                                <li><strong>Batch Gradient Descent</strong>: Uses the entire dataset for each step.</li>
                                <li><strong>Stochastic Gradient Descent (SGD)</strong>: Updates weights for each individual sample, speeding up training.</li>
                                <li><strong>Mini-batch Gradient Descent</strong>: Combines batch and stochastic methods for more stable convergence.</li>
                            </ul>
                            <p class="fw-bold">Applications</p>
                            <ul>
                                <li>Training neural networks and other machine learning models.</li>
                                <li>Optimizing regression models and minimizing error.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 13. Generative AI -->
                    <div class="card" id="dataScience">
                        <h3><li>Generative AI</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://ediscoverytoday.com/wp-content/uploads/2023/11/2023GenerativeAILegalUseCases.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Generative AI is a type of technology that uses AI to create content, including text, video, code and images. A generative AI system is trained using large amounts of data, so that it can find patterns for generating new content.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Generative AI refers to deep-learning models that can generate high-quality text, images, and other content based on the data they were trained on.
                            Artificial intelligence has gone through many cycles of hype, but even to skeptics, the release of ChatGPT seems to mark a turning point. OpenAI’s chatbot, powered by its latest large language model, can write poems, tell jokes, and churn out essays that look like a human created them. Prompt ChatGPT with a few words, and out comes love poems in the form of Yelp reviews, or song lyrics in the style of Nick Cave.

                            The last time generative AI loomed this large, the breakthroughs were in computer vision. Selfies transformed into Renaissance-style portraits and prematurely aged faces filled social media feeds. Five years later, it’s the leap forward in natural language processing, and the ability of large language models to riff on just about any theme, that has seized the popular imagination. And it’s not just language: Generative models can also learn the grammar of software code, molecules, natural images, and a variety of other data types.

                            The applications for this technology are growing every day, and we’re just starting to explore the possibilities. At IBM Research, we’re working to help our customers use generative models to write high-quality software code faster, discover new molecules, and train trustworthy conversational chatbots grounded on enterprise data. We’re even using generative AI to create synthetic data to build more robust and trustworthy AI models and to stand-in for real data protected by privacy and copyright laws.

                            As the field continues to evolve, we thought we’d take a step back and explain what we mean by generative AI, how we got here, and how these models work.

                            <p class="fw-bold">The rise of deep generative models</p>
                            Generative AI refers to deep-learning models that can take raw data — say, all of Wikipedia or the collected works of Rembrandt — and “learn” to generate statistically probable outputs when prompted. At a high level, generative models encode a simplified representation of their training data and draw from it to create a new work that’s similar, but not identical, to the original data.

                            Generative models have been used for years in statistics to analyze numerical data. The rise of deep learning, however, made it possible to extend them to images, speech, and other complex data types. Among the first class of models to achieve this cross-over feat were variational autoencoders, or VAEs, introduced in 2013. VAEs were the first deep-learning models to be widely used for generating realistic images and speech.

                            “VAEs opened the floodgates to deep generative modeling by making models easier to scale,” said Akash Srivastava, an expert on generative AI at the MIT-IBM Watson AI Lab. “Much of what we think of today as generative AI started here.”

                            Autoencoders work by encoding unlabeled data into a compressed representation, and then decoding the data back into its original form. “Plain” autoencoders were used for a variety of purposes, including reconstructing corrupted or blurry images. Variational autoencoders added the critical ability to not just reconstruct data, but to output variations on the original data.

                            This ability to generate novel data ignited a rapid-fire succession of new technologies, from generative adversarial networks (GANs) to diffusion models, capable of producing ever more realistic — but fake — images. In this way, VAEs set the stage for today’s generative AI.

                            They are built out of blocks of encoders and decoders, an architecture that also underpins today’s large language models. Encoders compress a dataset into a dense representation, arranging similar data points closer together in an abstract space. Decoders sample from this space to create something new while preserving the dataset’s most important features.

                            Transformers, introduced by Google in 2017 in a landmark paper “Attention Is All You Need,” combined the encoder-decoder architecture with a text-processing mechanism called attention to change how language models were trained. An encoder converts raw unannotated text into representations known as embeddings; the decoder takes these embeddings together with previous outputs of the model, and successively predicts each word in a sentence.

                            Through fill-in-the-blank guessing games, the encoder learns how words and sentences relate to each other, building up a powerful representation of language without anyone having to label parts of speech and other grammatical features. Transformers, in fact, can be pre-trained at the outset without a particular task in mind. Once these powerful representations are learned, the models can later be specialized — with much less data — to perform a given task.

                            Several innovations made this possible. Transformers processed words in a sentence all at once, allowing text to be processed in parallel, speeding up training. Earlier techniques like recurrent neural networks (RNNs) and Long Short-Term Memory (LSTM) networks processed words one by one. Transformers also learned the positions of words and their relationships, context that allowed them to infer meaning and disambiguate words like “it” in long sentences.

                            By eliminating the need to define a task upfront, transformers made it practical to pre-train language models on vast amounts of raw text, allowing them to grow dramatically in size. Previously, people gathered and labeled data to train one model on a specific task. With transformers, you could train one model on a massive amount of data and then adapt it to multiple tasks by fine-tuning it on a small amount of labeled task-specific data.

                            Transformers have come to be known as foundation models for their versatility. “If you wanted to improve a classifier, you used to have to feed it more labeled data,” said Srivastava. “Now, with foundation models, you can feed the model large amounts of unlabeled data to learn a representation that generalizes well to many tasks.”

                            Language transformers today are used for non-generative tasks like classification and entity extraction as well as generative tasks like translation, summarization, and question answering. More recently, transformers have stunned the world with their capacity to generate convincing dialogue, essays, and other content.

                            Language transformers fall into three main categories: encoder-only models, decoder-only models, and encoder-decoder models.

                            Encoder-only models like BERT power search engines and customer-service chatbots, including IBM’s Watson Assistant. Encoder-only models are widely used for non-generative tasks like classifying customer feedback and extracting information from long documents. In a project with NASA, IBM is building an encoder-only model to mine millions of earth-science journals for new knowledge.

                            Decoder-only models like the GPT family of models are trained to predict the next word without an encoded representation. GPT-3, at 175 billion parameters, was the largest language model of its kind when OpenAI released it in 2020. Other massive models — Google’s PaLM (540 billion parameters) and open-access BLOOM (176 billion parameters), among others, have since joined the scene.

                            Encoder-decoder models, like Google’s Text-to-Text Transfer Transformer, or T5, combine features of both BERT and GPT-style models. They can do many of the generative tasks that decoder-only models can, but their compact size makes them faster and cheaper to tune and serve.

                            Generative AI and large language models have been progressing at a dizzying pace, with new models, architectures, and innovations appearing almost daily.

    
            
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 14. Guardrails -->
                    <div class="card" id="dataScience">
                        <h3><li>Guardrails</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://media.licdn.com/dms/image/v2/D4D12AQEowMU_fw0Jtw/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1708006841675?e=2147483647&v=beta&t=LGAiZ_AB2hgGNYrRHsDgogjO92AA5dTOWlqxxee3s4M" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">An AI guardrail is a safeguard that is put in place to prevent artificial intelligence (AI) from causing harm. AI guardrails are a lot like highway guardrails – they are both created to keep people safe and guide positive outcomes.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                DAs AI evolves, guardrails are becoming increasingly important for maintaining public trust in AI and ensuring that AI-enabled technology operates safely within ethical and legal boundaries.

                                Creating AI guardrails that everyone agrees with is challenging, however, due to the rapid pace of technological advancement, different legal systems around the world, and the difficulty of balancing AI innovation with the need for privacy, fairness, and public safety.
    
                            <p class="fw-bold">Why Do We Need AI Guardrails?</p>
                            AI guardrails are a critical component of AI governance and the development, deployment and use of responsible AI.

                            In the past year, guardrails have often been mentioned in the context of generative AI, but it’s important to remember that safeguards are vital considerations for any type of AI system that can make a decision autonomously. 

                            This includes relatively simple machine learning (ML) algorithms that decide between two choices, as well as multimodal AI systems whose decisions can have literally billions of potential outcomes.

                            As the world has seen, when guardrails don’t exist, AI technology can perpetuate biases, create new concerns about privacy, make erroneous or unethical decisions that directly impact people’s lives, and get misused for harmful purposes. 

                            It’s no surprise that this, in turn, has led many people to mistrust AI. 
    
                            <p class="fw-bold">Who Is Responsible For Creating AI Guardrails?</p>
    
                            The creation and implementation of AI guardrails is a collaborative effort that involves a diverse group of stakeholders. This includes:

                            Big Tech companies and AI startups;
                            Business leaders;
                            AI researchers;
                            Civic organizations;
                            Ethicists;
                            Professional organizations;
                            Government agencies;
                            Prominent global organizations;
                            Legal experts. 
                            Each of these stakeholders brings a unique perspective and skill set to the table, which in theory, should contribute to a more holistic approach to the development of guardrails for specific types of artificial intelligence.

                            The challenge is that when stakeholder interests are too diverse, it can be difficult to reconcile competing priorities and find a balance that everyone can live with. Too many guardrails can stifle innovation, and too few guardrails can leave the door open for harmful consequences that undermine safety and public trust.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 15. Hallucination -->
                    <div class="card" id="dataScience">
                        <h3><li>Hallucination</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.aporia.com/wp-content/webp-express/webp-images/uploads/2024/02/image-32.png.webp" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">An AI hallucination is when an artificial intelligence (AI) model produces incorrect or misleading results. This can happen when an AI model: Has insufficient training data, Makes incorrect assumptions, Uses biased data for training, Processes data incorrectly, and Misapplies learned patterns.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            AI hallucinations are incorrect or misleading results that AI models generate. These errors can be caused by a variety of factors, including insufficient training data, incorrect assumptions made by the model, or biases in the data used to train the model. AI hallucinations can be a problem for AI systems that are used to make important decisions, such as medical diagnoses or financial trading.
                            <p class="fw-bold">How do AI hallucinations occur?</p>
                            AI models are trained on data, and they learn to make predictions by finding patterns in the data. However, the accuracy of these predictions often depends on the quality and completeness of the training data. If the training data is incomplete, biased, or otherwise flawed, the AI model may learn incorrect patterns, leading to inaccurate predictions or hallucinations.

                            For example, an AI model that is trained on a dataset of medical images may learn to identify cancer cells. However, if the dataset does not include any images of healthy tissue, the AI model may incorrectly predict that healthy tissue is cancerous. 

                            Flawed training data is just one reason why AI hallucinations can occur. Another factor that may contribute is a lack of proper grounding. An AI model may struggle to accurately understand real-world knowledge, physical properties, or factual information. This lack of grounding can cause the model to generate outputs that, while seemingly plausible, are actually factually incorrect, irrelevant, or nonsensical. This can even extend to fabricating links to web pages that never existed.

                            An example of this would be an AI model designed to generate summaries of news articles may produce a summary that includes details not present in the original article, or even fabricates information entirely. 

                            Understanding these potential causes of AI hallucinations is important for developers working with AI models. By carefully considering the quality and completeness of training data, as well as ensuring proper grounding, developers may minimize the risk of AI hallucinations and ensure the accuracy and reliability of their models. <br>
    
                            <p class="fw-bold">Examples of AI hallucinations</p>
    
                            AI hallucinations can take many different forms. Some common examples include:

                            Incorrect predictions: An AI model may predict that an event will occur when it is unlikely to happen. For example, an AI model that is used to predict the weather may predict that it will rain tomorrow when there is no rain in the forecast.
                            False positives: When working with an AI model, it may identify something as being a threat when it is not. For example, an AI model that is used to detect fraud may flag a transaction as fraudulent when it is not.
                            False negatives: An AI model may fail to identify something as being a threat when it is. For example, an AI model that is used to detect cancer may fail to identify a cancerous tumor.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Hierarchical Clustering -->
                    <div class="card" id="">
                        <h3><li>Hierarchical Clustering</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://images.datacamp.com/image/upload/v1674149823/Dendrogram_of_Divisive_Clustering_Approach_8623354c7b.png" alt="hierarchical_clustering" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Hierarchical Clustering is a clustering technique that groups data in a tree-like structure based on similarities.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            In Hierarchical Clustering, data points are successively merged (agglomerative) or split (divisive) to form a hierarchy or tree-like structure called a dendrogram. This method is especially useful for understanding data relationships.
                            </p>
                            <p class="fw-bold">Types of Hierarchical Clustering</p>
                            <ul>
                                <li><strong>Agglomerative</strong>: Starts with each data point as an individual cluster and merges them step by step.</li>
                                <li><strong>Divisive</strong>: Begins with all data points in a single cluster, splitting them iteratively.</li>
                            </ul>
                            <p class="fw-bold">Applications</p>
                            <ul>
                                <li>Organizing data in taxonomies, such as biological classifications.</li>
                                <li>Customer segmentation and market research.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 16. Hyperparameter -->
                    <div class="card" id="dataScience">
                        <h3><li>Hyperparameter</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn.labellerr.com/ML%20Process/Everything%20you%20need%20to%20know%20about%20AI%20Model%20Training/Hyperparameter%20Tuning.webp" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A hyperparameter is a parameter whose value is set before the machine learning process begins. In contrast, the values of other parameters are derived via training. Algorithm hyperparameters affect the speed and quality of the learning process.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Hyperparameters in Machine learning are those parameters that are explicitly defined by the user to control the learning process. These hyperparameters are used to improve the learning of the model, and their values are set before starting the learning process of the model.
                            <p class="fw-bold">What are hyperparameters?</p>
                            In Machine Learning/Deep Learning, a model is represented by its parameters. In contrast, a training process involves selecting the best/optimal hyperparameters that are used by learning algorithms to provide the best result. So, what are these hyperparameters? The answer is, "Hyperparameters are defined as the parameters that are explicitly defined by the user to control the learning process."

                            Here the prefix "hyper" suggests that the parameters are top-level parameters that are used in controlling the learning process. The value of the Hyperparameter is selected and set by the machine learning engineer before the learning algorithm begins training the model. Hence, these are external to the model, and their values cannot be changed during the training process.

                            <p class="fw-bold">Some examples of Hyperparameters in Machine Learning</p>
    
                            The k in kNN or K-Nearest Neighbour algorithm
                            Learning rate for training a neural network
                            Train-test split ratio
                            Batch Size
                            Number of Epochs
                            Branches in Decision Tree
                            Number of clusters in Clustering Algorithm
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Hyperparameter Tuning -->
                    <div class="card" id="">
                        <h3><li>Hyperparameter Tuning</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://d3lkc3n5th01x7.cloudfront.net/wp-content/uploads/2024/01/10235944/What-is-hyperparameter-tuning.png" alt="hyperparameter_tuning" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Hyperparameter Tuning is the process of selecting optimal hyperparameters to improve model performance.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Hyperparameters are the configuration settings that control the learning process, such as learning rate, batch size, and regularization parameters. Tuning these parameters helps achieve the best possible performance for a given model.
                            </p>
                            <p class="fw-bold">Common Hyperparameter Tuning Methods</p>
                            <ul>
                                <li><strong>Grid Search</strong>: Exhaustively searching over a specified parameter grid.</li>
                                <li><strong>Random Search</strong>: Randomly sampling parameter values within specified ranges.</li>
                                <li><strong>Bayesian Optimization</strong>: Using probabilistic models to guide the search.</li>
                            </ul>
                            <p class="fw-bold">Applications</p>
                            <ul>
                                <li>Improving accuracy and stability of machine learning models.</li>
                                <li>Optimizing deep learning architectures for specific tasks.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Instance Segmentation -->
                    <div class="card" id="">
                        <h3><li>Instance Segmentation</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://framerusercontent.com/images/YumWmAYZeguGhURTsoF2t8nvqA.png" alt="instance_segmentation" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Instance Segmentation is a technique in computer vision that classifies each pixel of an image into object instances.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Instance segmentation not only identifies the object class but also distinguishes each instance within that class. For example, in an image with multiple cars, it will segment each car individually rather than marking all pixels as “car.”
                            </p>
                            <p class="fw-bold">How Instance Segmentation Works</p>
                            <p>
                            This method combines object detection (localizing objects) and semantic segmentation (classifying pixels) to provide a detailed understanding of each object instance in an image.
                            </p>
                            <p class="fw-bold">Applications</p>
                            <ul>
                                <li>Autonomous driving, for detecting and separating objects like pedestrians and vehicles.</li>
                                <li>Medical imaging, for isolating and analyzing individual cells or lesions.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 17. Image recognition -->
                    <div class="card" id="dataScience">
                        <h3><li>Image Recognition</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://azati.ai/wp-content/uploads/2020/04/object-detection-800x400-1.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A hyperparameter is a parameter whose value is set before the machine learning process begins. In contrast, the values of other parameters are derived via training. Algorithm hyperparameters affect the speed and quality of the learning process.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Hyperparameters in Machine learning are those parameters that are explicitly defined by the user to control the learning process. These hyperparameters are used to improve the learning of the model, and their values are set before starting the learning process of the model.
                            <p class="fw-bold">What are hyperparameters?</p>
                            In Machine Learning/Deep Learning, a model is represented by its parameters. In contrast, a training process involves selecting the best/optimal hyperparameters that are used by learning algorithms to provide the best result. So, what are these hyperparameters? The answer is, "Hyperparameters are defined as the parameters that are explicitly defined by the user to control the learning process."

                            Here the prefix "hyper" suggests that the parameters are top-level parameters that are used in controlling the learning process. The value of the Hyperparameter is selected and set by the machine learning engineer before the learning algorithm begins training the model. Hence, these are external to the model, and their values cannot be changed during the training process.

                            <p class="fw-bold">Some examples of Hyperparameters in Machine Learning</p>
    
                            The k in kNN or K-Nearest Neighbour algorithm
                            Learning rate for training a neural network
                            Train-test split ratio
                            Batch Size
                            Number of Epochs
                            Branches in Decision Tree
                            Number of clusters in Clustering Algorithm
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- k-Nearest Neighbors (k-NN) -->
                    <div class="card" id="">
                        <h3><li>k-Nearest Neighbors (k-NN)</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://images.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning2.png" alt="k_nearest_neighbors" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">k-Nearest Neighbors (k-NN) is a simple algorithm that classifies data based on the majority class of its nearest neighbors.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            The k-NN algorithm finds the 'k' closest data points to a new point and assigns it the most common label among those neighbors. It’s a non-parametric, instance-based learning technique often used for classification and regression tasks.
                            </p>
                            <p class="fw-bold">How k-NN Works</p>
                            <p>
                            For each new data point, k-NN calculates the distance to all other data points, selects the closest 'k' points, and assigns the label based on majority voting.
                            </p>
                            <p class="fw-bold">Applications</p>
                            <ul>
                                <li>Image recognition, for comparing pixel patterns.</li>
                                <li>Recommendation systems, for finding similar users or products.</li>
                                <li>Medical diagnosis, for classifying patients based on similar cases.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 18. Large language model -->
                    <div class="card" id="dataScience">
                        <h3><li>Large Language Model</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://pbs.twimg.com/media/GM9UJKOXIAAMINO.jpg:large" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A large language model (LLM) is an AI model that has been trained on large amounts of text so that it can understand language and generate human-like text.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Hyperparameters in Machine learning are those parameters that are explicitly defined by the user to control the learning process. These hyperparameters are used to improve the learning of the model, and their values are set before starting the learning process of the model.
                            <p class="fw-bold">What are LLMs?</p>
                            Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks.

                            LLMs have become a household name thanks to the role they have played in bringing generative AI to the forefront of the public interest, as well as the point on which organizations are focusing to adopt artificial intelligence across numerous business functions and use cases.

                            Outside of the enterprise context, it may seem like LLMs have arrived out of the blue along with new developments in generative AI. However, many companies, including IBM, have spent years implementing LLMs at different levels to enhance their natural language understanding (NLU) and natural language processing (NLP) capabilities. This has occurred alongside advances in machine learning, machine learning models, algorithms, neural networks and the transformer models that provide the architecture for these AI systems.

                            LLMs are a class of foundation models, which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark contrast to the idea of building and training domain specific models for each of these use cases individually, which is prohibitive under many criteria (most importantly cost and infrastructure), stifles synergies and can even lead to inferior performance.

                            LLMs represent a significant breakthrough in NLP and artificial intelligence, and are easily accessible to the public through interfaces like Open AI’s Chat GPT-3 and GPT-4, which have garnered the support of Microsoft. Other examples include Meta’s Llama models and Google’s bidirectional encoder representations from transformers (BERT/RoBERTa) and PaLM models. IBM has also recently launched its Granite model series on watsonx.ai, which has become the generative AI backbone for other IBM products like watsonx Assistant and watsonx Orchestrate. 

                            In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or code generation tasks. 

                            They are able to do this thanks to billions of parameters that enable them to capture intricate patterns in language and perform a wide array of language-related tasks. LLMs are revolutionizing applications in various fields, from chatbots and virtual assistants to content generation, research assistance and language translation.

                            As they continue to evolve and improve, LLMs are poised to reshape the way we interact with technology and access information, making them a pivotal part of the modern digital landscape.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Latent Variable -->
                    <div class="card" id="">
                        <h3><li>Latent Variable</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.fastercapital.com/i/Latent-Variables--Uncovering-the-Unseen--Latent-Variables-in-Factor-Analysis--Future-Directions-in-Latent-Variable-Research.webp" alt="latent_variable" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Latent Variable is a hidden or unobserved variable inferred from observed data, often used in probabilistic models.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Latent variables are variables that are not directly observed but are inferred from other variables that are observed. They are commonly used in probabilistic and statistical models, such as factor analysis and Bayesian networks.
                            </p>
                            <p class="fw-bold">Uses of Latent Variables</p>
                            <ul>
                                <li>In psychometrics to measure underlying traits like intelligence or personality.</li>
                                <li>In machine learning for dimensionality reduction and feature extraction.</li>
                            </ul>
                            <p class="fw-bold">Applications</p>
                            <ul>
                                <li>Used in models like Latent Dirichlet Allocation (LDA) for topic modeling.</li>
                                <li>Helps in understanding underlying structures in data, such as clustering in unsupervised learning.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Logistic Regression -->
                    <div class="card" id="">
                        <h3><li>Logistic Regression</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.edushots.com/upload/infographics-images/original/logistic.png" alt="logistic_regression" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Logistic Regression is a regression model used for binary classification tasks.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Logistic regression is a statistical model that uses a logistic function to model a binary dependent variable. Instead of predicting a continuous value, it outputs probabilities that can be thresholded to predict binary outcomes.
                            </p>
                            <p class="fw-bold">How Logistic Regression Works</p>
                            <p>
                            Logistic regression calculates the odds of a particular class and applies the sigmoid function to convert the result into a probability between 0 and 1.
                            </p>
                            <p class="fw-bold">Applications</p>
                            <ul>
                                <li>Spam detection in emails, by classifying messages as spam or not spam.</li>
                                <li>Medical diagnosis, such as predicting the presence or absence of a disease.</li>
                                <li>Customer churn prediction in business, identifying customers likely to leave.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 19. Machine learning -->
                    <div class="card" id="dataScience">
                        <h3><li>Machine Learning</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://pbs.twimg.com/media/CLyZCTBW8AA2qsV.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Machine learning is a subset of AI that incorporates aspects of computer science, mathematics, and coding. Machine learning focuses on developing algorithms and models that help machines learn from data and predict trends and behaviors, without human assistance.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Machine learning is one of the most trendy fields in technology today. It fuels the technology behind Netflix recommendations and the speech-to-text recognition on your smartphone. A mix of math, computer science, and coding, a career in machine learning requires extensive education and training to land a job as an engineer.

                            So, is machine learning hard to learn? You'll need to learn programming languages like Python, practice using and modifying algorithms, and keeping up with trends in AI. There are plenty of educational resources online, such as courses and specializations, to gain the skills and experience you need for a career in machine learning.

                            Use this guide to decide if machine learning is right for you and if it's "hard" to learn. We'll help you map out your career path in machine learning.


                            <p class="fw-bold">What is machine learning?</p>
                            Machine learning is a branch of artificial intelligence that imitates how humans learn. It is also a division of computer science that uses algorithms and data to adjust its actions as it gathers more information.

                            Machine learning is used in many applications we use daily. Voice-to-text technology, which iPhones and Androids use, is created with machine learning—specifically deep learning—because it analyzes speech and translates to text based on the software’s established knowledge of how audio can be interpreted as language.

                            <p class="fw-bold">The importance of machine learning</p>
    
                            Machine learning can automate simple tasks, such as data entry or compiling contact information lists into a particular format. It can also make significant technological changes, such as dynamic pricing for event tickets or public transportation delay alerts. The following explains in more detail the benefits and advantages of machine learning.

                            Automating manual tasks: Machine learning programs aim to automate tasks and draw conclusions from data sets more quickly than humans could by manually analyzing it. It also saves us a lot of time.

                            Spotting trends and patterns: Machine learning detects patterns in data and recommends actions based on those patterns. Netflix's algorithm spots patterns in your TV watching to recommend shows that you will like based on your preferences.

                            Range of applications: From "smart homes" to self-driving cars, machine learning informs many recent groundbreaking innovations in technology.

                            Constant improvement: Careful attention to an algorithm and the data sets fed into it, as well as the use of programming languages such as Python, can identify areas of improvement for a machine learning application to offer quality assurance. Adjusting an algorithm as often as possible helps uphold AI ethics to establish avoidable bias.

                            Rapid handling of multi-dimensional data: Machine learning applications allow us to analyze data and draw conclusions at a faster pace and a higher level of sophistication than humans can do on their own. For example, banks use AI to detect money laundering or fraud. To achieve this without machines would require too many employees, who would likely miss a significant amount of illicit activity.
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- Markov Decision Process (MDP) -->
                    <div class="card" id="">
                        <h3><li>Markov Decision Process (MDP)</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.slideteam.net/media/catalog/product/cache/1280x720/m/a/markov_decision_process_with_business_cases_unlocking_ai_potential_ppt_example_ai_ss_v_slide01.jpg" alt="markov_decision_process" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Markov Decision Process (MDP) is a framework for modeling decision-making in reinforcement learning.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            An MDP provides a mathematical framework for modeling environments in reinforcement learning. It consists of states, actions, rewards, and transition probabilities, and it assumes that future states depend only on the current state and action, following the Markov property.
                            </p>
                            <p class="fw-bold">Components of an MDP</p>
                            <ul>
                                <li><strong>States (S)</strong>: The possible situations the agent can be in.</li>
                                <li><strong>Actions (A)</strong>: Choices available to the agent in each state.</li>
                                <li><strong>Rewards (R)</strong>: Immediate returns received after taking actions.</li>
                                <li><strong>Transition probabilities (P)</strong>: Probabilities of moving between states.</li>
                            </ul>
                            <p class="fw-bold">Applications</p>
                            <ul>
                                <li>Used in robotics for path planning and decision-making.</li>
                                <li>Helps in game theory to model environments in video games and AI strategies.</li>
                                <li>Applied in operations research for optimizing business processes and resource allocation.</li>
                            </ul>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 20. Natural language processing -->
                    <div class="card" id="dataScience">
                        <h3><li>Natural language processing</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://f5b623aa.rocketcdn.me/wp-content/uploads/2022/10/How-NLP-Works-760px.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Natural language processing (NLP) is a type of AI that enables computers to understand spoken and written human language. NLP enables features like text and speech recognition on devices.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Natural language processing (NLP) is a form of artificial intelligence (AI) that allows computers to understand human language, whether it be written, spoken, or even scribbled. As AI-powered devices and services become increasingly more intertwined with our daily lives and world, so too does the impact that NLP has on ensuring a seamless human-computer experience.
                            <p class="fw-bold">Natural language techniques </p>
                            NLP encompasses a wide range of techniques to analyze human language. Some of the most common techniques you will likely encounter in the field include:

                            Sentiment analysis: An NLP technique that analyzes text to identify its sentiments, such as “positive,” “negative,” or “neutral.” Sentiment analysis is commonly used by businesses to better understand customer feedback. 

                            Summarization: An NLP technique that summarizes a longer text, in order to make it more manageable for time-sensitive readers. Some common texts that are summarized include reports and articles. 

                            Keyword extraction: An NLP technique that analyzes a text to identify the most important keywords or phrases. Keyword extraction is commonly used for search engine optimization (SEO), social media monitoring, and business intelligence purposes. 

                            Tokenization: The process of breaking characters, words, or subwords down into “tokens” that can be analyzed by a program. Tokenization undergirds common NLP tasks like word modeling, vocabulary building, and frequent word occurrence. 
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- Neural network -->
                    <div class="card" id="dataScience">
                        <h3><li>Neural Network</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.shutterstock.com/image-vector/ai-infographic-template-artificial-intelligence-260nw-2415784423.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A neural network is a deep learning technique designed to resemble the human brain’s structure. Neural networks require large data sets to perform calculations and create outputs, which enables features like speech and vision recognition.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            A neural network is a method in artificial intelligence (AI) that teaches computers to process data in a way that is inspired by the human brain. It is a type of machine learning (ML) process, called deep learning, that uses interconnected nodes or neurons in a layered structure that resembles the human brain. It creates an adaptive system that computers use to learn from their mistakes and improve continuously. Thus, artificial neural networks attempt to solve complicated problems, like summarizing documents or recognizing faces, with greater accuracy.
                            <p class="fw-bold">What are neural networks used for?</p>
                            Neural networks have several use cases across many industries, such as the following:

                            Medical diagnosis by medical image classification
                            Targeted marketing by social network filtering and behavioral data analysis
                            Financial predictions by processing historical data of financial instruments
                            Electrical load and energy demand forecasting
                            Process and quality control
                            Chemical compound identification
                            We give four of the important applications of neural networks below.

                            Computer vision
                            Computer vision is the ability of computers to extract information and insights from images and videos. With neural networks, computers can distinguish and recognize images similar to humans. Computer vision has several applications, such as the following:

                            Visual recognition in self-driving cars so they can recognize road signs and other road users
                            Content moderation to automatically remove unsafe or inappropriate content from image and video archives
                            Facial recognition to identify faces and recognize attributes like open eyes, glasses, and facial hair
                            Image labeling to identify brand logos, clothing, safety gear, and other image details
                            Speech recognition
                            Neural networks can analyze human speech despite varying speech patterns, pitch, tone, language, and accent. Virtual assistants like Amazon Alexa and automatic transcription software use speech recognition to do tasks like these:

                            Assist call center agents and automatically classify calls
                            Convert clinical conversations into documentation in real time
                            Accurately subtitle videos and meeting recordings for wider content reach
                            Natural language processing
                            Natural language processing (NLP) is the ability to process natural, human-created text. Neural networks help computers gather insights and meaning from text data and documents. NLP has several use cases, including in these functions:

                            Automated virtual agents and chatbots
                            Automatic organization and classification of written data
                            Business intelligence analysis of long-form documents like emails and forms
                            Indexing of key phrases that indicate sentiment, like positive and negative comments on social media
                            Document summarization and article generation for a given topic
                            Recommendation engines
                            Neural networks can track user activity to develop personalized recommendations. They can also analyze all user behavior and discover new products or services that interest a specific user. For example, Curalate, a Philadelphia-based startup, helps brands convert social media posts into sales. Brands use Curalate’s intelligent product tagging (IPT) service to automate the collection and curation of user-generated social content. IPT uses neural networks to automatically find and recommend products relevant to the user’s social media activity. Consumers don't have to hunt through online catalogs to find a specific product from a social media image. Instead, they can use Curalate’s auto product tagging to purchase the product with ease.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 22. Overfitting -->
                    <div class="card" id="dataScience">
                        <h3><li>Overfitting</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://365datascience.com/resources/blog/x671k7dla1f-overfitting-vs-underfitting-classification-examples.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Overfitting occurs in machine learning training when the algorithm can only work on specific examples within the training data. A typical functioning AI model should be able to generalize patterns in the data to tackle new tasks.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Overfitting is an undesirable machine learning behavior that occurs when the machine learning model gives accurate predictions for training data but not for new data. When data scientists use machine learning models for making predictions, they first train the model on a known data set. Then, based on this information, the model tries to predict outcomes for new data sets. An overfit model can give inaccurate predictions and cannot perform well for all types of new data.
                            <p class="fw-bold">Why does overfitting occur?</p>
                            You only get accurate predictions if the machine learning model generalizes to all types of data within its domain. Overfitting occurs when the model cannot generalize and fits too closely to the training dataset instead. Overfitting happens due to several reasons, such as:
                            •    The training data size is too small and does not contain enough data samples to accurately represent all possible input data values.
                            •    The training data contains large amounts of irrelevant information, called noisy data.
                            •    The model trains for too long on a single sample set of data.
                            •    The model complexity is high, so it learns the noise within the training data.

                            Overfitting examples
                            Consider a use case where a machine learning model has to analyze photos and identify the ones that contain dogs in them. If the machine learning model was trained on a data set that contained majority photos showing dogs outside in parks , it may may learn to use grass as a feature for classification, and may not recognize a dog inside a room.
                            Another overfitting example is a machine learning algorithm that predicts a university student's academic performance and graduation outcome by analyzing several factors like family income, past academic performance, and academic qualifications of parents. However, the test data only includes candidates from a specific gender or ethnic group. In this case, overfitting causes the algorithm's prediction accuracy to drop for candidates with gender or ethnicity outside of the test dataset.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 23. Pattern recognition -->
                    <div class="card" id="dataScience">
                        <h3><li>Pattern Recognition</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://aiperceiver.com/wp-content/uploads/2024/06/Benefits-of-Pattern-Recognition-in-AI-1024x1024.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Pattern is everything around in this digital world. A pattern can either be seen physically or it can be observed mathematically by applying algorithms. 

                        Example: The colors on the clothes, speech pattern, etc. In computer science, a pattern is represented using vector feature values. </p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Pattern recognition is the method of using computer algorithms to analyze, detect, and label regularities in data. This informs how the data gets classified into different categories.
                            <p class="fw-bold">What is Pattern Recognition? </p>
                            Pattern recognition is the process of recognizing patterns by using a machine learning algorithm. Pattern recognition can be defined as the classification of data based on knowledge already gained or on statistical information extracted from patterns and/or their representation. One of the important aspects of pattern recognition is its application potential. 

                            Examples: Speech recognition, speaker identification, multimedia document recognition (MDR), automatic medical diagnosis. 
                            In a typical pattern recognition application, the raw data is processed and converted into a form that is amenable for a machine to use. Pattern recognition involves the classification and cluster of patterns. 

                            In classification, an appropriate class label is assigned to a pattern based on an abstraction that is generated using a set of training patterns or domain knowledge. Classification is used in supervised learning.
                            Clustering generated a partition of the data which helps decision making, the specific decision-making activity of interest to us. Clustering is used in unsupervised learning.
                            Features may be represented as continuous, discrete, or discrete binary variables. A feature is a function of one or more measurements, computed so that it quantifies some significant characteristics of the object. 

                            Example: consider our face then eyes, ears, nose, etc are features of the face. 
                            A set of features that are taken together, forms the features vector. 

                            Example: In the above example of a face, if all the features (eyes, ears, nose, etc) are taken together then the sequence is a feature vector([eyes, ears, nose]). The feature vector is the sequence of a feature represented as a d-dimensional column vector. In the case of speech, MFCC (Mel-frequency Cepstral Coefficient) is the spectral feature of the speech. The sequence of the first 13 features forms a feature vector. 
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 24. Predictive analytics -->
                    <div class="card" id="dataScience">
                        <h3><li>Predictive analysis</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.tierpoint.com/wp-content/uploads/2024/02/Predictive-AI-Techniques-1024x627.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Predictive analytics is a type of analytics that uses technology to predict what will happen in a specific time frame based on historical data and patterns.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Predictive analytics is one of the four key types of data analytics, and typically forecasts what will happen in the future, such as how sales will shift during different seasons or how consumers will respond to a change in price. Businesses often use predictive analytics to make data-driven decisions and optimize outcomes.
                            <p class="fw-bold">What is predictive analytics?</p>
                            Businesses use data to understand what's happening—both now and in the future. Predictive analytics falls under the latter category. It uses historical data to predict potential future events or behaviors so companies can better position themselves in the present.

                            In order to calculate the future, predictive analytics relies on a number of techniques from statistics, data analytics, artificial intelligence (AI), and machine learning. Some common business applications include detecting fraud, predicting customer behavior, and forecasting demand.
                            <p class="fw-bold">Benefits of predictive analytics</p>
    
                            Predictive analytics can help businesses make stronger, more informed decisions. It can help you identify patterns and trends within data that enable different business functions to make a probabilistic determination about future events. Other benefits include:

                            Decision making: Improve how a business function makes decisions by relying on data to determine potential outcomes

                            Risk management: Develop risk management strategies for potential risks and prioritize the most detrimental risks

                            Customer insights: Better understand potential customers and what they need so that you can develop more specific marketing campaigns to reach them

                            Operational efficiency: Make companies operate more efficiently by turning to historical data to understand resources and better manage them
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 25. Prompt Engineering -->
                    <div class="card" id="dataScience">
                        <h3><li>Prompt Engineering</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn-infographic.pressidium.com/wp-content/uploads/2024/06/Types-Of-Prompts-For-AI-Creativity-960x720.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Prompt engineering is the art and science of designing and optimizing prompts to guide AI models, particularly LLMs, towards generating the desired responses.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            In the context of AI, a prompt is the input you provide to the model to elicit a specific response. This can take various forms, ranging from simple questions or keywords to complex instructions, code snippets, or even creative writing samples. The effectiveness of your prompt directly influences the quality and relevance of the AI's output.

                            <p class="fw-bold">What do you need for prompt engineering?</p>
                            Several key elements contribute to effective prompt engineering. Mastering these allows you to communicate effectively with AI models and unlock their full potential.

                            Prompt format
                            The structure and style of your prompt play a significant role in guiding the AI's response. Different models may respond better to specific formats, such as:

                            The format of your prompt plays a significant role in how the AI interprets your request. Different models may respond better to specific formats, such as natural language questions, direct commands, or structured inputs with specific fields. Understanding the model's capabilities and preferred format is essential for crafting effective prompts.

                            Context and examples
                            Providing context and relevant examples within your prompt helps the AI understand the desired task and generate more accurate and relevant outputs. For instance, if you're looking for a creative story, including a few sentences describing the desired tone or theme can significantly improve the results.

                            Fine-tuning and adapting
                            Fine-tuning the AI model on specific tasks or domains using tailored prompts can enhance its performance. Additionally, adapting prompts based on user feedback or model outputs can further improve the model's responses over time.

                            Multi-turn conversations
                            Designing prompts for multi-turn conversations allows users to engage in continuous and context-aware interactions with the AI model, enhancing the overall user experience.
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <div class="card" id="">
                        <h3><li>Pooling Layer</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://connectjaya.com/wp-content/uploads/2021/06/Slide7-2-1024x576.jpg" alt="pooling_layer" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A layer in CNNs used to reduce spatial dimensions, helping to make the model computationally efficient and focus on important features.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Pooling layers are an essential component of Convolutional Neural Networks (CNNs). They downsample feature maps to reduce spatial dimensions, decrease computation, and prevent overfitting. Common pooling techniques include:
                            </p>
                            <ul>
                                <li><strong>Max Pooling</strong>: Retains the maximum value from each region.</li>
                                <li><strong>Average Pooling</strong>: Computes the average of values in each region.</li>
                            </ul>
                            <p>By focusing on the most critical features, pooling layers enable models to identify high-level patterns, making them effective for image recognition and other visual tasks.</p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <div class="card" id="">
                        <h3><li>Precision</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://img.freepik.com/premium-vector/ai-ml-precision-medicine-circle-infographic-template_106317-36597.jpg" alt="precision" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A metric that measures the proportion of true positives among all positive predictions, often used in evaluating classification models.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Precision is crucial for understanding how reliable a model's positive predictions are, especially in applications where false positives have a high cost. It is calculated using the formula:
                            </p>
                            <p class="fw-bold">Precision = True Positives / (True Positives + False Positives)</p>
                            <p>High precision is desirable when false alarms can have serious consequences, such as in fraud detection or medical diagnosis.</p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <div class="card" id="">
                        <h3><li>Principal Component Analysis (PCA)</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://ashutoshtripathi.com/wp-content/uploads/2019/07/pca1-1.png" alt="principal_component_analysis" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A dimensionality reduction technique used to reduce the number of features while retaining most of the variation in the data.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                PCA simplifies datasets by transforming them into a smaller set of uncorrelated components while preserving most of the data's variance. It involves:
                            </p>
                            <ul>
                                <li>Standardizing the data to zero mean and unit variance.</li>
                                <li>Computing the covariance matrix and eigenvectors.</li>
                                <li>Projecting the data onto principal components.</li>
                            </ul>
                            <p>It is widely used in exploratory data analysis, visualization, and noise reduction.</p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 26. Quantum computing -->
                    <div class="card" id="dataScience">
                        <h3><li>Quantum Computing</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://img.freepik.com/premium-vector/quantum-computing-with-engineers-physics-infographics_1268-7705.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Quantum computing is the process of using quantum-mechanical phenomena such as entanglement and superposition to perform calculations. Quantum machine learning uses these algorithms on quantum computers to expedite work because it performs much faster than a classic machine learning program and computer.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <!-- <p>Quantum computing is the process of using quantum-mechanical phenomena such as entanglement and superposition to perform calculations. Quantum machine learning uses these algorithms on quantum computers to expedite work because it performs much faster than a classic machine learning program and computer. -->
                            <p class="fw-bold">What is quantum machine learning? </p>
                            Quantum machine learning uses algorithms run on quantum devices, such as quantum computers, to supplement, expedite, or support the work performed by a classical machine learning program. Also called quantum-enhanced machine learning, quantum machine learning leverages the information processing power of quantum technologies to enhance and speed up the work performed by a machine learning model.

                            While classical computers are constrained by limited storage and processing capacities, quantum-enabled ones allow for exponentially more storage and processing power. This ability to store and process huge amounts of information means that quantum computers can analyze massive data sets that would take classical methods significantly longer to perform. As a result, quantum machine learning leverages this out-sized processing power to expedite and improve the development of machine learning models, neural networks, and other forms of artificial intelligence (AI).
                            <p class="fw-bold">Quantum machine learning uses </p>
    
                            From crunching massive amounts of big data to powering transformative technological advances, both quantum computing and machine learning stand to make waves in the future. While quantum machine learning is still in its infancy, researchers and professionals are already using it in numerous ways. Some of these applications include to: 

                            Develop new machine learning algorithms
                            Speed up already existing machine learning algorithms 

                            Employ quantum-enhanced reinforcement learning, in which a machine learning algorithm learns based on its interactions within a quantum environment 

                            Create quantum neural networks, which can operate at fewer steps and with greater processing speed than traditional neural networks.

                            Despite these intriguing applications, though, the field of quantum computing and machine learning is still growing and changing. As a result, many other applications used to solve real-world problems will likely develop in the near and distant future. 
                            </p>
    
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <div class="card" id="">
                        <h3><li>Recurrent Neural Network (RNN)</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTftIrPyjOyo6MHdt9sexZsjReLfAGT7BJ-FA&s" alt="recurrent_neural_network" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A neural network architecture for sequential data where connections form cycles, enabling memory of previous inputs.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                RNNs process sequential data by maintaining a memory of past inputs. This makes them ideal for tasks such as:
                            </p>
                            <ul>
                                <li>Language modeling and translation.</li>
                                <li>Time-series forecasting.</li>
                                <li>Speech recognition.</li>
                            </ul>
                            <p>Variants like LSTMs and GRUs address RNN limitations, such as the vanishing gradient problem, to better handle long-term dependencies.</p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <div class="card" id="">
                        <h3><li>Regularization</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://media.licdn.com/dms/image/v2/C5612AQEvrSbm51wq6w/article-inline_image-shrink_1000_1488/article-inline_image-shrink_1000_1488/0/1626163383700?e=1732752000&v=beta&t=4dMGtXbhYZw7T9UKaJDKFPC2t_6Wc2TcjbeaH30SFcQ" alt="regularization" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Techniques used to prevent overfitting in models by adding constraints, such as L1 and L2 regularization.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Regularization helps improve a model’s generalization by penalizing overly complex models. Common techniques include:
                            </p>
                            <ul>
                                <li><strong>L1 Regularization</strong>: Adds the absolute values of weights to the loss function, encouraging sparsity.</li>
                                <li><strong>L2 Regularization</strong>: Adds the squared weights to the loss function, discouraging large weights.</li>
                            </ul>
                            <p>These methods prevent overfitting by balancing model complexity and accuracy, ensuring better performance on unseen data.</p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 27. Reinforcement learning -->
                    <div class="card" id="dataScience">
                        <h3><li>Reinforcement Learning</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://pbs.twimg.com/media/CLyZCTBW8AA2qsV.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Reinforcement learning is a type of machine learning in which an algorithm learns by interacting with its environment and then is either rewarded or penalized based on its actions.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals. Software actions that work towards your goal are reinforced, while actions that detract from the goal are ignored. 

                            RL algorithms use a reward-and-punishment paradigm as they process data. They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes. The algorithms are also capable of delayed gratification. The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way. RL is a powerful method to help artificial intelligence (AI) systems achieve optimal outcomes in unseen environments.
                            <p class="fw-bold">What are the benefits of reinforcement learning?</p>
                            There are many benefits to using reinforcement learning (RL). However, these three often stand out.

                            Excels in complex environments
                            RL algorithms can be used in complex environments with many rules and dependencies. In the same environment, a human may not be capable of determining the best path to take, even with superior knowledge of the environment. Instead, model-free RL algorithms adapt quickly to continuously changing environments and find new strategies to optimize results.

                            Requires less human interaction
                            In traditional ML algorithms, humans must label data pairs to direct the algorithm. When you use an RL algorithm, this isn’t necessary. It learns by itself. At the same time, it offers mechanisms to integrate human feedback, allowing for systems that adapt to human preferences, expertise, and corrections.

                            Optimizes for long-term goals
                            RL inherently focuses on long-term reward maximization, which makes it apt for scenarios where actions have prolonged consequences. It is particularly well-suited for real-world situations where feedback isn't immediately available for every step, since it can learn from delayed rewards.

                            For example, decisions about energy consumption or storage might have long-term consequences. RL can be used to optimize long-term energy efficiency and cost. With appropriate architectures, RL agents can also generalize their learned strategies across similar but not identical tasks.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 28. Sentiment analysis -->
                    <div class="card" id="dataScience">
                        <h3><li>Sentiment Analysis</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn.shopify.com/s/files/1/1905/9639/files/Benefits_of_AI-based_sentiment_analysis_-_Lucent_innovation_2048x2048.png?v=1705062130" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Sentiment analysis is a popular task in natural language processing. The goal of sentiment analysis is to classify the text based on the mood or mentality expressed in the text, which can be positive negative, or neutral.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Sentiment analysis is the process of classifying whether a block of text is positive, negative, or neutral. The goal that Sentiment mining tries to gain is to be analysed people’s opinions in a way that can help businesses expand. It focuses not only on polarity (positive, negative & neutral) but also on emotions (happy, sad, angry, etc.). It uses various Natural Language Processing algorithms such as Rule-based, Automatic, and Hybrid.
                            <p class="fw-bold">Why is Sentiment Analysis Important?</p>
                            Sentiment analysis is the contextual meaning of words that indicates the social sentiment of a brand and also helps the business to determine whether the product they are manufacturing is going to make a demand in the market or not.

                            According to the survey,80% of the world’s data is unstructured. The data needs to be analyzed and be in a structured manner whether it is in the form of emails, texts, documents, articles, and many more.

                            Sentiment Analysis is required as it stores data in an efficient, cost friendly.
                            Sentiment analysis solves real-time issues and can help you solve all real-time scenarios.
                            Here are some key reasons why sentiment analysis is important for business:

                            Customer Feedback Analysis: Businesses can analyze customer reviews, comments, and feedback to understand the sentiment behind them helping in identifying areas for improvement and addressing customer concerns, ultimately enhancing customer satisfaction.
                            Brand Reputation Management: Sentiment analysis allows businesses to monitor their brand reputation in real-time.
                            By tracking mentions and sentiments on social media, review platforms, and other online channels, companies can respond promptly to both positive and negative sentiments, mitigating potential damage to their brand.
                            Product Development and Innovation: Understanding customer sentiment helps identify features and aspects of their products or services that are well-received or need improvement. This information is invaluable for product development and innovation, enabling companies to align their offerings with customer preferences.
                            Competitor Analysis: Sentiment Analysis can be used to compare the sentiment around a company’s products or services with those of competitors.
                            Businesses identify their strengths and weaknesses relative to competitors, allowing for strategic decision-making.
                            Marketing Campaign Effectiveness
                            Businesses can evaluate the success of their marketing campaigns by analyzing the sentiment of online discussions and social media mentions.
                            Positive sentiment indicates that the campaign is resonating with the target audience, while negative sentiment may signal the need for adjustments.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    
                    <!-- 29. Structured data -->
                    <div class="card" id="dataScience">
                        <h3><li>Strucutred data</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.expert.ai/wp-content/uploads/2020/10/StructuredVsUnstructuredData-300x234.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Structured data is data that has a standardized format for efficient access by software and humans alike. It is typically tabular with rows and columns that clearly define data attributes. Computers can effectively process structured data for insights due to its quantitative nature.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            When it comes to data, files can come in many different forms. There are two main types of data—structured and unstructured. Each is sourced and collected in different ways, living on different types of databases, so their differences are important for data professionals.

                            But, how do they differ from one another exactly? And, why would you want to use one over the other?
                            <p class="fw-bold">Structured vs. unstructured data</p>
                            The main difference is that structured data is defined and searchable. This includes data like dates, phone numbers, and product SKUs. Unstructured data is everything else, which is more difficult to categorize or search, like photos, videos, podcasts, social media posts, and emails. Most of the data in the world is unstructured data.
                            <p class="fw-bold">What is structured data?</p>
                            TStructured data is typically quantitative data that is organized and easily searchable. The programming language Structured Query Language (SQL) is used in a relational database to “query” to input and search within structured data. 

                            Common types of structured data include names, addresses, credit card numbers, telephone numbers, star ratings from customers, bank information, and other data that can be easily searched using SQL. 

                            Structured data examples
                            In the real world, structured data could be used for things like:

                            Booking a flight: Flight and reservation data, such as dates, prices, and destinations, fit neatly within the Excel spreadsheet format. When you book a flight, this information is stored in a database.

                            Customer relationship management (CRM): CRM software such as Salesforce runs structured data through analytical tools to create new data sets for businesses to analyze customer behavior and preferences.

                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <div class="card" id="">
                        <h3><li>Stochastic Gradient Descent (SGD)</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn.botpenguin.com/assets/website/Stochastic_Gradient_Descent_c95b69fa98.webp" alt="stochastic_gradient_descent" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A variant of gradient descent where updates are made based on random samples, speeding up training for large datasets.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Stochastic Gradient Descent (SGD) improves training efficiency by updating model parameters based on individual or small batches of data. Key benefits include:
                            </p>
                            <ul>
                                <li>Faster convergence on large datasets.</li>
                                <li>Introduction of noise that helps escape local minima.</li>
                            </ul>
                            <p>Variants like mini-batch SGD and momentum-based SGD further optimize performance for specific use cases.</p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <div class="card" id="">
                        <h3><li>Support Vector Machine (SVM)</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.infodiagram.com/api/img/f:png/w:1200/aHR0cHM6Ly9jZG4uaW5mb2RpYWdyYW0uY29tL2MvNzExNmU0L3N1cHBvcnQtdmVjdG9yLW1hY2hpbmVzLXN2bS1pbGx1c3RyYXRpb24tY2hhcnQud2VicA==" alt="support_vector_machine" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A supervised learning algorithm that separates classes with the maximum margin hyperplane, effective for classification tasks.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                SVMs work by finding the hyperplane that best separates different classes in the data. Key features include:
                            </p>
                            <ul>
                                <li>Effective for high-dimensional spaces.</li>
                                <li>Can handle non-linear data using kernel functions.</li>
                                <li>Robust to overfitting when used with proper regularization.</li>
                            </ul>
                            <p>Applications range from image classification to bioinformatics.</p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 30. Supervised learning -->
                    <div class="card" id="dataScience">
                        <h3><li>Supervised Learning</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://i.redd.it/tyjjqjswi5621.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Supervised learning is a type of machine learning in which classified output data is used to train the machine and produce the correct algorithms. It is much more common than unsupervised learning.
                        </p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Supervised learning is a category of machine learning that uses labeled datasets to train algorithms to predict outcomes and recognize patterns. Unlike unsupervised learning, supervised learning algorithms are given labeled training to learn the relationship between the input and the outputs. 

                            Supervised machine learning algorithms make it easier for organizations to create complex models that can make accurate predictions. As a result, they are widely used across various industries and fields, including healthcare, marketing, financial services, and more. 
                            <p class="fw-bold">How does supervised learning work?</p>
                            The data used in supervised learning is labeled — meaning that it contains examples of both inputs (called features) and correct outputs (labels). The algorithms analyze a large dataset of these training pairs to infer what a desired output value would be when asked to make a prediction on new data.

                            For instance, let’s pretend you want to teach a model to identify pictures of trees. You provide a labeled dataset that contains many different examples of types of trees and the names of each species. You let the algorithm try to define what set of characteristics belongs to each tree based on the labeled outputs. You can then test the model by showing it a tree picture and asking it to guess what species it is. If the model provides an incorrect answer, you can continue training it and adjusting its parameters with more examples to improve its accuracy and minimize errors. 

                            Once the model has been trained and tested, you can use it to make predictions on unknown data based on the previous knowledge it has learned.

                            <p class="fw-bold">Types of supervised learning</p>
                            Supervised learning in machine learning is generally divided into two categories: classification and regression. 

                            Classification
                            Classification algorithms are used to group data by predicting a categorical label or output variable based on the input data. Classification is used when output variables are categorical, meaning there are two or more classes.

                            One of the most common examples of classification algorithms in use is the spam filter in your email inbox. Here, a supervised learning model is trained to predict whether an email is spam or not with a dataset that contains labeled examples of both spam and legitimate emails. The algorithm extracts information about each email, including the sender, the subject line, body copy, and more. It then uses these features and corresponding output labels to learn patterns and assign a score that indicates whether an email is real or spam.

                            Regression
                            Regression algorithms are used to predict a real or continuous value, where the algorithm detects a relationship between two or more variables. 

                            A common example of a regression task might be predicting a salary based on work experience. For instance, a supervised learning algorithm would be fed inputs related to work experience (e.g., length of time, the industry or field, location, etc.) and the corresponding assigned salary amount. After the model is trained, it could be used to predict the average salary based on work experience.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 31. Token -->
                    <div class="card" id="dataScience">
                        <h3><li>Token</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://d230m64oxp1vr8.cloudfront.net/blogs/codeZeros954hb9sh1712984901598.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A token is a basic unit of text that an LLM uses to understand and generate language. A token may be an entire word or parts of a word.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            It's a term that floats around the realm of Generative AI, often leaving many scratching their heads. Far from the realm of cryptocurrency or reward systems, in the world of Artificial Intelligence, tokens play a pivotal role in understanding and generating human-like text. <br>
                            At its core, a token is the smallest unit into which text data can be broken down for an AI model to process. Think of it as similar to how we might break sentences into words or characters. However, for AI, especially in the context of language models, these tokens can represent a character, a word, or even larger chunks of text like phrases, depending on the model and its configuration.

                            <p class="fw-bold">Why Are Tokens Important?</p>
                            Data Representation: Tokens serve as the bridge between raw human language and a format that AI models can process. Every token is converted into a numerical format (often a high-dimensional vector) using embeddings. These numerical representations capture the semantic essence of the token and can be processed by neural networks.

                            Memory and Computation: AI models, especially large ones, have a fixed number of tokens they can handle in one go, known as their "context window" or "attention span". By understanding the nature and number of tokens, developers can effectively interact with the model and structure the input to ensure optimal performance.

                            Granularity and Flexibility: Since tokens can represent varying sizes of text chunks, they provide flexibility. For example, a model designed to operate on word-level tokens might be ideal for certain languages or applications, while character-level tokens might be more suited for others.


                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 32. Training data -->
                    <div class="card" id="dataScience">
                        <h3><li>Training Data</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.cognilytica.com/wp-content/uploads/2022/08/The-Steps-for-an-AI-Project.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Training data is the information or examples given to an AI system to enable it to learn, find patterns, and create new content.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            In the world of artificial intelligence and machine learning, data training is inevitable. This is the process that makes machine learning modules accurate, efficient and fully functional. In this post, we explore in detail what AI training data is, training data quality, data collection & licensing and more.

                            It is estimated that on average adult makes decisions on life and everyday things based on past learning. These, in turn, come from life experiences shaped by situations and people. In the literal sense, situations, instances, and people are nothing but data that gets fed into our minds. As we accumulate years of data in the form of experience, the human mind tends to make seamless decisions.
                            <p class="fw-bold">What does this convey? That data is inevitable in learning.</p>
                            Similar to how a child needs a label called an alphabet to understand the letters A, B, C, D a machine also needs to understand the data it is receiving.

                            That’s exactly what Artificial Intelligence (AI) training is all about. A machine is no different than a child who has yet to learn things from what they are about to be taught. The machine does not know to differentiate between a cat and a dog or a bus and a car because they haven’t yet experienced those items or been taught what they look like.

                            So, for someone building a self-driving car, the primary function that needs to be added is the system’s ability to understand all the everyday elements the car may encounter, so the vehicle can identify them and make appropriate driving decisions. This is where AI training data comes into play. 

                            Today, artificial intelligence modules offer us many conveniences in the form of recommendation engines, navigation, automation, and more. All of that happens due to AI data training that was used to train the algorithms while they were built.

                            AI training data is a fundamental process in building machine learning and AI algorithms. If you are developing an app that is based on these tech concepts, you need to train your systems to understand data elements for optimized processing. Without training, your AI model will be inefficient, flawed and potentially pointless.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 33. Transfer learning -->
                    <div class="card" id="dataScience">
                        <h3><li>Transfer Learning</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://d1krbhyfejrtpz.cloudfront.net/blog/wp-content/uploads/2024/02/19161749/Infographic2-10-1024x526.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Transfer learning is a machine learning system that takes existing, previously learned data and applies it to new tasks and activities.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Transfer learning is a machine learning technique where a model trained on one task is repurposed as the foundation for a second task. This approach is beneficial when the second task is related to the first or when data for the second task is limited. Leveraging learned features from the initial task, the model can adapt more efficiently to the new task, accelerating learning and improving performance. Transfer learning also reduces overfitting risk, as the model already incorporates generalizable features useful for the second task.
                            <p class="fw-bold">Why is Transfer Learning Important?</p>
                            Transfer learning is a critical technique in machine learning, offering solutions to key challenges:

                            Limited Data: Acquiring extensive labeled data is often challenging and costly. Transfer learning enables us to use pre-trained models, reducing the dependency on large datasets.
                            Enhanced Performance: Starting with a pre-trained model, which has already learned from substantial data, allows for faster and more accurate results on new tasks—ideal for applications needing high accuracy and efficiency.
                            Time and Cost Efficiency: Transfer learning shortens training time and conserves resources by utilizing existing models, eliminating the need for training from scratch.
                            Adaptability: Models trained on one task can be fine-tuned for related tasks, making transfer learning versatile for various applications, from image recognition to natural language processing.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 34. Turing test -->
                    <div class="card" id="dataScience">
                        <h3><li>Turing Test</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://i.abcnewsfe.com/a/187ba44a-8c4a-4a87-96e7-140a1767243f/TuringTestInfographic_v01_DG_1689800738777_hpEmbed_16x9.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">The Turing test was created by computer scientist Alan Turing to evaluate a machine’s ability to exhibit intelligence equal to humans, especially in language and behavior. </p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            The Turing Test is a method of inquiry in artificial intelligence (AI) for determining whether or not a computer is capable of thinking like a human being. The test is named after Alan Turing, the founder of the Turing Test and an English computer scientist, cryptanalyst, mathematician and theoretical biologist.
                            <p class="fw-bold">History of the Turing Test</p>
                            The test is named after Alan Turing, who pioneered machine learning during the 1940s and 1950s. Turing introduced the test in his 1950 paper called "Computing Machinery and Intelligence" while at the University of Manchester.

                            In his paper, Turing proposed a twist on what is called "The Imitation Game." The Imitation Game involves no use of AI, but rather three human participants in three separate rooms. Each room is connected via a screen and keyboard, one containing a male, the other a female, and the other containing a male or female judge. The female tries to convince the judge that she is the male, and the judge tries to disseminate which is which.

                            Turing changes the concept of this game to include an AI, a human and a human questioner. The questioner's job is then to decide which is the AI and which is the human. Since the formation of the test, many AI have been able to pass; one of the first is a program created by Joseph Weizenbaum called ELIZA.
                            <p class="fw-bold">Limitations of the Turing Test</p>
                            <p>The Turing Test has been criticized over the years, in particular because historically, the nature of the questioning had to be limited in order for a computer to exhibit human-like intelligence. For many years, a computer might only score high if the questioner formulated the queries, so they had "Yes" or "No" answers or pertained to a narrow field of knowledge. When questions were open-ended and required conversational answers, it was less likely that the computer program could successfully fool the questioner.

                            In addition, a program such as ELIZA could pass the Turing Test by manipulating symbols it does not understand fully. John Searle argued that this does not determine intelligence comparable to humans.

                            To many researchers, the question of whether or not a computer can pass a Turing Test has become irrelevant. Instead of focusing on how to convince someone they are conversing with a human and not a computer program, the real focus should be on how to make a human-machine interaction more intuitive and efficient. For example, by using a conversational interface.</p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <!-- 35. Unstructured data -->
                    <div class="card" id="dataScience">
                        <h3><li>Unstructured data</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.expert.ai/wp-content/uploads/2020/10/StructuredVsUnstructuredData-300x234.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Unstructured data is data that is undefined and difficult to search. This includes audio, photo, and video content. Most of the data in the world is unstructured.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Unstructured data is every other type of data that is not structured. Approximately 80-90% of data is unstructured, meaning it has huge potential for competitive advantage if companies find ways to leverage it [1]. Unstructured data includes a variety of formats such as emails, images, video files, audio files, social media posts, PDFs, and much more.

                            Unstructured data is typically stored in data lakes, NoSQL databases, data warehouses, and applications. Today, this information can be processed by artificial intelligence algorithms and delivers huge value for organizations.
                            <p class="fw-bold">Examples of unstructured data</p>
                            In the real world, unstructured data could be used for things like:

                            Chatbots: Chatbots are programmed to perform text analysis to answer customer questions and provide the right information.

                            Market predictions: Data can be maneuvered to predict changes in the stock market so that analysts can adjust their calculations and investment decisions.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <div class="card" id="">
                        <h3><li>Underfitting</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://365datascience.com/resources/blog/x671k7dla1f-overfitting-vs-underfitting-classification-examples.png" alt="underfitting" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A problem where a model is too simple to capture the patterns in the data, resulting in poor performance on both training and test sets.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Underfitting occurs when a model has high bias and fails to learn the underlying relationships in the data. Common causes include:
                            </p>
                            <ul>
                                <li>Using too few features or oversimplified algorithms.</li>
                                <li>Insufficient training time or data.</li>
                            </ul>
                            <p>Techniques like increasing model complexity or using more features can address underfitting.</p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 36. Unsupervised learning -->
                    <div class="card" id="dataScience">
                        <h3><li>Unsupervised Learning</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://i.redd.it/tyjjqjswi5621.png" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Unsupervised learning is a type of machine learning in which an algorithm is trained with unclassified and unlabeled data so that it acts without supervision.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Unsupervised learning in artificial intelligence is a type of machine learning that learns from data without human supervision. Unlike supervised learning, unsupervised machine learning models are given unlabeled data and allowed to discover patterns and insights without any explicit guidance or instruction. 
                            <p class="fw-bold">How does unsupervised learning work?</p>
                            As the name suggests, unsupervised learning uses self-learning algorithms—they learn without any labels or prior training. Instead, the model is given raw, unlabeled data and has to infer its own rules and structure the information based on similarities, differences, and patterns without explicit instructions on how to work with each piece of data.

                            Unsupervised learning algorithms are better suited for more complex processing tasks, such as organizing large datasets into clusters. They are useful for identifying previously undetected patterns in data and can help identify features useful for categorizing data. 

                            Imagine that you have a large dataset about weather. An unsupervised learning algorithm will go through the data and identify patterns in the data points. For instance, it might group data by temperature or similar weather patterns. 

                            While the algorithm itself does not understand these patterns based on any previous information you provided, you can then go through the data groupings and attempt to classify them based on your understanding of the dataset. For instance, you might recognize that the different temperature groups represent all four seasons or that the weather patterns are separated into different types of weather, such as rain, sleet, or snow. 
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <!-- 37. Voice recognition -->
                    <div class="card" id="dataScience">
                        <h3><li>Voice recognition</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://img.freepik.com/premium-vector/voice-recognition-infographics_372769-1792.jpg" alt="ai_infographic" height="500px" width="100%" style="display: none;">
    
                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">Voice recognition, also called speech recognition, is a method of human-computer interaction in which computers listen and interpret human dictation (speech) and produce written or spoken outputs.</p>
    
                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                            Voice recognition, otherwise known as speaker recognition, is a software program that has been trained to identify, decode, distinguish and authenticate the voice of a person based on their distinct voiceprint.
                            <p class="fw-bold">How Does Voice Recognition Work?</p>
                            Audio Input: The process begins with capturing the audio input using a microphone.

                            Preprocessing: The audio signal is cleaned up by removing noise and normalizing the volume.

                            Feature Extraction: The system analyzes the audio to extract key features such as pitch, tone, and frequency.

                            Pattern Recognition: The extracted features are compared to known patterns of speech stored in a database.

                            Language Processing: The recognized patterns are converted into text, and natural language processing (NLP) algorithms interpret the meaning.
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>


                    <div class="card" id="">
                        <h3><li>Vanishing Gradient</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://cdn.prod.website-files.com/5ef788f07804fb7d78a4127a/6245a9aca7defe61cea5ea7d_Engati-vanishing-point-problem.jpg" alt="vanishing_gradient" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A problem where gradients become too small during backpropagation, making it difficult to update the weights and train deep neural networks.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                The vanishing gradient problem arises in deep networks with activation functions like sigmoid or tanh. Solutions include:
                            </p>
                            <ul>
                                <li>Using ReLU or similar activation functions.</li>
                                <li>Initializing weights carefully to prevent small gradients.</li>
                                <li>Employing architectures like LSTMs or residual networks (ResNets).</li>
                            </ul>
                            <p>These techniques improve gradient flow and enable effective training of deep networks.</p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>

                    <div class="card" id="">
                        <h3><li>Variance</li></h3>
                        <img class="toggleImg p-3 align-center" src="https://www.6sigma.us/wp-content/uploads/2024/09/applications-of-standard-deviation-and-variance.webp" alt="variance" height="500px" width="100%" style="display: none;">

                        <!-- Main content that is always shown -->
                        <p id="shortContent" class="fw-bold">A measure of how much predictions for a given model vary based on different training data, related to the bias-variance tradeoff.</p>

                        <!-- Additional content that will be shown upon clicking "Read More" -->
                        <div class="moreContent" style="display: none;">
                            <p>
                                Variance indicates a model’s sensitivity to small changes in training data. High variance leads to overfitting, where the model performs well on training data but poorly on unseen data. Balancing bias and variance is critical to achieving good generalization.
                            </p>
                        </div>
                        <!-- Link to toggle visibility -->
                        <a href="#" class="readMoreLink">Read More</a>
                    </div>
                    </ol>

                </div>
            </div>
        </div>
        <div class="goToTop">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="30px" height="30px" color="#000000" fill="none">
                <path d="M12 4L12 20" stroke="currentColor" stroke-width="2.7" stroke-linecap="round" stroke-linejoin="round" />
                <path d="M16.9998 8.99996C16.9998 8.99996 13.3174 4.00001 11.9998 4C10.6822 3.99999 6.99982 9 6.99982 9" stroke="currentColor" stroke-width="2.7" stroke-linecap="round" stroke-linejoin="round" />
            </svg>
        </div>
        <p>More Contet will be added soon!</p>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    <script src="script.js"></script>
</body>

</html>